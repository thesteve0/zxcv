
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>Evaluating Models â€” FiftyOne 1.3.0 documentation</title>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
<link href="../_static/css/voxel51-website.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="using_aggregations.html" rel="next" title="Using Aggregations"/>
<link href="annotation.html" rel="prev" title="Annotating Datasets"/>
<meta content="https://voxel51.com/wp-content/uploads/2024/03/3.24_webpages_Home_AV.png" property="og:image">
<link href="https://fonts.googleapis.com/css?family=Palanquin:400,600,700,800" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" rel="stylesheet"/>
<script src="https://tag.clearbitscripts.com/v1/pk_b9ed71c8234edd4f77326bcbfab5a4ca/tags.js"></script>
<script src="../_static/js/modernizr.min.js"></script>
</meta></head>

<body class="pytorch-body">
<div class="table-of-contents-link-wrapper">
<span>Table of Contents</span>
<a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#"></a>
</div>
<nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
<div class="pytorch-side-scroll">
<div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
<div class="pytorch-left-menu-search">
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search Docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../teams/index.html">FiftyOne Teams ðŸš€</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/index.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheets/index.html">Cheat Sheets</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_zoo/index.html">Dataset Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brain.html">FiftyOne Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../integrations/index.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins/index.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/fiftyone.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deprecation.html">Deprecation Notices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>
</div>
</div>
</nav>
<div class="pytorch-container">
<div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
<div class="pytorch-breadcrumbs-wrapper">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="pytorch-breadcrumbs">
<li>
<a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>
<li><a href="index.html">FiftyOne User Guide</a> &gt;</li>
<li>Evaluating Models</li>
<li class="pytorch-breadcrumbs-aside">
</li>
</ul>
</div>
</div>
<div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Contents
        </div>
</div>
<section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
<div class="pytorch-content-left">
<div class="rst-content style-external-links">
<div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
<div class="section" id="evaluating-models">
<span id="id1"></span><h1>Evaluating Models<a class="headerlink" href="#evaluating-models" title="Permalink to this headline">Â¶</a></h1>
<p>FiftyOne provides a variety of builtin methods for evaluating your model
predictions, including regressions, classifications, detections, polygons,
instance and semantic segmentations, on both image and video datasets.</p>
<p>When you evaluate a model in FiftyOne, you get access to the standard aggregate
metrics such as classification reports, confusion matrices, and PR curves
for your model. In addition, FiftyOne can also record fine-grained statistics
like accuracy and false positive counts at the sample-level, which you can
<span class="xref std std-ref">interactively explore</span> in the App to diagnose
the strengths and weaknesses of your models on individual data samples.</p>
<p>Sample-level analysis often leads to critical insights that will help you
improve your datasets and models. For example, viewing the samples with the
most false positive predictions can reveal errors in your annotation schema.
Or, viewing the cluster of samples with the lowest accuracy can reveal gaps in
your training dataset that you need to address in order to improve your modelâ€™s
performance. A key goal of FiftyOne is to help you uncover these insights on
your data!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check out the <a class="reference internal" href="../tutorials/index.html#tutorials"><span class="std std-ref">tutorials page</span></a> for in-depth walkthroughs
of evaluating various types of models with FiftyOne.</p>
</div>
<div class="section" id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">Â¶</a></h2>
<p>FiftyOneâ€™s evaluation methods are conveniently exposed as methods on all
<a class="reference internal" href="../api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset" title="fiftyone.core.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> and <a class="reference internal" href="../api/fiftyone.core.view.html#fiftyone.core.view.DatasetView" title="fiftyone.core.view.DatasetView"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetView</span></code></a> objects, which means that you can evaluate entire
datasets or specific views into them via the same syntax.</p>
<p>Letâ€™s illustrate the basic workflow by loading the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart"><span class="std std-ref">quickstart dataset</span></a> and analyzing the object
detections in its <code class="code docutils literal notranslate"><span class="pre">predictions</span></code> field using the
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>

<span class="c1"># Evaluate the objects in the `predictions` field with respect to the</span>
<span class="c1"># objects in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<div class="section" id="model-evaluation-panel-sub-new">
<h3>Model Evaluation panel <strong style="color: hsl(25, 100%, 51%); font-size: 0.85em; vertical-align: top">NEW</strong><a class="headerlink" href="#model-evaluation-panel-sub-new" title="Permalink to this headline">Â¶</a></h3>
<p>When you load a dataset in the App that contains one or more
<a class="reference internal" href="#evaluating-models"><span class="std std-ref">evaluations</span></a>, you can open the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a> to visualize and
interactively explore the evaluation results in the App:</p>
<img alt="model-evaluation-compare" class="align-center" src="../_images/model-evaluation-compare.gif"/>
</div>
<div class="section" id="per-class-metrics">
<h3>Per-class metrics<a class="headerlink" href="#per-class-metrics" title="Permalink to this headline">Â¶</a></h3>
<p>You can also retrieve and interact with evaluation results via the SDK.</p>
<p>Running an evaluation returns an instance of a task-specific subclass of
<a class="reference internal" href="../api/fiftyone.core.evaluation.html#fiftyone.core.evaluation.EvaluationResults" title="fiftyone.core.evaluation.EvaluationResults"><code class="xref py py-class docutils literal notranslate"><span class="pre">EvaluationResults</span></code></a> that provides a handful of methods for generating aggregate
statistics about your dataset.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Get the 10 most common classes in the dataset</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">"ground_truth.detections.label"</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>

<span class="c1"># Print a classification report for the top-10 classes</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>               precision    recall  f1-score   support

       person       0.45      0.74      0.56       783
         kite       0.55      0.72      0.62       156
          car       0.12      0.54      0.20        61
         bird       0.63      0.67      0.65       126
       carrot       0.06      0.49      0.11        47
         boat       0.05      0.24      0.08        37
    surfboard       0.10      0.43      0.17        30
traffic light       0.22      0.54      0.31        24
     airplane       0.29      0.67      0.40        24
      giraffe       0.26      0.65      0.37        23

    micro avg       0.32      0.68      0.44      1311
    macro avg       0.27      0.57      0.35      1311
 weighted avg       0.42      0.68      0.51      1311
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For details on micro, macro, and weighted averaging, see the
<a class="reference external" href="https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html#sklearn.metrics.precision_recall_fscore_support">sklearn.metrics documentation</a>.</p>
</div>
</div>
<div class="section" id="per-sample-metrics">
<h3>Per-sample metrics<a class="headerlink" href="#per-sample-metrics" title="Permalink to this headline">Â¶</a></h3>
<p>In addition to standard aggregate metrics, when you pass an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code>
parameter to the evaluation routine, FiftyOne will populate helpful
task-specific information about your modelâ€™s predictions on each sample, such
as false negative/positive counts and per-sample accuracies.</p>
<p>Continuing with our example, letâ€™s use <a class="reference internal" href="using_views.html#using-views"><span class="std std-ref">dataset views</span></a> and
the <a class="reference internal" href="app.html#fiftyone-app"><span class="std std-ref">FiftyOne App</span></a> to leverage these sample metrics to
investigate the samples with the most false positive predictions in the
dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="c1"># Create a view that has samples with the most false positives first, and</span>
<span class="c1"># only includes false positive boxes in the `predictions` field</span>
<span class="n">view</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dataset</span>
    <span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">"eval_fp"</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">"predictions"</span><span class="p">,</span> <span class="n">F</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"fp"</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Visualize results in the App</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">view</span><span class="p">)</span>
</pre></div>
</div>
<img alt="quickstart-evaluate-detections" class="align-center" src="../_images/quickstart_evaluate_detections.gif"/>
<p><br/>
Notice anything wrong? The sample with the most false positives is a plate of
carrots where the entire plate has been boxed as a single example in the ground
truth while the model is generating predictions for individual carrots!</p>
<p>If youâ€™re familiar with <a class="reference external" href="https://cocodataset.org/#format-data">COCO format</a>
(which is recognized by
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
by default), youâ€™ll notice that the issue here is that the <code class="docutils literal notranslate"><span class="pre">iscrowd</span></code>
attribute of this ground truth annotation has been incorrectly set to <code class="docutils literal notranslate"><span class="pre">0</span></code>.
Resolving mistakes like these will provide a much more accurate picture of the
real performance of a model.</p>
</div>
<div class="section" id="confusion-matrices">
<span id="id2"></span><h3>Confusion matrices<a class="headerlink" href="#confusion-matrices" title="Permalink to this headline">Â¶</a></h3>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to work with confusion matrices in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
<p>When you use evaluation methods such as
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
that support confusion matrices, you can use the
<a class="reference internal" href="../api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults.plot_confusion_matrix" title="fiftyone.utils.eval.detection.DetectionResults.plot_confusion_matrix"><code class="xref py py-meth docutils literal notranslate"><span class="pre">plot_confusion_matrix()</span></code></a>
method to render responsive plots that can be attached to App instances to
interactively explore specific cases of your modelâ€™s performance:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Plot confusion matrix</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Connect to session</span>
<span class="n">session</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">plot</span><span class="p">)</span>
</pre></div>
</div>
<img alt="detection-evaluation" class="align-center" src="../_images/detection-evaluation.gif"/>
<p>In this setup, you can click on individual cells of the confusion matrix to
select the corresponding ground truth and/or predicted objects in the App. For
example, if you click on a diagonal cell of the confusion matrix, you will
see the true positive examples of that class in the App.</p>
<p>Likewise, whenever you modify the Sessionâ€™s view, either in the App or by
programmatically setting
<a class="reference internal" href="../api/fiftyone.core.session.html#fiftyone.core.session.Session.view" title="fiftyone.core.session.Session.view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">session.view</span></code></a>, the confusion matrix
is automatically updated to show the cell counts for only those objects that
are included in the current view.</p>
</div>
<div class="section" id="managing-evaluations">
<span id="id3"></span><h3>Managing evaluations<a class="headerlink" href="#managing-evaluations" title="Permalink to this headline">Â¶</a></h3>
<p>When you run an evaluation with an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> argument, the evaluation is
recorded on the dataset and you can retrieve information about it later, rename
it, delete it (along with any modifications to your dataset that were performed
by it), and <a class="reference internal" href="#load-evaluation-view"><span class="std std-ref">retrieve the view</span></a> that you evaluated
on using the following methods on your dataset:</p>
<ul class="simple">
<li><p><a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.list_evaluations" title="fiftyone.core.collections.SampleCollection.list_evaluations"><code class="xref py py-meth docutils literal notranslate"><span class="pre">list_evaluations()</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.get_evaluation_info" title="fiftyone.core.collections.SampleCollection.get_evaluation_info"><code class="xref py py-meth docutils literal notranslate"><span class="pre">get_evaluation_info()</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.load_evaluation_results" title="fiftyone.core.collections.SampleCollection.load_evaluation_results"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_evaluation_results()</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.load_evaluation_view" title="fiftyone.core.collections.SampleCollection.load_evaluation_view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_evaluation_view()</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.rename_evaluation" title="fiftyone.core.collections.SampleCollection.rename_evaluation"><code class="xref py py-meth docutils literal notranslate"><span class="pre">rename_evaluation()</span></code></a></p></li>
<li><p><a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.delete_evaluation" title="fiftyone.core.collections.SampleCollection.delete_evaluation"><code class="xref py py-meth docutils literal notranslate"><span class="pre">delete_evaluation()</span></code></a></p></li>
</ul>
<p>The example below demonstrates the basic interface:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># List evaluations you've run on a dataset</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">list_evaluations</span><span class="p">()</span>
<span class="c1"># ['eval']</span>

<span class="c1"># Print information about an evaluation</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">get_evaluation_info</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">))</span>

<span class="c1"># Load existing evaluation results and use them</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">load_evaluation_results</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>

<span class="c1"># Rename the evaluation</span>
<span class="c1"># This will automatically rename any evaluation fields on your dataset</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">rename_evaluation</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">,</span> <span class="s2">"still_eval"</span><span class="p">)</span>

<span class="c1"># Delete the evaluation</span>
<span class="c1"># This will remove any evaluation data that was populated on your dataset</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">delete_evaluation</span><span class="p">(</span><span class="s2">"still_eval"</span><span class="p">)</span>
</pre></div>
</div>
<p>The sections below discuss evaluating various types of predictions in more
detail.</p>
</div>
</div>
<div class="section" id="regressions">
<span id="evaluating-regressions"></span><h2>Regressions<a class="headerlink" href="#regressions" title="Permalink to this headline">Â¶</a></h2>
<p>You can use the
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_regressions" title="fiftyone.core.collections.SampleCollection.evaluate_regressions"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_regressions()</span></code></a>
method to evaluate the predictions of a regression model stored in a
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Regression" title="fiftyone.core.labels.Regression"><code class="xref py py-class docutils literal notranslate"><span class="pre">Regression</span></code></a> field of your dataset.</p>
<p>Invoking
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_regressions" title="fiftyone.core.collections.SampleCollection.evaluate_regressions"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_regressions()</span></code></a>
returns a <a class="reference internal" href="../api/fiftyone.utils.eval.regression.html#fiftyone.utils.eval.regression.RegressionResults" title="fiftyone.utils.eval.regression.RegressionResults"><code class="xref py py-class docutils literal notranslate"><span class="pre">RegressionResults</span></code></a> instance that provides a variety of methods for
evaluating your model.</p>
<p>In addition, when you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, helpful fields will be
populated on each sample that you can leverage via the
<a class="reference internal" href="app.html#fiftyone-app"><span class="std std-ref">FiftyOne App</span></a> to interactively explore the strengths and
weaknesses of your model on individual samples.</p>
<div class="section" id="simple-evaluation-default">
<h3>Simple evaluation (default)<a class="headerlink" href="#simple-evaluation-default" title="Permalink to this headline">Â¶</a></h3>
<p>By default,
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_regressions" title="fiftyone.core.collections.SampleCollection.evaluate_regressions"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_regressions()</span></code></a>
will evaluate each prediction by directly comparing its <code class="docutils literal notranslate"><span class="pre">value</span></code> to the
associated ground truth value.</p>
<p>You can explicitly request that simple evaluation be used by setting the
<code class="docutils literal notranslate"><span class="pre">method</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">"simple"</span></code>.</p>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a float <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> field will be
populated on each sample that records the error of that sampleâ€™s prediction
with respect to its ground truth value. By default, the squared error will be
computed, but you can customize this via the optional <code class="docutils literal notranslate"><span class="pre">metric</span></code> argument to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_regressions" title="fiftyone.core.collections.SampleCollection.evaluate_regressions"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_regressions()</span></code></a>,
which can take any value supported by
<a class="reference internal" href="../api/fiftyone.utils.eval.regression.html#fiftyone.utils.eval.regression.SimpleEvaluationConfig" title="fiftyone.utils.eval.regression.SimpleEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">SimpleEvaluationConfig</span></code></a>.</p>
<p>The example below demonstrates simple evaluation on the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart"><span class="std std-ref">quickstart dataset</span></a> with some fake regression
data added to it to demonstrate the workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span><span class="o">.</span><span class="n">select_fields</span><span class="p">()</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="c1"># Populate some fake regression + weather data</span>
<span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">ytrue</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">*</span> <span class="n">idx</span>
    <span class="n">ypred</span> <span class="o">=</span> <span class="n">ytrue</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">ytrue</span><span class="p">)</span>
    <span class="n">confidence</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">"ground_truth"</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Regression</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">ytrue</span><span class="p">)</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">"predictions"</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Regression</span><span class="p">(</span><span class="n">value</span><span class="o">=</span><span class="n">ypred</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="n">confidence</span><span class="p">)</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">"weather"</span><span class="p">]</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">"sunny"</span><span class="p">,</span> <span class="s2">"cloudy"</span><span class="p">,</span> <span class="s2">"rainy"</span><span class="p">])</span>
    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the predictions in the `predictions` field with respect to the</span>
<span class="c1"># values in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_regressions</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Print some standard regression evaluation metrics</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_metrics</span><span class="p">()</span>

<span class="c1"># Plot a scatterplot of the results colored by `weather` and scaled by</span>
<span class="c1"># `confidence`</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_results</span><span class="p">(</span><span class="n">labels</span><span class="o">=</span><span class="s2">"weather"</span><span class="p">,</span> <span class="n">sizes</span><span class="o">=</span><span class="s2">"predictions.confidence"</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Launch the App to explore</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Show the samples with the smallest regression error</span>
<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span>

<span class="c1"># Show the samples with the largest regression error</span>
<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>mean squared error        59.69
root mean squared error   7.73
mean absolute error       5.48
median absolute error     3.57
r2 score                  0.97
explained variance score  0.97
max error                 31.77
support                   200
</pre></div>
</div>
<img alt="regression-evaluation-plot" class="align-center" src="../_images/regression_evaluation_plot.png"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Did you know? You can
<a class="reference internal" href="plots.html#regression-plots"><span class="std std-ref">attach regression plots to the App</span></a> and
interactively explore them by selecting scatter points and/or modifying
your view in the App.</p>
</div>
</div>
</div>
<div class="section" id="classifications">
<span id="evaluating-classifications"></span><h2>Classifications<a class="headerlink" href="#classifications" title="Permalink to this headline">Â¶</a></h2>
<p>You can use the
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications" title="fiftyone.core.collections.SampleCollection.evaluate_classifications"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_classifications()</span></code></a>
method to evaluate the predictions of a classifier stored in a
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Classification" title="fiftyone.core.labels.Classification"><code class="xref py py-class docutils literal notranslate"><span class="pre">Classification</span></code></a> field of your dataset.</p>
<p>By default, the classifications will be treated as a generic multiclass
classification task, but you can specify other evaluation strategies such as
top-k accuracy or binary evaluation via the <code class="docutils literal notranslate"><span class="pre">method</span></code> parameter.</p>
<p>Invoking
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications" title="fiftyone.core.collections.SampleCollection.evaluate_classifications"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_classifications()</span></code></a>
returns a <a class="reference internal" href="../api/fiftyone.utils.eval.classification.html#fiftyone.utils.eval.classification.ClassificationResults" title="fiftyone.utils.eval.classification.ClassificationResults"><code class="xref py py-class docutils literal notranslate"><span class="pre">ClassificationResults</span></code></a> instance that provides a variety of methods
for generating various aggregate evaluation reports about your model.</p>
<p>In addition, when you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a number of helpful
fields will be populated on each sample that you can leverage via the
<a class="reference internal" href="app.html#fiftyone-app"><span class="std std-ref">FiftyOne App</span></a> to interactively explore the strengths and
weaknesses of your model on individual samples.</p>
<div class="section" id="id4">
<h3>Simple evaluation (default)<a class="headerlink" href="#id4" title="Permalink to this headline">Â¶</a></h3>
<p>By default,
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications" title="fiftyone.core.collections.SampleCollection.evaluate_classifications"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_classifications()</span></code></a>
will treat your classifications as generic multiclass predictions, and it will
evaluate each prediction by directly comparing its <code class="docutils literal notranslate"><span class="pre">label</span></code> to the associated
ground truth prediction.</p>
<p>You can explicitly request that simple evaluation be used by setting the
<code class="docutils literal notranslate"><span class="pre">method</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">"simple"</span></code>.</p>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a boolean <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> field will
be populated on each sample that records whether that sampleâ€™s prediction is
correct.</p>
<p>The example below demonstrates simple evaluation on the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-cifar10"><span class="std std-ref">CIFAR-10 dataset</span></a> with some fake predictions added
to it to demonstrate the workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"cifar10"</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">"test"</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># Create some test predictions by copying the ground truth labels into a</span>
<span class="c1"># new `predictions` field with 10% of the labels perturbed at random</span>
<span class="c1">#</span>

<span class="n">classes</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">distinct</span><span class="p">(</span><span class="s2">"ground_truth.label"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">jitter</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">val</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">fo</span><span class="o">.</span><span class="n">Classification</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">jitter</span><span class="p">(</span><span class="n">gt</span><span class="o">.</span><span class="n">label</span><span class="p">),</span> <span class="n">confidence</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">())</span>
    <span class="k">for</span> <span class="n">gt</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="s2">"ground_truth"</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">set_values</span><span class="p">(</span><span class="s2">"predictions"</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the predictions in the `predictions` field with respect to the</span>
<span class="c1"># labels in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_classifications</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval_simple"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Print a classification report</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>

<span class="c1"># Plot a confusion matrix</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">()</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Launch the App to explore</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># View only the incorrect predictions in the App</span>
<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">"eval_simple"</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

    airplane       0.91      0.90      0.91       118
  automobile       0.93      0.90      0.91       101
        bird       0.93      0.87      0.90       103
         cat       0.92      0.91      0.92        94
        deer       0.88      0.92      0.90       116
         dog       0.85      0.84      0.84        86
        frog       0.85      0.92      0.88        84
       horse       0.88      0.91      0.89        96
        ship       0.93      0.95      0.94        97
       truck       0.92      0.89      0.90       105

    accuracy                           0.90      1000
   macro avg       0.90      0.90      0.90      1000
weighted avg       0.90      0.90      0.90      1000
</pre></div>
</div>
<img alt="cifar10-simple-confusion-matrix" class="align-center" src="../_images/cifar10_simple_confusion_matrix.png"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to analyze models in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
</div>
<div class="section" id="top-k-evaluation">
<h3>Top-k evaluation<a class="headerlink" href="#top-k-evaluation" title="Permalink to this headline">Â¶</a></h3>
<p>Set the <code class="docutils literal notranslate"><span class="pre">method</span></code> parameter of
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications" title="fiftyone.core.collections.SampleCollection.evaluate_classifications"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_classifications()</span></code></a>
to <code class="docutils literal notranslate"><span class="pre">top-k</span></code> in order to use top-k matching to evaluate your classifications.</p>
<p>Under this strategy, predictions are deemed to be correct if the corresponding
ground truth label is within the top <code class="docutils literal notranslate"><span class="pre">k</span></code> predictions.</p>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a boolean <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> field will
be populated on each sample that records whether that sampleâ€™s prediction is
correct.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to use top-k evaluation, you must populate the <code class="docutils literal notranslate"><span class="pre">logits</span></code> field
of your predictions, and you must provide the list of corresponding class
labels via the <code class="docutils literal notranslate"><span class="pre">classes</span></code> parameter of
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications" title="fiftyone.core.collections.SampleCollection.evaluate_classifications"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_classifications()</span></code></a>.</p>
<p>Did you know? Many models from the <a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">Model Zoo</span></a>
provide support for storing logits for their predictions!</p>
</div>
<p>The example below demonstrates top-k evaluation on a
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-imagenet-sample"><span class="std std-ref">small ImageNet sample</span></a> with predictions
from a pre-trained model from the <a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">Model Zoo</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"imagenet-sample"</span><span class="p">,</span> <span class="n">dataset_name</span><span class="o">=</span><span class="s2">"top-k-eval-demo"</span>
<span class="p">)</span>

<span class="c1"># We need the list of class labels corresponding to the logits</span>
<span class="n">logits_classes</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">default_classes</span>

<span class="c1"># Add predictions (with logits) to 25 random samples</span>
<span class="n">predictions_view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">51</span><span class="p">)</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span><span class="s2">"resnet50-imagenet-torch"</span><span class="p">)</span>
<span class="n">predictions_view</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"predictions"</span><span class="p">,</span> <span class="n">store_logits</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">predictions_view</span><span class="p">)</span>

<span class="c1"># Evaluate the predictions in the `predictions` field with respect to the</span>
<span class="c1"># labels in the `ground_truth` field using top-5 accuracy</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">predictions_view</span><span class="o">.</span><span class="n">evaluate_classifications</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval_top_k"</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">"top-k"</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">logits_classes</span><span class="p">,</span>
    <span class="n">k</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get the 10 most common classes in the view</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">predictions_view</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">"ground_truth.label"</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>

<span class="c1"># Print a classification report for the top-10 classes</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>

<span class="c1"># Launch the App to explore</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># View only the incorrect predictions for the 10 most common classes</span>
<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">predictions_view</span>
    <span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">"ground_truth.label"</span><span class="p">)</span><span class="o">.</span><span class="n">is_in</span><span class="p">(</span><span class="n">classes</span><span class="p">))</span>
    <span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">"eval_top_k"</span><span class="p">)</span> <span class="o">==</span> <span class="kc">False</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<img alt="imagenet-top-k-eval" class="align-center" src="../_images/imagenet_top_k_eval.png"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to analyze models in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
</div>
<div class="section" id="binary-evaluation">
<h3>Binary evaluation<a class="headerlink" href="#binary-evaluation" title="Permalink to this headline">Â¶</a></h3>
<p>If your classifier is binary, set the <code class="docutils literal notranslate"><span class="pre">method</span></code> parameter of
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications" title="fiftyone.core.collections.SampleCollection.evaluate_classifications"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_classifications()</span></code></a>
to <code class="docutils literal notranslate"><span class="pre">"binary"</span></code> in order to access binary-specific evaluation information such
as precision-recall curves for your model.</p>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a string <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> field will
be populated on each sample that records whether the sample is a true positive,
false positive, true negative, or false negative.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to use binary evaluation, you must provide the
<code class="docutils literal notranslate"><span class="pre">(neg_label,</span> <span class="pre">pos_label)</span></code> for your model via the <code class="docutils literal notranslate"><span class="pre">classes</span></code> parameter of
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_classifications" title="fiftyone.core.collections.SampleCollection.evaluate_classifications"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_classifications()</span></code></a>.</p>
</div>
<p>The example below demonstrates binary evaluation on the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-cifar10"><span class="std std-ref">CIFAR-10 dataset</span></a> with some fake binary predictions
added to it to demonstrate the workflow:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># Load a small sample from the ImageNet dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"cifar10"</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">"test"</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># Binarize the ground truth labels to `cat` and `other`, and add</span>
<span class="c1"># predictions that are correct proportionally to their confidence</span>
<span class="c1">#</span>

<span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"other"</span><span class="p">,</span> <span class="s2">"cat"</span><span class="p">]</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">gt_label</span> <span class="o">=</span> <span class="s2">"cat"</span> <span class="k">if</span> <span class="n">sample</span><span class="o">.</span><span class="n">ground_truth</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="s2">"cat"</span> <span class="k">else</span> <span class="s2">"other"</span>

    <span class="n">confidence</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&gt;</span> <span class="n">confidence</span><span class="p">:</span>
        <span class="n">pred_label</span> <span class="o">=</span> <span class="s2">"cat"</span> <span class="k">if</span> <span class="n">gt_label</span> <span class="o">==</span> <span class="s2">"other"</span> <span class="k">else</span> <span class="s2">"other"</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">pred_label</span> <span class="o">=</span> <span class="n">gt_label</span>

    <span class="n">sample</span><span class="o">.</span><span class="n">ground_truth</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">gt_label</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">"predictions"</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Classification</span><span class="p">(</span>
        <span class="n">label</span><span class="o">=</span><span class="n">pred_label</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="n">confidence</span>
    <span class="p">)</span>

    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the predictions in the `predictions` field with respect to the</span>
<span class="c1"># labels in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_classifications</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval_binary"</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">"binary"</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Print a classification report</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>

<span class="c1"># Plot a PR curve</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_pr_curve</span><span class="p">()</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

       other       0.90      0.48      0.63       906
         cat       0.09      0.50      0.15        94

    accuracy                           0.48      1000
   macro avg       0.50      0.49      0.39      1000
weighted avg       0.83      0.48      0.59      1000
</pre></div>
</div>
<img alt="cifar10-binary-pr-curve" class="align-center" src="../_images/cifar10_binary_pr_curve.png"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to analyze models in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
</div>
</div>
<div class="section" id="detections">
<span id="evaluating-detections"></span><h2>Detections<a class="headerlink" href="#detections" title="Permalink to this headline">Â¶</a></h2>
<p>You can use the
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
method to evaluate the predictions of an object detection model stored in a
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detections" title="fiftyone.core.labels.Detections"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detections</span></code></a>, <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines" title="fiftyone.core.labels.Polylines"><code class="xref py py-class docutils literal notranslate"><span class="pre">Polylines</span></code></a>, or <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoints" title="fiftyone.core.labels.Keypoints"><code class="xref py py-class docutils literal notranslate"><span class="pre">Keypoints</span></code></a> field of your dataset or of a
temporal detection model stored in a <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetections" title="fiftyone.core.labels.TemporalDetections"><code class="xref py py-class docutils literal notranslate"><span class="pre">TemporalDetections</span></code></a> field of your
dataset.</p>
<p>Invoking
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
returns a <a class="reference internal" href="../api/fiftyone.utils.eval.detection.html#fiftyone.utils.eval.detection.DetectionResults" title="fiftyone.utils.eval.detection.DetectionResults"><code class="xref py py-class docutils literal notranslate"><span class="pre">DetectionResults</span></code></a> instance that provides a variety of methods for
generating various aggregate evaluation reports about your model.</p>
<p>In addition, when you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a number of helpful
fields will be populated on each sample and its predicted/ground truth
objects that you can leverage via the <a class="reference internal" href="app.html#fiftyone-app"><span class="std std-ref">FiftyOne App</span></a> to
interactively explore the strengths and weaknesses of your model on individual
samples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOne uses the <a class="reference internal" href="#evaluating-detections-coco"><span class="std std-ref">COCO-style</span></a> evaluation
by default, but
<a class="reference internal" href="#evaluating-detections-open-images"><span class="std std-ref">Open Images-style</span></a> evaluation is
also natively supported.</p>
</div>
<div class="section" id="supported-types">
<span id="evaluation-detection-types"></span><h3>Supported types<a class="headerlink" href="#supported-types" title="Permalink to this headline">Â¶</a></h3>
<p>The <a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
method supports all of the following task types:</p>
<ul class="simple">
<li><p><a class="reference internal" href="using_datasets.html#object-detection"><span class="std std-ref">Object detection</span></a></p></li>
<li><p><a class="reference internal" href="using_datasets.html#instance-segmentation"><span class="std std-ref">Instance segmentations</span></a></p></li>
<li><p><a class="reference internal" href="using_datasets.html#polylines"><span class="std std-ref">Polygon detection</span></a></p></li>
<li><p><a class="reference internal" href="using_datasets.html#keypoints"><span class="std std-ref">Keypoints</span></a></p></li>
<li><p><a class="reference internal" href="using_datasets.html#temporal-detection"><span class="std std-ref">Temporal detections</span></a></p></li>
<li><p><a class="reference internal" href="using_datasets.html#d-detections"><span class="std std-ref">3D detections</span></a></p></li>
</ul>
<p>The only difference between each task type is in how the IoU between objects is
calculated:</p>
<ul class="simple">
<li><p>For object detections, IoUs are computed between each pair of bounding boxes</p></li>
<li><p>For instance segmentations and polygons, IoUs are computed between the
polygonal shapes rather than their rectangular bounding boxes</p></li>
<li><p>For keypoint tasks,
<a class="reference external" href="https://cocodataset.org/#keypoints-eval">object keypoint similarity</a>
is computed for each pair of objects, using the extent of the ground truth
keypoints as a proxy for the area of the objectâ€™s bounding box, and
assuming uniform falloff (<span class="math notranslate nohighlight">\(\kappa\)</span>)</p></li>
<li><p>For temporal detections, IoU is computed between the 1D support of two
temporal segments</p></li>
</ul>
<p>For object detection tasks, the ground truth and predicted objects should be
stored in <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detections" title="fiftyone.core.labels.Detections"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detections</span></code></a> format.</p>
<p>For instance segmentation tasks, the ground truth and predicted objects should
be stored in <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detections" title="fiftyone.core.labels.Detections"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detections</span></code></a> format, and each <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detection" title="fiftyone.core.labels.Detection"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detection</span></code></a> instance should have its
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detection.mask" title="fiftyone.core.labels.Detection.mask"><code class="xref py py-attr docutils literal notranslate"><span class="pre">mask</span></code></a> attribute populated to
define the extent of the object within its bounding box.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>In order to use instance masks for IoU calculations, pass <code class="docutils literal notranslate"><span class="pre">use_masks=True</span></code>
to <a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>.</p>
</div>
<p>For polygon detection tasks, the ground truth and predicted objects should be
stored in <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines" title="fiftyone.core.labels.Polylines"><code class="xref py py-class docutils literal notranslate"><span class="pre">Polylines</span></code></a> format with their
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Polyline.filled" title="fiftyone.core.labels.Polyline.filled"><code class="xref py py-attr docutils literal notranslate"><span class="pre">filled</span></code></a> attribute set to
<code class="docutils literal notranslate"><span class="pre">True</span></code> to indicate that they represent closed polygons (as opposed to
polylines).</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you are evaluating polygons but would rather use bounding boxes rather
than the actual polygonal geometries for IoU calculations, you can pass
<code class="docutils literal notranslate"><span class="pre">use_boxes=True</span></code> to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>.</p>
</div>
<p>For keypoint tasks, each <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoint" title="fiftyone.core.labels.Keypoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Keypoint</span></code></a> instance must contain point arrays of equal
length and semantic ordering.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If a particular point is missing or not visible for a <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Keypoint" title="fiftyone.core.labels.Keypoint"><code class="xref py py-class docutils literal notranslate"><span class="pre">Keypoint</span></code></a> instance,
use nan values for its coordinates. <a class="reference internal" href="using_datasets.html#keypoints"><span class="std std-ref">See here</span></a> for more
information about structuring keypoints.</p>
</div>
<p>For temporal detection tasks, the ground truth and predicted objects should be
stored in <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetections" title="fiftyone.core.labels.TemporalDetections"><code class="xref py py-class docutils literal notranslate"><span class="pre">TemporalDetections</span></code></a> format.</p>
</div>
<div class="section" id="evaluation-patches-views">
<span id="evaluation-patches"></span><h3>Evaluation patches views<a class="headerlink" href="#evaluation-patches-views" title="Permalink to this headline">Â¶</a></h3>
<p>Once you have run
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
on a dataset, you can use
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.to_evaluation_patches" title="fiftyone.core.collections.SampleCollection.to_evaluation_patches"><code class="xref py py-meth docutils literal notranslate"><span class="pre">to_evaluation_patches()</span></code></a>
to transform the dataset (or a view into it) into a new view that contains one
sample for each true positive, false positive, and false negative example.</p>
<p>True positive examples will result in samples with both their ground truth and
predicted fields populated, while false positive/negative examples will only
have one of their corresponding predicted/ground truth fields populated,
respectively.</p>
<p>If multiple predictions are matched to a ground truth object (e.g., if the
evaluation protocol includes a crowd attribute), then all matched predictions
will be stored in the single sample along with the ground truth object.</p>
<p>Evaluation patches views also have top-level <code class="docutils literal notranslate"><span class="pre">type</span></code> and <code class="docutils literal notranslate"><span class="pre">iou</span></code> fields
populated based on the evaluation results for that example, as well as a
<code class="docutils literal notranslate"><span class="pre">sample_id</span></code> field recording the sample ID of the example, and a <code class="docutils literal notranslate"><span class="pre">crowd</span></code>
field if the evaluation protocol defines a crowd attribute.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Evaluation patches views generate patches for <strong>only</strong> the contents of the
current view, which may differ from the view on which the <code class="docutils literal notranslate"><span class="pre">eval_key</span></code>
evaluation was performed. This may exclude some labels that were evaluated
and/or include labels that were not evaluated.</p>
<p>If you would like to see patches for the exact view on which an
evaluation was performed, first call
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.load_evaluation_view" title="fiftyone.core.collections.SampleCollection.load_evaluation_view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_evaluation_view()</span></code></a>
to load the view and then convert to patches.</p>
</div>
<p>The example below demonstrates loading an evaluation patches view for the
results of an evaluation on the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart"><span class="std std-ref">quickstart dataset</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>

<span class="c1"># Evaluate `predictions` w.r.t. labels in `ground_truth` field</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span> <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span> <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Convert to evaluation patches</span>
<span class="n">eval_patches</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">to_evaluation_patches</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">eval_patches</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">eval_patches</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">"type"</span><span class="p">))</span>
<span class="c1"># {'fn': 246, 'fp': 4131, 'tp': 986}</span>

<span class="c1"># View patches in the App</span>
<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">eval_patches</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Dataset:     quickstart
Media type:  image
Num patches: 5363
Patch fields:
    filepath:     fiftyone.core.fields.StringField
    tags:         fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)
    metadata:     fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)
    predictions:  fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)
    ground_truth: fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)
    sample_id:    fiftyone.core.fields.StringField
    type:         fiftyone.core.fields.StringField
    iou:          fiftyone.core.fields.FloatField
    crowd:        fiftyone.core.fields.BooleanField
View stages:
    1. ToEvaluationPatches(eval_key='eval', config=None)
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Did you know? You can convert to evaluation patches view directly
<a class="reference internal" href="app.html#app-evaluation-patches"><span class="std std-ref">from the App</span></a>!</p>
</div>
<img alt="evaluation-patches" class="align-center" src="../_images/evaluation_patches.gif"/>
<p><br/>
Evaluation patches views are just like any other
<a class="reference internal" href="using_views.html#using-views"><span class="std std-ref">dataset view</span></a> in the sense that:</p>
<ul class="simple">
<li><p>You can append view stages via the <a class="reference internal" href="app.html#app-create-view"><span class="std std-ref">App view bar</span></a> or
<a class="reference internal" href="using_views.html#using-views"><span class="std std-ref">views API</span></a></p></li>
<li><p>Any modifications to ground truth or predicted label tags that you make via
the Appâ€™s <a class="reference internal" href="app.html#app-tagging"><span class="std std-ref">tagging menu</span></a> or via API methods like
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.tag_labels" title="fiftyone.core.collections.SampleCollection.tag_labels"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tag_labels()</span></code></a>
and <a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.untag_labels" title="fiftyone.core.collections.SampleCollection.untag_labels"><code class="xref py py-meth docutils literal notranslate"><span class="pre">untag_labels()</span></code></a>
will be reflected on the source dataset</p></li>
<li><p>Any modifications to the predicted or ground truth <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Label" title="fiftyone.core.labels.Label"><code class="xref py py-class docutils literal notranslate"><span class="pre">Label</span></code></a> elements in the
patches view that you make by iterating over the contents of the view or
calling
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.set_values" title="fiftyone.core.collections.SampleCollection.set_values"><code class="xref py py-meth docutils literal notranslate"><span class="pre">set_values()</span></code></a>
will be reflected on the source dataset</p></li>
<li><p>Calling <a class="reference internal" href="../api/fiftyone.core.patches.html#fiftyone.core.patches.EvaluationPatchesView.save" title="fiftyone.core.patches.EvaluationPatchesView.save"><code class="xref py py-meth docutils literal notranslate"><span class="pre">save()</span></code></a>
on an evaluation patches view (typically one that contains additional view
stages that filter or modify its contents) will sync any <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Label" title="fiftyone.core.labels.Label"><code class="xref py py-class docutils literal notranslate"><span class="pre">Label</span></code></a> edits or
deletions with the source dataset</p></li>
</ul>
<p>However, because evaluation patches views only contain a subset of the contents
of a <a class="reference internal" href="../api/fiftyone.core.sample.html#fiftyone.core.sample.Sample" title="fiftyone.core.sample.Sample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sample</span></code></a> from the source dataset, there are some differences in behavior
compared to non-patch views:</p>
<ul class="simple">
<li><p>Tagging or untagging patches themselves (as opposed to their labels) will
not affect the tags of the underlying <a class="reference internal" href="../api/fiftyone.core.sample.html#fiftyone.core.sample.Sample" title="fiftyone.core.sample.Sample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sample</span></code></a></p></li>
<li><p>Any new fields that you add to an evaluation patches view will not be added
to the source dataset</p></li>
</ul>
</div>
<div class="section" id="coco-style-evaluation-default-spatial">
<span id="evaluating-detections-coco"></span><h3>COCO-style evaluation (default spatial)<a class="headerlink" href="#coco-style-evaluation-default-spatial" title="Permalink to this headline">Â¶</a></h3>
<p>By default,
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
will use <a class="reference external" href="https://cocodataset.org/#detection-eval">COCO-style evaluation</a> to
analyze predictions when the specified label fields are <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detections" title="fiftyone.core.labels.Detections"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detections</span></code></a> or
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Polylines" title="fiftyone.core.labels.Polylines"><code class="xref py py-class docutils literal notranslate"><span class="pre">Polylines</span></code></a>.</p>
<p>You can also explicitly request that COCO-style evaluation be used by setting
the <code class="docutils literal notranslate"><span class="pre">method</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">"coco"</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOneâ€™s implementation of COCO-style evaluation matches the reference
implementation available via
<a class="reference external" href="https://github.com/cocodataset/cocoapi">pycocotools</a>.</p>
</div>
<div class="section" id="id5">
<h4>Overview<a class="headerlink" href="#id5" title="Permalink to this headline">Â¶</a></h4>
<p>When running COCO-style evaluation using
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>:</p>
<ul class="simple">
<li><p>Predicted and ground truth objects are matched using a specified IoU
threshold (default = 0.50). This threshold can be customized via the
<code class="docutils literal notranslate"><span class="pre">iou</span></code> parameter</p></li>
<li><p>By default, only objects with the same <code class="docutils literal notranslate"><span class="pre">label</span></code> will be matched. Classwise
matching can be disabled via the <code class="docutils literal notranslate"><span class="pre">classwise</span></code> parameter</p></li>
<li><p>Ground truth objects can have an <code class="docutils literal notranslate"><span class="pre">iscrowd</span></code> attribute that indicates
whether the annotation contains a crowd of objects. Multiple predictions
can be matched to crowd ground truth objects. The name of this attribute
can be customized by passing the optional <code class="docutils literal notranslate"><span class="pre">iscrowd</span></code> attribute of
<a class="reference internal" href="../api/fiftyone.utils.eval.coco.html#fiftyone.utils.eval.coco.COCOEvaluationConfig" title="fiftyone.utils.eval.coco.COCOEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">COCOEvaluationConfig</span></code></a> to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a></p></li>
</ul>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a number of helpful fields will be
populated on each sample and its predicted/ground truth objects:</p>
<ul>
<li><p>True positive (TP), false positive (FP), and false negative (FN) counts
for the each sample are saved in top-level fields of each sample:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_tp</span>
<span class="n">FP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fp</span>
<span class="n">FN</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fn</span>
</pre></div>
</div>
</li>
<li><p>The fields listed below are populated on each individual object instance;
these fields tabulate the TP/FP/FN status of the object, the ID of the
matching object (if any), and the matching IoU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="o">/</span><span class="n">FP</span><span class="o">/</span><span class="n">FN</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span>
      <span class="n">ID</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_id</span>
     <span class="n">IoU</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_iou</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="../api/fiftyone.utils.eval.coco.html#fiftyone.utils.eval.coco.COCOEvaluationConfig" title="fiftyone.utils.eval.coco.COCOEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">COCOEvaluationConfig</span></code></a> for complete descriptions of the optional
keyword arguments that you can pass to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
when running COCO-style evaluation.</p>
</div>
</div>
<div class="section" id="example-evaluation">
<h4>Example evaluation<a class="headerlink" href="#example-evaluation" title="Permalink to this headline">Â¶</a></h4>
<p>The example below demonstrates COCO-style detection evaluation on the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart"><span class="std std-ref">quickstart dataset</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the objects in the `predictions` field with respect to the</span>
<span class="c1"># objects in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get the 10 most common classes in the dataset</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">"ground_truth.detections.label"</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>

<span class="c1"># Print a classification report for the top-10 classes</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>

<span class="c1"># Print some statistics about the total TP/FP/FN counts</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"TP: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_tp"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"FP: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_fp"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"FN: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_fn"</span><span class="p">))</span>

<span class="c1"># Create a view that has samples with the most false positives first, and</span>
<span class="c1"># only includes false positive boxes in the `predictions` field</span>
<span class="n">view</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dataset</span>
    <span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">"eval_fp"</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">"predictions"</span><span class="p">,</span> <span class="n">F</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"fp"</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Visualize results in the App</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">view</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>               precision    recall  f1-score   support

       person       0.45      0.74      0.56       783
         kite       0.55      0.72      0.62       156
          car       0.12      0.54      0.20        61
         bird       0.63      0.67      0.65       126
       carrot       0.06      0.49      0.11        47
         boat       0.05      0.24      0.08        37
    surfboard       0.10      0.43      0.17        30
     airplane       0.29      0.67      0.40        24
traffic light       0.22      0.54      0.31        24
        bench       0.10      0.30      0.15        23

    micro avg       0.32      0.68      0.43      1311
    macro avg       0.26      0.54      0.32      1311
 weighted avg       0.42      0.68      0.50      1311
</pre></div>
</div>
<img alt="quickstart-evaluate-detections" class="align-center" src="../_images/quickstart_evaluate_detections.png"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to analyze models in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
</div>
<div class="section" id="map-mar-and-pr-curves">
<h4>mAP, mAR and PR curves<a class="headerlink" href="#map-mar-and-pr-curves" title="Permalink to this headline">Â¶</a></h4>
<p>You can compute mean average precision (mAP), mean average recall (mAR), and
precision-recall (PR) curves for your predictions by passing the
<code class="docutils literal notranslate"><span class="pre">compute_mAP=True</span></code> flag to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All mAP and mAR calculations are performed according to the
<a class="reference external" href="https://cocodataset.org/#detection-eval">COCO evaluation protocol</a>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Performs an IoU sweep so that mAP, mAR, and PR curves can be computed</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">compute_mAP</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mAP</span><span class="p">())</span>
<span class="c1"># 0.3957</span>

<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mAR</span><span class="p">())</span>
<span class="c1"># 0.5210</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_pr_curves</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">"person"</span><span class="p">,</span> <span class="s2">"kite"</span><span class="p">,</span> <span class="s2">"car"</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="coco-pr-curves" class="align-center" src="../_images/coco_pr_curves.png"/>
</div>
<div class="section" id="id6">
<h4>Confusion matrices<a class="headerlink" href="#id6" title="Permalink to this headline">Â¶</a></h4>
<p>You can also easily generate <a class="reference internal" href="#confusion-matrices"><span class="std std-ref">confusion matrices</span></a> for
the results of COCO-style evaluations.</p>
<p>In order for the confusion matrix to capture anything other than false
positive/negative counts, you will likely want to set the
<a class="reference internal" href="../api/fiftyone.utils.eval.coco.html#fiftyone.utils.eval.coco.COCOEvaluationConfig" title="fiftyone.utils.eval.coco.COCOEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">classwise</span></code></a> parameter
to <code class="docutils literal notranslate"><span class="pre">False</span></code> during evaluation so that predicted objects can be matched with
ground truth objects of different classes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>

<span class="c1"># Perform evaluation, allowing objects to be matched between classes</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span> <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span> <span class="n">classwise</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="c1"># Generate a confusion matrix for the specified classes</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">"car"</span><span class="p">,</span> <span class="s2">"truck"</span><span class="p">,</span> <span class="s2">"motorcycle"</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="coco-confusion-matrix" class="align-center" src="../_images/coco_confusion_matrix.png"/>
</div>
</div>
<div class="section" id="open-images-style-evaluation">
<span id="evaluating-detections-open-images"></span><h3>Open Images-style evaluation<a class="headerlink" href="#open-images-style-evaluation" title="Permalink to this headline">Â¶</a></h3>
<p>The <a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
method also supports
<a class="reference external" href="https://storage.googleapis.com/openimages/web/evaluation.html">Open Images-style evaluation</a>.</p>
<p>In order to run Open Images-style evaluation, simply set the <code class="docutils literal notranslate"><span class="pre">method</span></code>
parameter to <code class="docutils literal notranslate"><span class="pre">"open-images"</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOneâ€™s implementation of Open Images-style evaluation matches the
reference implementation available via the
<a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/object_detection">TF Object Detection API</a>.</p>
</div>
<div class="section" id="id8">
<h4>Overview<a class="headerlink" href="#id8" title="Permalink to this headline">Â¶</a></h4>
<p>Open Images-style evaluation provides additional features not found in
<a class="reference internal" href="#evaluating-detections-coco"><span class="std std-ref">COCO-style evaluation</span></a> that you may find
useful when evaluating your custom datasets.</p>
<p>The two primary differences are:</p>
<ul class="simple">
<li><p><strong>Non-exhaustive image labeling:</strong> positive and negative sample-level
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Classifications" title="fiftyone.core.labels.Classifications"><code class="xref py py-class docutils literal notranslate"><span class="pre">Classifications</span></code></a> fields can be provided to indicate which object classes
were considered when annotating the image. Predicted objects whose classes
are not included in the sample-level labels for a sample are ignored.
The names of these fields can be specified via the <code class="docutils literal notranslate"><span class="pre">pos_label_field</span></code> and
<code class="docutils literal notranslate"><span class="pre">neg_label_field</span></code> parameters</p></li>
<li><p><strong>Class hierarchies:</strong> If your dataset includes a
<a class="reference external" href="https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy_visualizer/circle.html">class hierarchy</a>,
you can configure this evaluation protocol to automatically expand ground
truth and/or predicted leaf classes so that all levels of the hierarchy can
be <a class="reference external" href="https://storage.googleapis.com/openimages/web/evaluation.html">correctly evaluated</a>.
You can provide a label hierarchy via the <code class="docutils literal notranslate"><span class="pre">hierarchy</span></code> parameter. By
default, if you provide a hierarchy, then image-level label fields and
ground truth detections will be expanded to incorporate parent classes
(child classes for negative image-level labels). You can disable this
feature by setting the <code class="docutils literal notranslate"><span class="pre">expand_gt_hierarchy</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Alternatively, you can expand predictions by setting the
<code class="docutils literal notranslate"><span class="pre">expand_pred_hierarchy</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
<p>In addition, note that:</p>
<ul class="simple">
<li><p>Like <a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/devkit_doc_08-May-2010.pdf">VOC-style evaluation</a>,
only one IoU (default = 0.5) is used to calculate mAP. You can customize
this value via the <code class="docutils literal notranslate"><span class="pre">iou</span></code> parameter</p></li>
<li><p>When dealing with crowd objects, Open Images-style evaluation dictates that
if a crowd is matched with multiple predictions, each counts as one true
positive when computing mAP</p></li>
</ul>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a number of helpful fields will be
populated on each sample and its predicted/ground truth objects:</p>
<ul>
<li><p>True positive (TP), false positive (FP), and false negative (FN) counts
for the each sample are saved in top-level fields of each sample:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_tp</span>
<span class="n">FP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fp</span>
<span class="n">FN</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fn</span>
</pre></div>
</div>
</li>
<li><p>The fields listed below are populated on each individual <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detection" title="fiftyone.core.labels.Detection"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detection</span></code></a>
instance; these fields tabulate the TP/FP/FN status of the object, the ID
of the matching object (if any), and the matching IoU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="o">/</span><span class="n">FP</span><span class="o">/</span><span class="n">FN</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span>
      <span class="n">ID</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_id</span>
     <span class="n">IoU</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_iou</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="../api/fiftyone.utils.eval.openimages.html#fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig" title="fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenImagesEvaluationConfig</span></code></a> for complete descriptions of the optional
keyword arguments that you can pass to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
when running Open Images-style evaluation.</p>
</div>
</div>
<div class="section" id="id9">
<h4>Example evaluation<a class="headerlink" href="#id9" title="Permalink to this headline">Â¶</a></h4>
<p>The example below demonstrates Open Images-style detection evaluation on the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart"><span class="std std-ref">quickstart dataset</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the objects in the `predictions` field with respect to the</span>
<span class="c1"># objects in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">"open-images"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get the 10 most common classes in the dataset</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">"ground_truth.detections.label"</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>

<span class="c1"># Print a classification report for the top-10 classes</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>

<span class="c1"># Print some statistics about the total TP/FP/FN counts</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"TP: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_tp"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"FP: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_fp"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"FN: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_fn"</span><span class="p">))</span>

<span class="c1"># Create a view that has samples with the most false positives first, and</span>
<span class="c1"># only includes false positive boxes in the `predictions` field</span>
<span class="n">view</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dataset</span>
    <span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">"eval_fp"</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">"predictions"</span><span class="p">,</span> <span class="n">F</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"fp"</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Visualize results in the App</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">view</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>               precision    recall  f1-score   support

       person       0.25      0.86      0.39       378
         kite       0.27      0.75      0.40        75
          car       0.18      0.80      0.29        61
         bird       0.20      0.51      0.28        51
       carrot       0.09      0.74      0.16        47
         boat       0.09      0.46      0.16        37
    surfboard       0.17      0.73      0.28        30
     airplane       0.36      0.83      0.50        24
traffic light       0.32      0.79      0.45        24
      giraffe       0.36      0.91      0.52        23

    micro avg       0.21      0.79      0.34       750
    macro avg       0.23      0.74      0.34       750
 weighted avg       0.23      0.79      0.36       750
</pre></div>
</div>
<img alt="quickstart-evaluate-detections-oi" class="align-center" src="../_images/quickstart_evaluate_detections_oi.png"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to analyze models in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
</div>
<div class="section" id="map-and-pr-curves">
<h4>mAP and PR curves<a class="headerlink" href="#map-and-pr-curves" title="Permalink to this headline">Â¶</a></h4>
<p>You can easily compute mean average precision (mAP) and precision-recall (PR)
curves using the results object returned by
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOneâ€™s implementation of Open Images-style evaluation matches the
reference implementation available via the
<a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/object_detection">TF Object Detection API</a>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">"open-images"</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mAP</span><span class="p">())</span>
<span class="c1"># 0.599</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_pr_curves</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">"person"</span><span class="p">,</span> <span class="s2">"dog"</span><span class="p">,</span> <span class="s2">"car"</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="oi-pr-curve" class="align-center" src="../_images/oi_pr_curve.png"/>
</div>
<div class="section" id="id11">
<h4>Confusion matrices<a class="headerlink" href="#id11" title="Permalink to this headline">Â¶</a></h4>
<p>You can also easily generate <a class="reference internal" href="#confusion-matrices"><span class="std std-ref">confusion matrices</span></a> for
the results of Open Images-style evaluations.</p>
<p>In order for the confusion matrix to capture anything other than false
positive/negative counts, you will likely want to set the
<a class="reference internal" href="../api/fiftyone.utils.eval.openimages.html#fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig" title="fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">classwise</span></code></a>
parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code> during evaluation so that predicted objects can be
matched with ground truth objects of different classes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>

<span class="c1"># Perform evaluation, allowing objects to be matched between classes</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">"open-images"</span><span class="p">,</span>
    <span class="n">classwise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Generate a confusion matrix for the specified classes</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">"car"</span><span class="p">,</span> <span class="s2">"truck"</span><span class="p">,</span> <span class="s2">"motorcycle"</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="oi-confusion-matrix" class="align-center" src="../_images/oi_confusion_matrix.png"/>
</div>
</div>
<div class="section" id="activitynet-style-evaluation-default-temporal">
<span id="evaluating-detections-activitynet"></span><h3>ActivityNet-style evaluation (default temporal)<a class="headerlink" href="#activitynet-style-evaluation-default-temporal" title="Permalink to this headline">Â¶</a></h3>
<p>By default,
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
will use
<a class="reference external" href="https://github.com/activitynet/ActivityNet/tree/master/Evaluation">ActivityNet-style temporal detection evaluation</a>.
to analyze predictions when the specified label fields are <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.TemporalDetections" title="fiftyone.core.labels.TemporalDetections"><code class="xref py py-class docutils literal notranslate"><span class="pre">TemporalDetections</span></code></a>.</p>
<p>You can also explicitly request that ActivityNet-style evaluation be used by setting
the <code class="docutils literal notranslate"><span class="pre">method</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">"activitynet"</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOneâ€™s implementation of ActivityNet-style evaluation matches the
reference implementation available via the
<a class="reference external" href="https://github.com/activitynet/ActivityNet/tree/master/Evaluation">ActivityNet API</a>.</p>
</div>
<div class="section" id="id12">
<h4>Overview<a class="headerlink" href="#id12" title="Permalink to this headline">Â¶</a></h4>
<p>When running ActivityNet-style evaluation using
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>:</p>
<ul class="simple">
<li><p>Predicted and ground truth segments are matched using a specified IoU
threshold (default = 0.50). This threshold can be customized via the
<code class="docutils literal notranslate"><span class="pre">iou</span></code> parameter</p></li>
<li><p>By default, only segments with the same <code class="docutils literal notranslate"><span class="pre">label</span></code> will be matched.
Classwise matching can be disabled by passing <code class="docutils literal notranslate"><span class="pre">classwise=False</span></code></p></li>
<li><p>mAP is computed by averaging over the same range of IoU values
<a class="reference internal" href="../integrations/coco.html#coco-map"><span class="std std-ref">used by COCO</span></a></p></li>
</ul>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a number of helpful fields will be
populated on each sample and its predicted/ground truth segments:</p>
<ul>
<li><p>True positive (TP), false positive (FP), and false negative (FN) counts
for the each sample are saved in top-level fields of each sample:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_tp</span>
<span class="n">FP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fp</span>
<span class="n">FN</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fn</span>
</pre></div>
</div>
</li>
<li><p>The fields listed below are populated on each individual temporal detection
segment; these fields tabulate the TP/FP/FN status of the segment, the ID
of the matching segment (if any), and the matching IoU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="o">/</span><span class="n">FP</span><span class="o">/</span><span class="n">FN</span><span class="p">:</span> <span class="n">segment</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span>
      <span class="n">ID</span><span class="p">:</span> <span class="n">segment</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_id</span>
     <span class="n">IoU</span><span class="p">:</span> <span class="n">segment</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_iou</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="../api/fiftyone.utils.eval.activitynet.html#fiftyone.utils.eval.activitynet.ActivityNetEvaluationConfig" title="fiftyone.utils.eval.activitynet.ActivityNetEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">ActivityNetEvaluationConfig</span></code></a> for complete descriptions of the optional
keyword arguments that you can pass to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
when running ActivityNet-style evaluation.</p>
</div>
</div>
<div class="section" id="id13">
<h4>Example evaluation<a class="headerlink" href="#id13" title="Permalink to this headline">Â¶</a></h4>
<p>The example below demonstrates ActivityNet-style temporal detection evaluation
on the <a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-activitynet-200"><span class="std std-ref">ActivityNet 200 dataset</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="kn">import</span> <span class="nn">random</span>

<span class="c1"># Load subset of ActivityNet 200</span>
<span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Bathing dog"</span><span class="p">,</span> <span class="s2">"Walking the dog"</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"activitynet-200"</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">"validation"</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Generate some fake predictions for this example</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">51</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">clone_sample_field</span><span class="p">(</span><span class="s2">"ground_truth"</span><span class="p">,</span> <span class="s2">"predictions"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">det</span> <span class="ow">in</span> <span class="n">sample</span><span class="o">.</span><span class="n">predictions</span><span class="o">.</span><span class="n">detections</span><span class="p">:</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">confidence</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
        <span class="n">det</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>

    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># Evaluate the segments in the `predictions` field with respect to the</span>
<span class="c1"># segments in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Print a classification report for the classes</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>

<span class="c1"># Print some statistics about the total TP/FP/FN counts</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"TP: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_tp"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"FP: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_fp"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"FN: </span><span class="si">%d</span><span class="s2">"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">"eval_fn"</span><span class="p">))</span>

<span class="c1"># Create a view that has samples with the most false positives first, and</span>
<span class="c1"># only includes false positive segments in the `predictions` field</span>
<span class="n">view</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dataset</span>
    <span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">"eval_fp"</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">"predictions"</span><span class="p">,</span> <span class="n">F</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span> <span class="o">==</span> <span class="s2">"fp"</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Visualize results in the App</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">view</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>                 precision    recall  f1-score   support

    Bathing dog       0.50      0.40      0.44         5
Walking the dog       0.50      0.60      0.55         5

      micro avg       0.50      0.50      0.50        10
      macro avg       0.50      0.50      0.49        10
   weighted avg       0.50      0.50      0.49        10
</pre></div>
</div>
<img alt="activitynet-evaluate-detections" class="align-center" src="../_images/activitynet_evaluate_detections.png"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to analyze models in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
</div>
<div class="section" id="id14">
<h4>mAP and PR curves<a class="headerlink" href="#id14" title="Permalink to this headline">Â¶</a></h4>
<p>You can compute mean average precision (mAP) and precision-recall (PR) curves
for your segments by passing the <code class="docutils literal notranslate"><span class="pre">compute_mAP=True</span></code> flag to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>All mAP calculations are performed according to the
<a class="reference external" href="https://github.com/activitynet/ActivityNet/tree/master/Evaluation">ActivityNet evaluation protocol</a>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># Load subset of ActivityNet 200</span>
<span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Bathing dog"</span><span class="p">,</span> <span class="s2">"Walking the dog"</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"activitynet-200"</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">"validation"</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Generate some fake predictions for this example</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">51</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">clone_sample_field</span><span class="p">(</span><span class="s2">"ground_truth"</span><span class="p">,</span> <span class="s2">"predictions"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">det</span> <span class="ow">in</span> <span class="n">sample</span><span class="o">.</span><span class="n">predictions</span><span class="o">.</span><span class="n">detections</span><span class="p">:</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">confidence</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
        <span class="n">det</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>

    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># Performs an IoU sweep so that mAP and PR curves can be computed</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span><span class="p">,</span>
    <span class="n">compute_mAP</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mAP</span><span class="p">())</span>
<span class="c1"># 0.367</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_pr_curves</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="activitynet-pr-curves" class="align-center" src="../_images/activitynet_pr_curves.png"/>
</div>
<div class="section" id="id15">
<h4>Confusion matrices<a class="headerlink" href="#id15" title="Permalink to this headline">Â¶</a></h4>
<p>You can also easily generate <a class="reference internal" href="#confusion-matrices"><span class="std std-ref">confusion matrices</span></a> for
the results of ActivityNet-style evaluations.</p>
<p>In order for the confusion matrix to capture anything other than false
positive/negative counts, you will likely want to set the
<code class="xref py py-class docutils literal notranslate"><span class="pre">classwise</span></code>
parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code> during evaluation so that predicted segments can be
matched with ground truth segments of different classes.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># Load subset of ActivityNet 200</span>
<span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">"Bathing dog"</span><span class="p">,</span> <span class="s2">"Grooming dog"</span><span class="p">,</span> <span class="s2">"Grooming horse"</span><span class="p">,</span> <span class="s2">"Walking the dog"</span><span class="p">]</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"activitynet-200"</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">"validation"</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Generate some fake predictions for this example</span>
<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">51</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">clone_sample_field</span><span class="p">(</span><span class="s2">"ground_truth"</span><span class="p">,</span> <span class="s2">"predictions"</span><span class="p">)</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">det</span> <span class="ow">in</span> <span class="n">sample</span><span class="o">.</span><span class="n">predictions</span><span class="o">.</span><span class="n">detections</span><span class="p">:</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">+=</span> <span class="n">random</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">det</span><span class="o">.</span><span class="n">support</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">det</span><span class="o">.</span><span class="n">confidence</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>
        <span class="n">det</span><span class="o">.</span><span class="n">label</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>

    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># Perform evaluation, allowing objects to be matched between classes</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span> <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span> <span class="n">classwise</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span>

<span class="c1"># Generate a confusion matrix for the specified classes</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
<img alt="activitynet-confusion-matrix" class="align-center" src="../_images/activitynet_confusion_matrix.png"/>
</div>
</div>
</div>
<div class="section" id="semantic-segmentations">
<span id="evaluating-segmentations"></span><h2>Semantic segmentations<a class="headerlink" href="#semantic-segmentations" title="Permalink to this headline">Â¶</a></h2>
<p>You can use the
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_segmentations" title="fiftyone.core.collections.SampleCollection.evaluate_segmentations"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_segmentations()</span></code></a>
method to evaluate the predictions of a semantic segmentation model stored in a
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Segmentation" title="fiftyone.core.labels.Segmentation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Segmentation</span></code></a> field of your dataset.</p>
<p>By default, the full segmentation masks will be evaluated at a pixel level, but
you can specify other evaluation strategies such as evaluating only boundary
pixels (see below for details).</p>
<p>Invoking
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_segmentations" title="fiftyone.core.collections.SampleCollection.evaluate_segmentations"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_segmentations()</span></code></a>
returns a <a class="reference internal" href="../api/fiftyone.utils.eval.segmentation.html#fiftyone.utils.eval.segmentation.SegmentationResults" title="fiftyone.utils.eval.segmentation.SegmentationResults"><code class="xref py py-class docutils literal notranslate"><span class="pre">SegmentationResults</span></code></a> instance that provides a variety of methods for
generating various aggregate evaluation reports about your model.</p>
<p>In addition, when you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a number of helpful
fields will be populated on each sample that you can leverage via the
<a class="reference internal" href="app.html#fiftyone-app"><span class="std std-ref">FiftyOne App</span></a> to interactively explore the strengths and
weaknesses of your model on individual samples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can <a class="reference internal" href="using_datasets.html#storing-mask-targets"><span class="std std-ref">store mask targets</span></a> for your
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Segmentation" title="fiftyone.core.labels.Segmentation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Segmentation</span></code></a> fields on your dataset so that you can view semantic labels
in the App and avoid having to manually specify the set of mask targets
each time you run
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_segmentations" title="fiftyone.core.collections.SampleCollection.evaluate_segmentations"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_segmentations()</span></code></a>
on a dataset.</p>
</div>
<div class="section" id="id16">
<h3>Simple evaluation (default)<a class="headerlink" href="#id16" title="Permalink to this headline">Â¶</a></h3>
<p>By default,
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_segmentations()</span></code></a>
will perform pixelwise evaluation of the segmentation masks, treating each
pixel as a multiclass classification.</p>
<p>Here are some things to keep in mind:</p>
<ul class="simple">
<li><p>If the size of a predicted mask does not match the ground truth mask, it is
resized to match the ground truth.</p></li>
<li><p>You can specify the optional <code class="docutils literal notranslate"><span class="pre">bandwidth</span></code> parameter to evaluate only along
the contours of the ground truth masks. By default, the entire masks are
evaluated.</p></li>
</ul>
<p>You can explicitly request that this strategy be used by setting the <code class="docutils literal notranslate"><span class="pre">method</span></code>
parameter to <code class="docutils literal notranslate"><span class="pre">"simple"</span></code>.</p>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, the accuracy, precision, and recall
of each sample is recorded in top-level fields of each sample:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span> Accuracy: sample.&lt;eval_key&gt;_accuracy
Precision: sample.&lt;eval_key&gt;_precision
   Recall: sample.&lt;eval_key&gt;_recall
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The mask values <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">#000000</span></code> are treated as a background class
for the purposes of computing evaluation metrics like precision and
recall.</p>
</div>
<p>The example below demonstrates segmentation evaluation by comparing the
masks generated by two DeepLabv3 models (with
<a class="reference internal" href="../model_zoo/models.html#model-zoo-deeplabv3-resnet50-coco-torch"><span class="std std-ref">ResNet50</span></a> and
<a class="reference internal" href="../model_zoo/models.html#model-zoo-deeplabv3-resnet101-coco-torch"><span class="std std-ref">ResNet101</span></a> backbones):</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># Load a few samples from COCO-2017</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"quickstart"</span><span class="p">,</span>
    <span class="n">dataset_name</span><span class="o">=</span><span class="s2">"segmentation-eval-demo"</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># The models are trained on the VOC classes</span>
<span class="n">CLASSES</span> <span class="o">=</span> <span class="p">(</span>
    <span class="s2">"background,aeroplane,bicycle,bird,boat,bottle,bus,car,cat,chair,cow,"</span> <span class="o">+</span>
    <span class="s2">"diningtable,dog,horse,motorbike,person,pottedplant,sheep,sofa,train,"</span> <span class="o">+</span>
    <span class="s2">"tvmonitor"</span>
<span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">default_mask_targets</span> <span class="o">=</span> <span class="p">{</span>
    <span class="n">idx</span><span class="p">:</span> <span class="n">label</span> <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">CLASSES</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">","</span><span class="p">))</span>
<span class="p">}</span>

<span class="c1"># Add DeepLabv3-ResNet101 predictions to dataset</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span><span class="s2">"deeplabv3-resnet101-coco-torch"</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"resnet101"</span><span class="p">)</span>

<span class="c1"># Add DeepLabv3-ResNet50 predictions to dataset</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span><span class="s2">"deeplabv3-resnet50-coco-torch"</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="s2">"resnet50"</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the masks w/ ResNet50 backbone, treating the masks w/ ResNet101</span>
<span class="c1"># backbone as "ground truth"</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_segmentations</span><span class="p">(</span>
    <span class="s2">"resnet50"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"resnet101"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval_simple"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get a sense for the per-sample variation in likeness</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Accuracy range: (</span><span class="si">%f</span><span class="s2">, </span><span class="si">%f</span><span class="s2">)"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">"eval_simple_accuracy"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Precision range: (</span><span class="si">%f</span><span class="s2">, </span><span class="si">%f</span><span class="s2">)"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">"eval_simple_precision"</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">"Recall range: (</span><span class="si">%f</span><span class="s2">, </span><span class="si">%f</span><span class="s2">)"</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">bounds</span><span class="p">(</span><span class="s2">"eval_simple_recall"</span><span class="p">))</span>

<span class="c1"># Print a classification report</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>

<span class="c1"># Visualize results in the App</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<img alt="evaluate-segmentations" class="align-center" src="../_images/evaluate_segmentations.gif"/>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The easiest way to analyze models in FiftyOne is via the
<a class="reference internal" href="app.html#app-model-evaluation-panel"><span class="std std-ref">Model Evaluation panel</span></a>!</p>
</div>
</div>
</div>
<div class="section" id="advanced-usage">
<span id="evaluation-advanced"></span><h2>Advanced usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline">Â¶</a></h2>
<div class="section" id="evaluating-views-into-your-dataset">
<span id="evaluating-views"></span><h3>Evaluating views into your dataset<a class="headerlink" href="#evaluating-views-into-your-dataset" title="Permalink to this headline">Â¶</a></h3>
<p>All evaluation methods are exposed on <a class="reference internal" href="../api/fiftyone.core.view.html#fiftyone.core.view.DatasetView" title="fiftyone.core.view.DatasetView"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetView</span></code></a> objects, which means that
you can define arbitrarily complex views into your datasets and run evaluation
on those.</p>
<p>For example, the snippet below evaluates only the medium-sized objects in a
dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">,</span> <span class="n">dataset_name</span><span class="o">=</span><span class="s2">"eval-demo"</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">compute_metadata</span><span class="p">()</span>

<span class="c1"># Create an expression that will match objects whose bounding boxes have</span>
<span class="c1"># areas between 32^2 and 96^2 pixels</span>
<span class="n">bbox_area</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">F</span><span class="p">(</span><span class="s2">"$metadata.width"</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="p">(</span><span class="s2">"bounding_box"</span><span class="p">)[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*</span>
    <span class="n">F</span><span class="p">(</span><span class="s2">"$metadata.height"</span><span class="p">)</span> <span class="o">*</span> <span class="n">F</span><span class="p">(</span><span class="s2">"bounding_box"</span><span class="p">)[</span><span class="mi">3</span><span class="p">]</span>
<span class="p">)</span>
<span class="n">medium_boxes</span> <span class="o">=</span> <span class="p">(</span><span class="mi">32</span> <span class="o">**</span> <span class="mi">2</span> <span class="o">&lt;</span> <span class="n">bbox_area</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">bbox_area</span> <span class="o">&lt;</span> <span class="mi">96</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span>

<span class="c1"># Create a view that contains only medium-sized objects</span>
<span class="n">medium_view</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dataset</span>
    <span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">"ground_truth"</span><span class="p">,</span> <span class="n">medium_boxes</span><span class="p">)</span>
    <span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">"predictions"</span><span class="p">,</span> <span class="n">medium_boxes</span><span class="p">)</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">medium_view</span><span class="p">)</span>

<span class="c1"># Evaluate the medium-sized objects</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">medium_view</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"ground_truth"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval_medium"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Print some aggregate metrics</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">metrics</span><span class="p">())</span>

<span class="c1"># View results in the App</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">medium_view</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you run evaluation on a complex view, donâ€™t worry, you can always
<a class="reference internal" href="#load-evaluation-view"><span class="std std-ref">load the view later</span></a>!</p>
</div>
</div>
<div class="section" id="loading-a-previous-evaluation-result">
<span id="load-evaluation-view"></span><h3>Loading a previous evaluation result<a class="headerlink" href="#loading-a-previous-evaluation-result" title="Permalink to this headline">Â¶</a></h3>
<p>You can view a list of evaluation keys for evaluations that you have previously
run on a dataset via
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.list_evaluations" title="fiftyone.core.collections.SampleCollection.list_evaluations"><code class="xref py py-meth docutils literal notranslate"><span class="pre">list_evaluations()</span></code></a>.</p>
<p>Evaluation keys are stored at the dataset-level, but if a particular evaluation
was run on a view into your dataset, you can use
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.load_evaluation_view" title="fiftyone.core.collections.SampleCollection.load_evaluation_view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_evaluation_view()</span></code></a>
to retrieve the exact view on which you evaluated:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="c1"># List available evaluations</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">list_evaluations</span><span class="p">()</span>
<span class="c1"># ["my_eval1", "my_eval2", ...]</span>

<span class="c1"># Load the view into the dataset on which `my_eval1` was run</span>
<span class="n">eval1_view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">load_evaluation_view</span><span class="p">(</span><span class="s2">"my_eval1"</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you have run multiple evaluations on a dataset, you can use the
<code class="code docutils literal notranslate"><span class="pre">select_fields</span></code> parameter of the
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.load_evaluation_view" title="fiftyone.core.collections.SampleCollection.load_evaluation_view"><code class="xref py py-meth docutils literal notranslate"><span class="pre">load_evaluation_view()</span></code></a>
method to hide any fields that were populated by other evaluation runs,
allowing you to, for example, focus on a specific set of evaluation results
in the App:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">load_dataset</span><span class="p">(</span><span class="o">...</span><span class="p">)</span>

<span class="c1"># Load a view that contains the results of evaluation `my_eval1` and</span>
<span class="c1"># hides all other evaluation data</span>
<span class="n">eval1_view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">load_evaluation_view</span><span class="p">(</span><span class="s2">"my_eval1"</span><span class="p">,</span> <span class="n">select_fields</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">eval1_view</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="evaluating-videos">
<span id="id17"></span><h3>Evaluating videos<a class="headerlink" href="#evaluating-videos" title="Permalink to this headline">Â¶</a></h3>
<p>All evaluation methods can be applied to frame-level labels in addition to
sample-level labels.</p>
<p>You can evaluate frame-level labels of a video dataset by adding the <code class="docutils literal notranslate"><span class="pre">frames</span></code>
prefix to the relevant prediction and ground truth frame fields.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>When evaluating frame-level labels, helpful statistics are tabulated at
both the sample- and frame-levels of your dataset. Refer to the
documentation of the relevant evaluation method for more details.</p>
</div>
<p>The example below demonstrates evaluating (mocked) frame-level detections on
the <a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart-video"><span class="std std-ref">quickstart-video dataset</span></a> from the
Dataset Zoo:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">"quickstart-video"</span><span class="p">,</span> <span class="n">dataset_name</span><span class="o">=</span><span class="s2">"video-eval-demo"</span>
<span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># Create some test predictions by copying the ground truth objects into a</span>
<span class="c1"># new `predictions` field of the frames with 10% of the labels perturbed at</span>
<span class="c1"># random</span>
<span class="c1">#</span>

<span class="n">classes</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">distinct</span><span class="p">(</span><span class="s2">"frames.detections.detections.label"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">jitter</span><span class="p">(</span><span class="n">val</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.10</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">classes</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">val</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample_gts</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="s2">"frames.detections"</span><span class="p">):</span>
    <span class="n">sample_predictions</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">frame_gts</span> <span class="ow">in</span> <span class="n">sample_gts</span><span class="p">:</span>
        <span class="n">sample_predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">fo</span><span class="o">.</span><span class="n">Detections</span><span class="p">(</span>
                <span class="n">detections</span><span class="o">=</span><span class="p">[</span>
                    <span class="n">fo</span><span class="o">.</span><span class="n">Detection</span><span class="p">(</span>
                        <span class="n">label</span><span class="o">=</span><span class="n">jitter</span><span class="p">(</span><span class="n">gt</span><span class="o">.</span><span class="n">label</span><span class="p">),</span>
                        <span class="n">bounding_box</span><span class="o">=</span><span class="n">gt</span><span class="o">.</span><span class="n">bounding_box</span><span class="p">,</span>
                        <span class="n">confidence</span><span class="o">=</span><span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">(),</span>
                    <span class="p">)</span>
                    <span class="k">for</span> <span class="n">gt</span> <span class="ow">in</span> <span class="n">frame_gts</span><span class="o">.</span><span class="n">detections</span>
                <span class="p">]</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="n">predictions</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample_predictions</span><span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">set_values</span><span class="p">(</span><span class="s2">"frames.predictions"</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the frame-level `predictions` against the frame-level</span>
<span class="c1"># `detections` objects</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">"frames.predictions"</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">"frames.detections"</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">"eval"</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Print a classification report</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>              precision    recall  f1-score   support

      person       0.76      0.93      0.84      1108
   road sign       0.90      0.94      0.92      2726
     vehicle       0.98      0.94      0.96      7511

   micro avg       0.94      0.94      0.94     11345
   macro avg       0.88      0.94      0.91     11345
weighted avg       0.94      0.94      0.94     11345
</pre></div>
</div>
<p>You can also view frame-level evaluation results as
<a class="reference internal" href="#evaluation-patches"><span class="std std-ref">evaluation patches</span></a> by first converting
<a class="reference internal" href="using_views.html#frame-views"><span class="std std-ref">to frames</span></a> and then <a class="reference internal" href="using_views.html#eval-patches-views"><span class="std std-ref">to patches</span></a>!</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Convert to frame evaluation patches</span>
<span class="n">frames</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">to_frames</span><span class="p">(</span><span class="n">sample_frames</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">frame_eval_patches</span> <span class="o">=</span> <span class="n">frames</span><span class="o">.</span><span class="n">to_evaluation_patches</span><span class="p">(</span><span class="s2">"eval"</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">frame_eval_patches</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">frame_eval_patches</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">"type"</span><span class="p">))</span>
<span class="c1"># {'tp': 10578, 'fn': 767, 'fp': 767}</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">frame_eval_patches</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>Dataset:     video-eval-demo
Media type:  image
Num patches: 12112
Patch fields:
    id:               fiftyone.core.fields.ObjectIdField
    sample_id:        fiftyone.core.fields.ObjectIdField
    frame_id:         fiftyone.core.fields.ObjectIdField
    filepath:         fiftyone.core.fields.StringField
    frame_number:     fiftyone.core.fields.FrameNumberField
    tags:             fiftyone.core.fields.ListField(fiftyone.core.fields.StringField)
    metadata:         fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.metadata.ImageMetadata)
    created_at:       fiftyone.core.fields.DateTimeField
    last_modified_at: fiftyone.core.fields.DateTimeField
    predictions:      fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)
    detections:       fiftyone.core.fields.EmbeddedDocumentField(fiftyone.core.labels.Detections)
    type:             fiftyone.core.fields.StringField
    iou:              fiftyone.core.fields.FloatField
    crowd:            fiftyone.core.fields.BooleanField
View stages:
    1. ToFrames(config=None)
    2. ToEvaluationPatches(eval_key='eval', config=None)
</pre></div>
</div>
</div>
</div>
<div class="section" id="custom-evaluation-backends">
<span id="id18"></span><h2>Custom evaluation backends<a class="headerlink" href="#custom-evaluation-backends" title="Permalink to this headline">Â¶</a></h2>
<p>If you would like to use an evaluation protocol that is not natively supported
by FiftyOne, you can follow the instructions below to implement an interface
for your protocol and then configure your environment so that FiftyOneâ€™s
evaluation methods will use it.</p>
</div>
<div class="section" id="evaluation-config">
<span id="id19"></span><h2>Evaluation config<a class="headerlink" href="#evaluation-config" title="Permalink to this headline">Â¶</a></h2>
<p>FiftyOne provides an evaluation config that you can use to either temporarily
or permanently configure the behavior of the evaluation API.</p>
<div class="section" id="viewing-your-config">
<h3>Viewing your config<a class="headerlink" href="#viewing-your-config" title="Permalink to this headline">Â¶</a></h3>
<p>You can print your current evaluation config at any time via the Python library
and the CLI:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>If you have customized your evaluation config via any of the methods
described below, printing your config is a convenient way to ensure that
the changes you made have taken effect as you expected.</p>
</div>
</div>
<div class="section" id="modifying-your-config">
<h3>Modifying your config<a class="headerlink" href="#modifying-your-config" title="Permalink to this headline">Â¶</a></h3>
<p>You can modify your evaluation config in a variety of ways. The following
sections describe these options in detail.</p>
<div class="section" id="order-of-precedence">
<h4>Order of precedence<a class="headerlink" href="#order-of-precedence" title="Permalink to this headline">Â¶</a></h4>
<p>The following order of precedence is used to assign values to your evaluation
config settings as runtime:</p>
<ol class="arabic simple">
<li><p>Config settings applied at runtime by directly editing
<code class="code docutils literal notranslate"><span class="pre">fiftyone.evaluation_config</span></code></p></li>
<li><p><code class="code docutils literal notranslate"><span class="pre">FIFTYONE_XXX</span></code> environment variables</p></li>
<li><p>Settings in your JSON config (<code class="code docutils literal notranslate"><span class="pre">~/.fiftyone/evaluation_config.json</span></code>)</p></li>
<li><p>The default config values</p></li>
</ol>
</div>
<div class="section" id="editing-your-json-config">
<h4>Editing your JSON config<a class="headerlink" href="#editing-your-json-config" title="Permalink to this headline">Â¶</a></h4>
<p>You can permanently customize your evaluation config by creating a
<code class="code docutils literal notranslate"><span class="pre">~/.fiftyone/evaluation_config.json</span></code> file on your machine. The JSON file may
contain any desired subset of config fields that you wish to customize.</p>
<p>For example, the following config JSON file declares a new <code class="code docutils literal notranslate"><span class="pre">custom</span></code> detection
evaluation backend without changing any other default config settings:</p>
<div class="highlight-json notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
<span class="w">    </span><span class="nt">"default_detection_backend"</span><span class="p">:</span><span class="w"> </span><span class="s2">"custom"</span><span class="p">,</span>
<span class="w">    </span><span class="nt">"detection_backends"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">        </span><span class="nt">"custom"</span><span class="p">:</span><span class="w"> </span><span class="p">{</span>
<span class="w">            </span><span class="nt">"config_cls"</span><span class="p">:</span><span class="w"> </span><span class="s2">"path.to.your.CustomDetectionEvaluationConfig"</span>
<span class="w">        </span><span class="p">}</span>
<span class="w">    </span><span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>When <code class="code docutils literal notranslate"><span class="pre">fiftyone</span></code> is imported, any options from your JSON config are merged into
the default config, as per the order of precedence described above.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>You can customize the location from which your JSON config is read by
setting the <code class="code docutils literal notranslate"><span class="pre">FIFTYONE_EVALUATION_CONFIG_PATH</span></code> environment variable.</p>
</div>
</div>
<div class="section" id="setting-environment-variables">
<h4>Setting environment variables<a class="headerlink" href="#setting-environment-variables" title="Permalink to this headline">Â¶</a></h4>
<p>Evaluation config settings may be customized on a per-session basis by setting
the <code class="code docutils literal notranslate"><span class="pre">FIFTYONE_&lt;TYPE&gt;_XXX</span></code> environment variable(s) for the desired config
settings, where <code class="code docutils literal notranslate"><span class="pre">&lt;TYPE&gt;</span></code> can be <code class="code docutils literal notranslate"><span class="pre">REGRESSION</span></code>, <code class="code docutils literal notranslate"><span class="pre">CLASSIFICATION</span></code>, <code class="code docutils literal notranslate"><span class="pre">DETECTION</span></code>, or
<code class="code docutils literal notranslate"><span class="pre">SEGMENTATION</span></code>.</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">FIFTYONE_DEFAULT_&lt;TYPE&gt;_BACKEND</span></code> environment variables allows you to
configure your default backend:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">FIFTYONE_DEFAULT_DETECTION_BACKEND</span><span class="o">=</span>coco
</pre></div>
</div>
<p>You can declare parameters for specific evaluation backends by setting
environment variables of the form <code class="code docutils literal notranslate"><span class="pre">FIFTYONE_&lt;TYPE&gt;_&lt;BACKEND&gt;_&lt;PARAMETER&gt;</span></code>. Any
settings that you declare in this way will be passed as keyword arguments to
methods like
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
whenever the corresponding backend is in use:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">FIFTYONE_DETECTION_COCO_ISCROWD</span><span class="o">=</span>is_crowd
</pre></div>
</div>
<p>The <code class="code docutils literal notranslate"><span class="pre">FIFTYONE_&lt;TYPE&gt;_BACKENDS</span></code> environment variables can be set to a
<code class="code docutils literal notranslate"><span class="pre">list,of,backends</span></code> that you want to expose in your session, which may exclude
native backends and/or declare additional custom backends whose parameters are
defined via additional config modifications of any kind:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">FIFTYONE_DETECTION_BACKENDS</span><span class="o">=</span>custom,coco,open-images
</pre></div>
</div>
<p>When declaring new backends, you can include <code class="code docutils literal notranslate"><span class="pre">*</span></code> to append new backend(s)
without omitting or explicitly enumerating the builtin backends. For example,
you can add a <code class="code docutils literal notranslate"><span class="pre">custom</span></code> detection evaluation backend as follows:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">FIFTYONE_DETECTION_BACKENDS</span><span class="o">=</span>*,custom
<span class="nb">export</span><span class="w"> </span><span class="nv">FIFTYONE_DETECTION_CUSTOM_CONFIG_CLS</span><span class="o">=</span>your.custom.DetectionEvaluationConfig
</pre></div>
</div>
</div>
<div class="section" id="modifying-your-config-in-code">
<h4>Modifying your config in code<a class="headerlink" href="#modifying-your-config-in-code" title="Permalink to this headline">Â¶</a></h4>
<p>You can dynamically modify your evaluation config at runtime by directly
editing the <code class="code docutils literal notranslate"><span class="pre">fiftyone.evaluation_config</span></code> object.</p>
<p>Any changes to your evaluation config applied via this manner will immediately
take effect in all subsequent calls to <code class="code docutils literal notranslate"><span class="pre">fiftyone.evaluation_config</span></code> during your
current session.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>

<span class="n">fo</span><span class="o">.</span><span class="n">evaluation_config</span><span class="o">.</span><span class="n">default_detection_backend</span> <span class="o">=</span> <span class="s2">"custom"</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>
</article>
</div>

</div>
</div>
<div class="pytorch-content-right" id="pytorch-content-right">
<div class="pytorch-right-menu" id="pytorch-right-menu">
<div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
<ul>
<li><a class="reference internal" href="#">Evaluating Models</a><ul>
<li><a class="reference internal" href="#overview">Overview</a><ul>
<li><a class="reference internal" href="#model-evaluation-panel-sub-new">Model Evaluation panel <strong style="color: hsl(25, 100%, 51%); font-size: 0.85em; vertical-align: top">NEW</strong></a></li>
<li><a class="reference internal" href="#per-class-metrics">Per-class metrics</a></li>
<li><a class="reference internal" href="#per-sample-metrics">Per-sample metrics</a></li>
<li><a class="reference internal" href="#confusion-matrices">Confusion matrices</a></li>
<li><a class="reference internal" href="#managing-evaluations">Managing evaluations</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regressions">Regressions</a><ul>
<li><a class="reference internal" href="#simple-evaluation-default">Simple evaluation (default)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#classifications">Classifications</a><ul>
<li><a class="reference internal" href="#id4">Simple evaluation (default)</a></li>
<li><a class="reference internal" href="#top-k-evaluation">Top-k evaluation</a></li>
<li><a class="reference internal" href="#binary-evaluation">Binary evaluation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#detections">Detections</a><ul>
<li><a class="reference internal" href="#supported-types">Supported types</a></li>
<li><a class="reference internal" href="#evaluation-patches-views">Evaluation patches views</a></li>
<li><a class="reference internal" href="#coco-style-evaluation-default-spatial">COCO-style evaluation (default spatial)</a><ul>
<li><a class="reference internal" href="#id5">Overview</a></li>
<li><a class="reference internal" href="#example-evaluation">Example evaluation</a></li>
<li><a class="reference internal" href="#map-mar-and-pr-curves">mAP, mAR and PR curves</a></li>
<li><a class="reference internal" href="#id6">Confusion matrices</a></li>
</ul>
</li>
<li><a class="reference internal" href="#open-images-style-evaluation">Open Images-style evaluation</a><ul>
<li><a class="reference internal" href="#id8">Overview</a></li>
<li><a class="reference internal" href="#id9">Example evaluation</a></li>
<li><a class="reference internal" href="#map-and-pr-curves">mAP and PR curves</a></li>
<li><a class="reference internal" href="#id11">Confusion matrices</a></li>
</ul>
</li>
<li><a class="reference internal" href="#activitynet-style-evaluation-default-temporal">ActivityNet-style evaluation (default temporal)</a><ul>
<li><a class="reference internal" href="#id12">Overview</a></li>
<li><a class="reference internal" href="#id13">Example evaluation</a></li>
<li><a class="reference internal" href="#id14">mAP and PR curves</a></li>
<li><a class="reference internal" href="#id15">Confusion matrices</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#semantic-segmentations">Semantic segmentations</a><ul>
<li><a class="reference internal" href="#id16">Simple evaluation (default)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#advanced-usage">Advanced usage</a><ul>
<li><a class="reference internal" href="#evaluating-views-into-your-dataset">Evaluating views into your dataset</a></li>
<li><a class="reference internal" href="#loading-a-previous-evaluation-result">Loading a previous evaluation result</a></li>
<li><a class="reference internal" href="#evaluating-videos">Evaluating videos</a></li>
</ul>
</li>
<li><a class="reference internal" href="#custom-evaluation-backends">Custom evaluation backends</a></li>
<li><a class="reference internal" href="#evaluation-config">Evaluation config</a><ul>
<li><a class="reference internal" href="#viewing-your-config">Viewing your config</a></li>
<li><a class="reference internal" href="#modifying-your-config">Modifying your config</a><ul>
<li><a class="reference internal" href="#order-of-precedence">Order of precedence</a></li>
<li><a class="reference internal" href="#editing-your-json-config">Editing your JSON config</a></li>
<li><a class="reference internal" href="#setting-environment-variables">Setting environment variables</a></li>
<li><a class="reference internal" href="#modifying-your-config-in-code">Modifying your config in code</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</section>
</div>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
<script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
<script src="../_static/js/voxel51-website.js"></script>
<script src="../_static/js/custom.js"></script>
<script src="../_static/js/vendor/popper.min.js" type="text/javascript"></script>
<script src="../_static/js/vendor/bootstrap.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Begin Footer -->


<!-- End Footer -->
<!-- Begin Mobile Menu -->

<!-- End Mobile Menu -->
<script src="../_static/js/vendor/anchor.min.js" type="text/javascript"></script>
<script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>