
<!DOCTYPE html>

<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en"> <!--<![endif]-->
<head>
<meta charset="utf-8"/>
<meta content="width=device-width, initial-scale=1.0" name="viewport"/>
<title>OpenCLIP Integration â€” FiftyOne 1.3.0 documentation</title>
<link href="../_static/favicon.ico" rel="shortcut icon"/>
<link href="../_static/css/theme.css" rel="stylesheet" type="text/css"/>
<!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
<link href="../_static/css/voxel51-website.css" rel="stylesheet" type="text/css"/>
<link href="../_static/css/custom.css" rel="stylesheet" type="text/css"/>
<link href="../genindex.html" rel="index" title="Index"/>
<link href="../search.html" rel="search" title="Search"/>
<link href="pytorch_hub.html" rel="next" title="PyTorch Hub Integration"/>
<link href="super_gradients.html" rel="prev" title="Super Gradients Integration"/>
<meta content="https://voxel51.com/wp-content/uploads/2024/03/3.24_webpages_Home_AV.png" property="og:image">
<link href="https://fonts.googleapis.com/css?family=Palanquin:400,600,700,800" rel="stylesheet"/>
<link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css" rel="stylesheet"/>
<script src="https://tag.clearbitscripts.com/v1/pk_b9ed71c8234edd4f77326bcbfab5a4ca/tags.js"></script>
<script src="../_static/js/modernizr.min.js"></script>
</meta></head>

<body class="pytorch-body">
<div class="table-of-contents-link-wrapper">
<span>Table of Contents</span>
<a class="toggle-table-of-contents" data-behavior="toggle-table-of-contents" href="#"></a>
</div>
<nav class="pytorch-left-menu" data-toggle="wy-nav-shift" id="pytorch-left-menu">
<div class="pytorch-side-scroll">
<div aria-label="main navigation" class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation">
<div class="pytorch-left-menu-search">
<div role="search">
<form action="../search.html" class="wy-form" id="rtd-search-form" method="get">
<input name="q" placeholder="Search Docs" type="text"/>
<input name="check_keywords" type="hidden" value="yes"/>
<input name="area" type="hidden" value="default"/>
</form>
</div>
</div>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../teams/index.html">FiftyOne Teams ðŸš€</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/index.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheets/index.html">Cheat Sheets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_zoo/index.html">Dataset Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brain.html">FiftyOne Brain</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins/index.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/fiftyone.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deprecation.html">Deprecation Notices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>
</div>
</div>
</nav>
<div class="pytorch-container">
<div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
<div class="pytorch-breadcrumbs-wrapper">
<div aria-label="breadcrumbs navigation" role="navigation">
<ul class="pytorch-breadcrumbs">
<li>
<a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>
<li><a href="index.html">FiftyOne Integrations</a> &gt;</li>
<li>OpenCLIP Integration</li>
<li class="pytorch-breadcrumbs-aside">
</li>
</ul>
</div>
</div>
<div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Contents
        </div>
</div>
<section class="pytorch-content-wrap" data-toggle="wy-nav-shift" id="pytorch-content-wrap">
<div class="pytorch-content-left">
<div class="rst-content style-external-links">
<div class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article" role="main">
<article class="pytorch-article" id="pytorch-article" itemprop="articleBody">
<div class="section" id="openclip-integration">
<span id="id1"></span><h1>OpenCLIP Integration<a class="headerlink" href="#openclip-integration" title="Permalink to this headline">Â¶</a></h1>
<p>FiftyOne integrates natively with the
<a class="reference external" href="https://github.com/mlfoundations/open_clip">OpenCLIP</a> library, an open
source implementation of OpenAIâ€™s CLIP (Contrastive Language-Image
Pre-training) model that you can use to run inference on your FiftyOne datasets
with a few lines of code!</p>
<div class="section" id="setup">
<span id="openclip-setup"></span><h2>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">Â¶</a></h2>
<p>To get started with OpenCLIP, install the <code class="code docutils literal notranslate"><span class="pre">open_clip_torch</span></code> package:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>open_clip_torch

<span class="c1"># May also be needed</span>
pip<span class="w"> </span>install<span class="w"> </span>timm<span class="w"> </span>--upgrade
</pre></div>
</div>
</div>
<div class="section" id="model-zoo">
<span id="openclip-model-zoo"></span><h2>Model zoo<a class="headerlink" href="#model-zoo" title="Permalink to this headline">Â¶</a></h2>
<p>You can load the original ViT-B-32 OpenAI pretrained model from the
<a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">FiftyOne Model Zoo</span></a> as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span><span class="s2">"open-clip-torch"</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also specify different model architectures and pretrained weights by
passing in optional parameters. Pretrained models can be loaded directly from
OpenCLIP or from
<a class="reference external" href="https://huggingface.co/docs/hub/models-the-hub">Hugging Faceâ€™s Model Hub</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">rn50</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">clip_model</span><span class="o">=</span><span class="s2">"RN50"</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="s2">"cc12m"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">meta_clip</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">clip_model</span><span class="o">=</span><span class="s2">"ViT-B-32-quickgelu"</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="s2">"metaclip_400m"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">eva_clip</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">clip_model</span><span class="o">=</span><span class="s2">"EVA02-B-16"</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="s2">"merged2b_s8b_b131k"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">clipa</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">clip_model</span><span class="o">=</span><span class="s2">"hf-hub:UCSC-VLAA/ViT-L-14-CLIPA-datacomp1B"</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">siglip</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">clip_model</span><span class="o">=</span><span class="s2">"hf-hub:timm/ViT-B-16-SigLIP"</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="s2">""</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="inference">
<span id="openclip-inference"></span><h2>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">Â¶</a></h2>
<p>When running inference with OpenCLIP, you can specify a text prompt to help
guide the model towards a solution as well as only specify a certain number of
classes to output during zero shot classification.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>While OpenCLIP models are typically set to train mode by default, the FiftyOne
integration sets the model to eval mode before running inference.</p>
</div>
<p>For example we can run inference as such:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">text_prompt</span><span class="o">=</span><span class="s2">"A photo of a"</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">"person"</span><span class="p">,</span> <span class="s2">"dog"</span><span class="p">,</span> <span class="s2">"cat"</span><span class="p">,</span> <span class="s2">"bird"</span><span class="p">,</span> <span class="s2">"car"</span><span class="p">,</span> <span class="s2">"tree"</span><span class="p">,</span> <span class="s2">"chair"</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">"clip_predictions"</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<img alt="zero-shot-classification-example" class="align-center" src="../_images/zsc-openclip.png"/>
</div>
<div class="section" id="embeddings">
<span id="openclip-embeddings"></span><h2>Embeddings<a class="headerlink" href="#embeddings" title="Permalink to this headline">Â¶</a></h2>
<p>Another application of OpenCLIP is
<a class="reference internal" href="../brain.html#brain-embeddings-visualization"><span class="std std-ref">embeddings visualization</span></a>.</p>
<p>For example, letâ€™s compare the embeddings of the original OpenAI CLIP model to
MetaCLIP. Weâ€™ll also perform a quick zero shot classification to color the
embeddings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.brain</span> <span class="k">as</span> <span class="nn">fob</span>

<span class="n">meta_clip</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">clip_model</span><span class="o">=</span><span class="s2">"ViT-B-32-quickgelu"</span><span class="p">,</span>
    <span class="n">pretrained</span><span class="o">=</span><span class="s2">"metaclip_400m"</span><span class="p">,</span>
    <span class="n">text_prompt</span><span class="o">=</span><span class="s2">"A photo of a"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">meta_clip</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">"meta_clip_classification"</span><span class="p">)</span>

<span class="n">fob</span><span class="o">.</span><span class="n">compute_visualization</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">meta_clip</span><span class="p">,</span>
    <span class="n">brain_key</span><span class="o">=</span><span class="s2">"meta_clip"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">openai_clip</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">text_prompt</span><span class="o">=</span><span class="s2">"A photo of a"</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">openai_clip</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">"openai_clip_classifications"</span><span class="p">)</span>

<span class="n">fob</span><span class="o">.</span><span class="n">compute_visualization</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="n">openai_clip</span><span class="p">,</span>
    <span class="n">brain_key</span><span class="o">=</span><span class="s2">"openai_clip"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Here is the final result!</p>
<img alt="clip-compare" class="align-center" src="../_images/clip-compare.gif"/>
</div>
<div class="section" id="text-similarity-search">
<span id="openclip-text-similarity-search"></span><h2>Text similarity search<a class="headerlink" href="#text-similarity-search" title="Permalink to this headline">Â¶</a></h2>
<p>OpenCLIP can also be used for
<a class="reference internal" href="../brain.html#brain-similarity-text"><span class="std std-ref">text similarity search</span></a>.</p>
<p>To use a specific pretrained-checkpoint pair for text similarity search, pass
these in as a  dictionary via the <code class="code docutils literal notranslate"><span class="pre">model_kwargs</span></code> argument to
<code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_similarity()</span></code>.</p>
<p>For example, for MetaCLIP, we can do the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">import</span> <span class="nn">fiftyone.brain</span> <span class="k">as</span> <span class="nn">fob</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">"quickstart"</span><span class="p">)</span>

<span class="n">model_kwargs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">"clip_model"</span><span class="p">:</span> <span class="s2">"ViT-B-32-quickgelu"</span><span class="p">,</span>
    <span class="s2">"pretrained"</span><span class="p">:</span> <span class="s2">"metaclip_400m"</span><span class="p">,</span>
    <span class="s2">"text_prompt"</span><span class="p">:</span> <span class="s2">"A photo of a"</span><span class="p">,</span>
<span class="p">}</span>

<span class="n">fob</span><span class="o">.</span><span class="n">compute_similarity</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">"open-clip-torch"</span><span class="p">,</span>
    <span class="n">model_kwargs</span><span class="o">=</span><span class="n">model_kwargs</span><span class="p">,</span>
    <span class="n">brain_key</span><span class="o">=</span><span class="s2">"sim_metaclip"</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>You can then search by text similarity in Python via the
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.sort_by_similarity" title="fiftyone.core.collections.SampleCollection.sort_by_similarity"><code class="xref py py-meth docutils literal notranslate"><span class="pre">sort_by_similarity()</span></code></a>
stage as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">query</span> <span class="o">=</span> <span class="s2">"kites flying in the sky"</span>

<span class="n">view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sort_by_similarity</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">brain_key</span><span class="o">=</span><span class="s2">"sim_metaclip"</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Did you know? You can also perform text similarity queries directly
<a class="reference internal" href="../user_guide/app.html#app-text-similarity"><span class="std std-ref">in the App</span></a>!</p>
</div>
</div>
</div>
</article>
</div>

</div>
</div>
<div class="pytorch-content-right" id="pytorch-content-right">
<div class="pytorch-right-menu" id="pytorch-right-menu">
<div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
<ul>
<li><a class="reference internal" href="#">OpenCLIP Integration</a><ul>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#model-zoo">Model zoo</a></li>
<li><a class="reference internal" href="#inference">Inference</a></li>
<li><a class="reference internal" href="#embeddings">Embeddings</a></li>
<li><a class="reference internal" href="#text-similarity-search">Text similarity search</a></li>
</ul>
</li>
</ul>
</div>
</div>
</div>
</section>
</div>
<script data-url_root="../" id="documentation_options" src="../_static/documentation_options.js" type="text/javascript"></script>
<script src="../_static/jquery.js"></script>
<script src="../_static/underscore.js"></script>
<script src="../_static/doctools.js"></script>
<script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
<script src="../_static/js/voxel51-website.js"></script>
<script src="../_static/js/custom.js"></script>
<script src="../_static/js/vendor/popper.min.js" type="text/javascript"></script>
<script src="../_static/js/vendor/bootstrap.min.js" type="text/javascript"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
<script src="../_static/js/theme.js" type="text/javascript"></script>
<script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<!-- Begin Footer -->


<!-- End Footer -->
<!-- Begin Mobile Menu -->

<!-- End Mobile Menu -->
<script src="../_static/js/vendor/anchor.min.js" type="text/javascript"></script>
<script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>