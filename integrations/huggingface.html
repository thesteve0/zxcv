


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Hugging Face Integration &mdash; FiftyOne 1.3.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/voxel51-website.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Ultralytics Integration" href="ultralytics.html" />
    <link rel="prev" title="LanceDB Integration" href="lancedb.html" />
<meta property="og:image" content="https://voxel51.com/wp-content/uploads/2024/03/3.24_webpages_Home_AV.png" />

<link
  href="https://fonts.googleapis.com/css?family=Palanquin:400,600,700,800"
  rel="stylesheet"
/>
<link
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
  rel="stylesheet"
/>
<script src="https://tag.clearbitscripts.com/v1/pk_b9ed71c8234edd4f77326bcbfab5a4ca/tags.js"></script>


  
  <script src="../_static/js/modernizr.min.js"></script>

  
</head>


<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <div class="container">
    <div class="header-container">
      <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>

      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <div class="ecosystem-dropdown">
              <a id="dropdownMenuButton" data-toggle="ecosystem-dropdown">
                Ecosystem
              </a>
              <div class="ecosystem-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/hub"">
                  <span class=dropdown-title>Models (Beta)</span>
                  <p>Discover, publish, and reuse pre-trained models</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/ecosystem">
                  <span class=dropdown-title>Tools & Libraries</span>
                  <p>Explore the ecosystem of tools and libraries</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <div class="resources-dropdown">
              <a id="resourcesDropdownButton" data-toggle="resources-dropdown">
                Resources
              </a>
              <div class="resources-dropdown-menu">
                <a class="nav-dropdown-item" href="https://pytorch.org/resources"">
                  <span class=dropdown-title>Developer Resources</span>
                  <p>Find resources and get questions answered</p>
                </a>
                <a class="nav-dropdown-item" href="https://pytorch.org/features">
                  <span class=dropdown-title>About</span>
                  <p>Learn about PyTorch’s features and capabilities</p>
                </a>
              </div>
            </div>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>

      <a class="main-menu-open-button" href="#" data-behavior="open-mobile-menu"></a>
    </div>

  </div>
</div>



<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../teams/index.html">FiftyOne Teams 🚀</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/index.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheets/index.html">Cheat Sheets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_zoo/index.html">Dataset Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brain.html">FiftyOne Brain</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins/index.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/fiftyone.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deprecation.html">Deprecation Notices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">FiftyOne Integrations</a> &gt;</li>
        
      <li>Hugging Face Integration</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Contents
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content style-external-links">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="hugging-face-integration">
<span id="huggingface-integration"></span><h1>Hugging Face Integration<a class="headerlink" href="#hugging-face-integration" title="Permalink to this headline">¶</a></h1>
<p>FiftyOne integrates natively with Hugging Face’s
<a class="reference external" href="https://huggingface.co/docs/transformers">Transformers</a> library, so
you can load, fine-tune, and run inference with your favorite Transformers
models on your FiftyOne datasets with just a few lines of code!</p>
<p>FiftyOne also integrates with the
<a class="reference external" href="https://huggingface.co/docs/hub/index">Hugging Face Hub</a>,  so you can push
datasets to and load datasets from the Hub with ease.</p>
<div class="section" id="transformers-library">
<span id="huggingface-transformers"></span><h2>Transformers Library<a class="headerlink" href="#transformers-library" title="Permalink to this headline">¶</a></h2>
<div class="section" id="setup">
<span id="huggingface-transformers-setup"></span><h3>Setup<a class="headerlink" href="#setup" title="Permalink to this headline">¶</a></h3>
<p>To get started with
<a class="reference external" href="https://huggingface.co/docs/transformers">Transformers</a>, just install the
<code class="code docutils literal notranslate"><span class="pre">transformers</span></code> package:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>transformers
</pre></div>
</div>
</div>
<div class="section" id="inference">
<span id="huggingface-transformers-inference"></span><h3>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h3>
<p>All
<a class="reference external" href="https://huggingface.co/docs/transformers/index#supported-models-and-frameworks">Transformers models</a>
that support image classification, object detection, semantic segmentation, or
monocular depth estimation tasks can be passed directly to your FiftyOne dataset’s
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.apply_model" title="fiftyone.core.collections.SampleCollection.apply_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_model()</span></code></a>
method.</p>
<p>The examples below show how to run inference with various Transformers models
on the following sample dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">select_fields</span><span class="p">()</span><span class="o">.</span><span class="n">keep_fields</span><span class="p">()</span>
</pre></div>
</div>
<div class="section" id="image-classification">
<span id="huggingface-transformers-image-classification"></span><h4>Image classification<a class="headerlink" href="#image-classification" title="Permalink to this headline">¶</a></h4>
<p>You can pass <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> classification models directly to FiftyOne
dataset’s
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.apply_model" title="fiftyone.core.collections.SampleCollection.apply_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_model()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># BeiT</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BeitForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BeitForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/beit-base-patch16-224&quot;</span>
<span class="p">)</span>

<span class="c1"># DeiT</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DeiTForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DeiTForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/deit-base-distilled-patch16-224&quot;</span>
<span class="p">)</span>

<span class="c1"># DINOv2</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Dinov2ForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Dinov2ForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/dinov2-small-imagenet1k-1-layer&quot;</span>
<span class="p">)</span>

<span class="c1"># MobileNetV2</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">MobileNetV2ForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MobileNetV2ForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/mobilenet_v2_1.0_224&quot;</span>
<span class="p">)</span>

<span class="c1"># Swin Transformer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">SwinForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SwinForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/swin-tiny-patch4-window7-224&quot;</span>
<span class="p">)</span>

<span class="c1"># ViT</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ViTForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ViTForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/vit-base-patch16-224&quot;</span>
<span class="p">)</span>

<span class="c1"># ViT-Hybrid</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ViTHybridForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">ViTHybridForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/vit-hybrid-base-bit-384&quot;</span>
<span class="p">)</span>

<span class="c1"># Any auto model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/levit-128S&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;classif_predictions&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, you can manually run inference with the <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model and
then use the
<a class="reference internal" href="../api/fiftyone.utils.transformers.html#fiftyone.utils.transformers.to_classification" title="fiftyone.utils.transformers.to_classification"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_classification()</span></code></a>
utility to convert the predictions to <a class="reference internal" href="../user_guide/using_datasets.html#classification"><span class="std std-ref">FiftyOne format</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">ViTHybridForImageClassification</span><span class="p">,</span> <span class="n">AutoProcessor</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">ViTHybridForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/vit-hybrid-base-bit-384&quot;</span>
<span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;google/vit-hybrid-base-bit-384&quot;</span><span class="p">)</span>
<span class="n">id2label</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;classif_predictions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fout</span><span class="o">.</span><span class="n">to_classification</span><span class="p">(</span><span class="n">result</span><span class="p">,</span> <span class="n">id2label</span><span class="p">)</span>
    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
<p>Finally, you can load <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> models directly from the
<a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">FiftyOne Model Zoo</span></a>!</p>
<p>To load a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> classification model from the zoo, specify
<code class="code docutils literal notranslate"><span class="pre">&quot;classification-transformer-torch&quot;</span></code> as the first argument, and pass in the
model’s name or path as a keyword argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;classification-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;facebook/levit-128S&quot;</span><span class="p">,</span>  <span class="c1"># HF model name or path</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;levit&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="object-detection">
<span id="huggingface-transformers-object-detection"></span><h4>Object detection<a class="headerlink" href="#object-detection" title="Permalink to this headline">¶</a></h4>
<p>You can pass <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> detection models directly to your FiftyOne
dataset’s
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.apply_model" title="fiftyone.core.collections.SampleCollection.apply_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_model()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># DETA</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetaForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DetaForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;jozhang97/deta-swin-large&quot;</span>
<span class="p">)</span>

<span class="c1"># DETR</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetrForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DetrForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/detr-resnet-50&quot;</span>
<span class="p">)</span>

<span class="c1"># DeformableDETR</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DeformableDetrForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DeformableDetrForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;SenseTime/deformable-detr&quot;</span>
<span class="p">)</span>

<span class="c1"># Table Transformer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TableTransformerForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">TableTransformerForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/table-transformer-detection&quot;</span>
<span class="p">)</span>

<span class="c1"># YOLOS</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">YolosForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YolosForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;hustvl/yolos-tiny&quot;</span>
<span class="p">)</span>

<span class="c1"># Any auto model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/conditional-detr-resnet-50&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;det_predictions&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, you can manually run inference with the <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model and
then use the
<a class="reference internal" href="../api/fiftyone.utils.transformers.html#fiftyone.utils.transformers.to_detections" title="fiftyone.utils.transformers.to_detections"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_detections()</span></code></a> utility to
convert the predictions to <a class="reference internal" href="../user_guide/using_datasets.html#object-detection"><span class="std std-ref">FiftyOne format</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForObjectDetection</span><span class="p">,</span> <span class="n">AutoProcessor</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">AutoModelForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/conditional-detr-resnet-50&quot;</span>
<span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/conditional-detr-resnet-50&quot;</span>
<span class="p">)</span>
<span class="n">id2label</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">target_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">image</span><span class="o">.</span><span class="n">size</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
    <span class="n">result</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">post_process_object_detection</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">target_sizes</span><span class="o">=</span><span class="n">target_sizes</span>
    <span class="p">)</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;det_predictions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fout</span><span class="o">.</span><span class="n">to_detections</span><span class="p">(</span>
        <span class="n">result</span><span class="p">,</span> <span class="n">id2label</span><span class="p">,</span> <span class="p">[</span><span class="n">image</span><span class="o">.</span><span class="n">size</span><span class="p">]</span>
    <span class="p">)</span>
    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
<p>Finally, you can load <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> models directly from the
<a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">FiftyOne Model Zoo</span></a>!</p>
<p>To load a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> detection model from the zoo, specify
<code class="code docutils literal notranslate"><span class="pre">&quot;detection-transformer-torch&quot;</span></code> as the first argument, and pass in the model’s
name or path as a keyword argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;detection-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;facebook/detr-resnet-50&quot;</span><span class="p">,</span>  <span class="c1"># HF model name or path</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;detr&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="semantic-segmentation">
<span id="huggingface-transformers-semantic-segmentation"></span><h4>Semantic segmentation<a class="headerlink" href="#semantic-segmentation" title="Permalink to this headline">¶</a></h4>
<p>You can pass a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> semantic segmentation model directly to your
FiftyOne dataset’s
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.apply_model" title="fiftyone.core.collections.SampleCollection.apply_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_model()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Mask2Former</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">Mask2FormerForUniversalSegmentation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Mask2FormerForUniversalSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/mask2former-swin-small-coco-instance&quot;</span>
<span class="p">)</span>

<span class="c1"># Mask2Former</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">MaskFormerForInstanceSegmentation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">MaskFormerForInstanceSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;facebook/maskformer-swin-base-ade&quot;</span>
<span class="p">)</span>

<span class="c1"># Segformer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">SegformerForSemanticSegmentation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">SegformerForSemanticSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;nvidia/segformer-b0-finetuned-ade-512-512&quot;</span>
<span class="p">)</span>

<span class="c1"># Any auto model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSemanticSegmentation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSemanticSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;Intel/dpt-large-ade&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;seg_predictions&quot;</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">default_mask_targets</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, you can manually run inference with the <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model and
then use the
<a class="reference internal" href="../api/fiftyone.utils.transformers.html#fiftyone.utils.transformers.to_segmentation" title="fiftyone.utils.transformers.to_segmentation"><code class="xref py py-func docutils literal notranslate"><span class="pre">to_segmentation()</span></code></a> utility
to convert the predictions to <a class="reference internal" href="../user_guide/using_datasets.html#semantic-segmentation"><span class="std std-ref">FiftyOne format</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSemanticSegmentation</span><span class="p">,</span> <span class="n">AutoProcessor</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">AutoModelForSemanticSegmentation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;Intel/dpt-large-ade&quot;</span>
<span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Intel/dpt-large-ade&quot;</span><span class="p">)</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="n">target_size</span> <span class="o">=</span> <span class="p">[</span><span class="n">image</span><span class="o">.</span><span class="n">size</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">result</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">post_process_semantic_segmentation</span><span class="p">(</span>
        <span class="n">output</span><span class="p">,</span> <span class="n">target_sizes</span><span class="o">=</span><span class="n">target_size</span>
    <span class="p">)</span>
    <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;seg_predictions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fout</span><span class="o">.</span><span class="n">to_segmentation</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
<p>Finally, you can load <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> models directly from the
<a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">FiftyOne Model Zoo</span></a>!</p>
<p>To load a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> semantic segmentation model from the zoo, specify
<code class="code docutils literal notranslate"><span class="pre">&quot;segmentation-transformer-torch&quot;</span></code> as the first argument, and pass in the
model’s name or path as a keyword argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;segmentation-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;nvidia/segformer-b0-finetuned-ade-512-512&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;segformer&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="monocular-depth-estimation">
<span id="huggingface-transformers-monocular-depth-estimation"></span><h4>Monocular depth estimation<a class="headerlink" href="#monocular-depth-estimation" title="Permalink to this headline">¶</a></h4>
<p>You can pass a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> monocular depth estimation model directly to your
FiftyOne dataset’s <a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.apply_model" title="fiftyone.core.collections.SampleCollection.apply_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_model()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># DPT</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DPTForDepthEstimation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DPTForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Intel/dpt-large&quot;</span><span class="p">)</span>

<span class="c1"># GLPN</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GLPNForDepthEstimation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GLPNForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;vinvino02/glpn-kitti&quot;</span><span class="p">)</span>

<span class="c1"># Depth Anything</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForDepthEstimation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;LiheYoung/depth-anything-small-hf&quot;</span><span class="p">)</span>

<span class="c1"># Depth Anything-V2</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForDepthEstimation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;depth-anything/Depth-Anything-V2-Small-hf&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;depth_predictions&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, you can load <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> depth estimation models directly from
the <a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">FiftyOne Model Zoo</span></a>!</p>
<p>To load a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> depth estimation model from the zoo, specify
<code class="code docutils literal notranslate"><span class="pre">&quot;depth-estimation-transformer-torch&quot;</span></code> as the first argument, and pass in the
model’s name or path as a keyword argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;depth-estimation-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;Intel/dpt-hybrid-midas&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;dpt_hybrid_midas&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="zero-shot-classification">
<span id="huggingface-transformers-zero-shot-classification"></span><h4>Zero-shot classification<a class="headerlink" href="#zero-shot-classification" title="Permalink to this headline">¶</a></h4>
<p>Zero-shot image classification models from <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> can be loaded
directly from the <a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">FiftyOne Model Zoo</span></a>!</p>
<p>To load a  <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> zero-shot classification model from the zoo, specify
<code class="code docutils literal notranslate"><span class="pre">&quot;zero-shot-classification-transformer-torch&quot;</span></code> as the first argument, and pass
in the model’s name or path as a keyword argument:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;zero-shot-classification-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;BAAI/AltCLIP&quot;</span><span class="p">,</span>  <span class="c1"># HF model name or path</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;bird&quot;</span><span class="p">,</span> <span class="s2">&quot;fish&quot;</span><span class="p">,</span> <span class="s2">&quot;turtle&quot;</span><span class="p">],</span>  <span class="c1"># optional</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Once loaded, you can pass the model directly to your FiftyOne dataset’s
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.apply_model" title="fiftyone.core.collections.SampleCollection.apply_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_model()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;altclip&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also generate embeddings for the samples in your dataset with zero shot
models as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;zero-shot-classification-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;BAAI/AltCLIP&quot;</span><span class="p">,</span>  <span class="c1"># HF model name or path</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">compute_embeddings</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">embeddings_field</span><span class="o">=</span><span class="s2">&quot;altclip_embeddings&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also change the label classes of zero shot models any time by setting
the <code class="code docutils literal notranslate"><span class="pre">classes</span></code> attribute of the model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;bird&quot;</span><span class="p">,</span> <span class="s2">&quot;fish&quot;</span><span class="p">,</span> <span class="s2">&quot;turtle&quot;</span><span class="p">]</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;altclip&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>The
<a class="reference internal" href="../api/fiftyone.utils.transformers.html#fiftyone.utils.transformers.convert_transformers_model" title="fiftyone.utils.transformers.convert_transformers_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_transformers_model()</span></code></a>
utility also allows you to manually convert a zero-shot <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model to
FiftyOne format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">CLIPSegModel</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">CLIPSegModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;CIDAS/clipseg-rd64-refined&quot;</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">fout</span><span class="o">.</span><span class="n">convert_transformers_model</span><span class="p">(</span>
    <span class="n">transformers_model</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;image-classification&quot;</span><span class="p">,</span>  <span class="c1"># or &quot;semantic-segmentation&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some zero-shot models are compatible with multiple tasks, so it is
recommended that you specify the task type when converting the model.</p>
</div>
</div>
<div class="section" id="zero-shot-object-detection">
<span id="huggingface-transformers-zero-shot-detection"></span><h4>Zero-shot object detection<a class="headerlink" href="#zero-shot-object-detection" title="Permalink to this headline">¶</a></h4>
<p>Zero-shot object detection models from <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> can be loaded directly
from the <a class="reference internal" href="../model_zoo/index.html#model-zoo"><span class="std std-ref">FiftyOne Model Zoo</span></a>!</p>
<p>To load a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> zero-shot object detection model from the zoo, specify
<code class="code docutils literal notranslate"><span class="pre">&quot;zero-shot-detection-transformer-torch&quot;</span></code> as the first argument, and pass
in the model’s name or path as a keyword argument. You can optionally pass in a
list of label classes as a keyword argument <code class="code docutils literal notranslate"><span class="pre">classes</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;zero-shot-detection-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;google/owlvit-base-patch32&quot;</span><span class="p">,</span>  <span class="c1"># HF model name or path</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;bird&quot;</span><span class="p">,</span> <span class="s2">&quot;fish&quot;</span><span class="p">,</span> <span class="s2">&quot;turtle&quot;</span><span class="p">],</span>  <span class="c1"># optional</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The
<a class="reference internal" href="../api/fiftyone.utils.transformers.html#fiftyone.utils.transformers.convert_transformers_model" title="fiftyone.utils.transformers.convert_transformers_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_transformers_model()</span></code></a>
utility also allows you to manually convert a zero-shot <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model to
FiftyOne format:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OwlViTForObjectDetection</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">OwlViTForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/owlvit-base-patch32&quot;</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">fout</span><span class="o">.</span><span class="n">convert_transformers_model</span><span class="p">(</span>
    <span class="n">transformers_model</span><span class="p">,</span>
    <span class="n">task</span><span class="o">=</span><span class="s2">&quot;object-detection&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Some zero-shot models are compatible with multiple tasks, so it is
recommended that you specify the task type when converting the model.</p>
</div>
<p>As of <code class="code docutils literal notranslate"><span class="pre">transformers&gt;=4.40.0</span></code> and <code class="code docutils literal notranslate"><span class="pre">fiftyone&gt;=0.24.0</span></code>, you can also use
<a class="reference external" href="https://huggingface.co/docs/transformers/main/en/model_doc/grounding-dino">Grounding DINO</a>
models for zero-shot object detection:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;zero-shot-detection-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;IDEA-Research/grounding-dino-tiny&quot;</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;cat&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;cats&quot;</span><span class="p">,</span> <span class="n">confidence_thresh</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">confidence_thresh</span></code> parameter is optional and can be used to filter out
predictions with confidence scores below the specified threshold. You may
need to adjust this value based on the model and dataset you are working.
Also note that whereas OwlViT models accept multiple classes, Grounding DINO
models only accept a single class.</p>
</div>
</div>
<div class="section" id="batch-inference">
<span id="huggingface-transformers-batch-inference"></span><h4>Batch inference<a class="headerlink" href="#batch-inference" title="Permalink to this headline">¶</a></h4>
<p>When using
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.apply_model" title="fiftyone.core.collections.SampleCollection.apply_model"><code class="xref py py-meth docutils literal notranslate"><span class="pre">apply_model()</span></code></a>,
you can request batch inference by passing the optional <code class="code docutils literal notranslate"><span class="pre">batch_size</span></code> parameter:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;det_predictions&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
<p>The manual inference loops can be also executed using batch inference via the
pattern below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.core.utils</span> <span class="kn">import</span> <span class="n">iter_batches</span>
<span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="c1"># Load a detection model and its corresponding processor</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">YolosForObjectDetection</span><span class="p">,</span> <span class="n">AutoProcessor</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">YolosForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;hustvl/yolos-tiny&quot;</span>
<span class="p">)</span>
<span class="n">processor</span> <span class="o">=</span> <span class="n">AutoProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;hustvl/yolos-tiny&quot;</span><span class="p">)</span>
<span class="n">id2label</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">id2label</span>

<span class="n">filepaths</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="s2">&quot;filepath&quot;</span><span class="p">)</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">paths</span> <span class="ow">in</span> <span class="n">iter_batches</span><span class="p">(</span><span class="n">filepaths</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="n">images</span> <span class="o">=</span> <span class="p">[</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">paths</span><span class="p">]</span>
    <span class="n">image_sizes</span> <span class="o">=</span> <span class="p">[</span><span class="n">i</span><span class="o">.</span><span class="n">size</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">images</span><span class="p">]</span>
    <span class="n">target_sizes</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">image</span><span class="o">.</span><span class="n">size</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">image</span> <span class="ow">in</span> <span class="n">images</span><span class="p">])</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">processor</span><span class="p">(</span><span class="n">images</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">transformers_model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>

    <span class="n">results</span> <span class="o">=</span> <span class="n">processor</span><span class="o">.</span><span class="n">post_process_object_detection</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">,</span> <span class="n">target_sizes</span><span class="o">=</span><span class="n">target_sizes</span>
    <span class="p">)</span>
    <span class="n">predictions</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">fout</span><span class="o">.</span><span class="n">to_detections</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">id2label</span><span class="p">,</span> <span class="n">image_sizes</span><span class="p">))</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">set_values</span><span class="p">(</span><span class="s2">&quot;det_predictions&quot;</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="../user_guide/using_datasets.html#batch-updates"><span class="std std-ref">this section</span></a> for more information about
performing batch updates to your FiftyOne datasets.</p>
</div>
</div>
</div>
<div class="section" id="embeddings">
<span id="huggingface-transformers-embeddings"></span><h3>Embeddings<a class="headerlink" href="#embeddings" title="Permalink to this headline">¶</a></h3>
<p>Any <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model that supports image classification or object detection
tasks — zero-shot or otherwise — can be used to compute embeddings for your
samples.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>For  zero-shot models, FiftyOne will use the <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model’s
<code class="code docutils literal notranslate"><span class="pre">get_image_features()</span></code> method to extract embeddings.</p>
<p>For non-zero-shot models, regardless of whether you use a classification,
detection, or base model, FiftyOne will extract embeddings from the
<code class="code docutils literal notranslate"><span class="pre">last_hidden_state</span></code> of the model’s base encoder.</p>
</div>
<div class="section" id="image-embeddings">
<span id="huggingface-transformers-image-embeddings"></span><h4>Image embeddings<a class="headerlink" href="#image-embeddings" title="Permalink to this headline">¶</a></h4>
<p>To compute embeddings for images, you can pass the <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model
directly to your FiftyOne dataset’s
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.compute_embeddings" title="fiftyone.core.collections.SampleCollection.compute_embeddings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_embeddings()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Embeddings from base model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BeitModel</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BeitModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>
<span class="p">)</span>

<span class="c1"># Embeddings from classification model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BeitForImageClassification</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BeitForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/beit-base-patch16-224&quot;</span>
<span class="p">)</span>

<span class="c1"># Embeddings from detection model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetaForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DetaForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;jozhang97/deta-swin-large-o365&quot;</span>
<span class="p">)</span>

<span class="c1"># Embeddings from zero-shot classification model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AltCLIPModel</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AltCLIPModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;BAAI/AltCLIP&quot;</span>
<span class="p">)</span>

<span class="c1"># Embeddings from zero-shot detection model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OwlViTForObjectDetection</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">OwlViTForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/owlvit-base-patch32&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">select_fields</span><span class="p">()</span><span class="o">.</span><span class="n">keep_fields</span><span class="p">()</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">compute_embeddings</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">embeddings_field</span><span class="o">=</span><span class="s2">&quot;embeddings&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Alternatively, you can use the
<a class="reference internal" href="../api/fiftyone.utils.transformers.html#fiftyone.utils.transformers.convert_transformers_model" title="fiftyone.utils.transformers.convert_transformers_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">convert_transformers_model()</span></code></a>
utility to convert a <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> model to FiftyOne format, which allows you
to check the model’s
<a class="reference internal" href="../api/fiftyone.core.models.html#fiftyone.core.models.Model.has_embeddings" title="fiftyone.core.models.Model.has_embeddings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">has_embeddings</span></code></a> property to
see if the model can be used to generate embeddings:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BeitModel</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">BeitModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>
<span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">fout</span><span class="o">.</span><span class="n">convert_transformers_model</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">has_embeddings</span><span class="p">)</span>  <span class="c1"># True</span>

<span class="c1"># Embed an image directly</span>
<span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">first</span><span class="p">()</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span>
<span class="n">embedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">image</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="section" id="text-embeddings">
<span id="huggingface-transformers-text-embeddings"></span><h4>Text embeddings<a class="headerlink" href="#text-embeddings" title="Permalink to this headline">¶</a></h4>
<p>Zero-shot image classification and object detection models from <code class="code docutils literal notranslate"><span class="pre">transformers</span></code>
can also be used to compute embeddings for text:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">select_fields</span><span class="p">()</span><span class="o">.</span><span class="n">keep_fields</span><span class="p">()</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;zero-shot-classification-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;BAAI/AltCLIP&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">embedding</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">embed_prompt</span><span class="p">(</span><span class="s2">&quot;a photo of a dog&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>You can check whether a model supports text embeddings by checking the
<a class="reference internal" href="../api/fiftyone.utils.transformers.html#fiftyone.utils.transformers.ZeroShotTransformerPromptMixin.embed_prompts" title="fiftyone.utils.transformers.ZeroShotTransformerPromptMixin.embed_prompts"><code class="xref py py-meth docutils literal notranslate"><span class="pre">can_embed_prompts</span></code></a>
property:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># A zero-shot model that supports text embeddings</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;zero-shot-classification-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;BAAI/AltCLIP&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">can_embed_prompts</span><span class="p">)</span>  <span class="c1"># True</span>

<span class="c1"># A classification model that does not support text embeddings</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;classification-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;microsoft/beit-base-patch16-224&quot;</span><span class="p">,</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">can_embed_prompts</span><span class="p">)</span>  <span class="c1"># False</span>
</pre></div>
</div>
</div>
<div class="section" id="batch-embeddings">
<span id="huggingface-transformers-batch-embeddings"></span><h4>Batch embeddings<a class="headerlink" href="#batch-embeddings" title="Permalink to this headline">¶</a></h4>
<p>You can request batch inference by passing the optional <code class="code docutils literal notranslate"><span class="pre">batch_size</span></code> parameter
to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.compute_embeddings" title="fiftyone.core.collections.SampleCollection.compute_embeddings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_embeddings()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span><span class="o">.</span><span class="n">compute_embeddings</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">embeddings_field</span><span class="o">=</span><span class="s2">&quot;embeddings&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">16</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="patch-embeddings">
<span id="huggingface-transformers-patch-embeddings"></span><h4>Patch embeddings<a class="headerlink" href="#patch-embeddings" title="Permalink to this headline">¶</a></h4>
<p>You can compute embeddings for image patches by passing <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> models
directly to your FiftyOne dataset’s
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.compute_patch_embeddings" title="fiftyone.core.collections.SampleCollection.compute_patch_embeddings"><code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_patch_embeddings()</span></code></a>
method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">import</span> <span class="nn">fiftyone.utils.transformers</span> <span class="k">as</span> <span class="nn">fout</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BeitModel</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BeitModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">compute_patch_embeddings</span><span class="p">(</span>
    <span class="n">model</span><span class="p">,</span>
    <span class="n">patches_field</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">embeddings_field</span><span class="o">=</span><span class="s2">&quot;embeddings&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="brain-methods">
<span id="huggingface-transformers-brain-methods"></span><h3>Brain methods<a class="headerlink" href="#brain-methods" title="Permalink to this headline">¶</a></h3>
<p>Because <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> models can be used to compute embeddings, they can be
passed to <a class="reference internal" href="../brain.html#fiftyone-brain"><span class="std std-ref">Brain methods</span></a> like
<code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_similarity()</span></code> and
<code class="xref py py-meth docutils literal notranslate"><span class="pre">compute_visualization()</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.brain</span> <span class="k">as</span> <span class="nn">fob</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="c1"># Classification model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BeitModel</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">BeitModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;microsoft/beit-base-patch16-224-pt22k&quot;</span>
<span class="p">)</span>

<span class="c1"># Detection model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DetaForObjectDetection</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">DetaForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;jozhang97/deta-swin-large&quot;</span>
<span class="p">)</span>

<span class="c1"># Zero-shot classification model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForImageClassification</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">AutoModelForImageClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;BAAI/AltCLIP&quot;</span>
<span class="p">)</span>

<span class="c1"># Zero-shot detection model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">OwlViTForObjectDetection</span>
<span class="n">transformers_model</span> <span class="o">=</span> <span class="n">OwlViTForObjectDetection</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="s2">&quot;google/owlvit-base-patch32&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Option 1: directly pass `transformers` model</span>
<span class="n">fob</span><span class="o">.</span><span class="n">compute_similarity</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">transformers_model</span><span class="p">,</span> <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;sim1&quot;</span><span class="p">)</span>
<span class="n">fob</span><span class="o">.</span><span class="n">compute_visualization</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">transformers_model</span><span class="p">,</span> <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;vis1&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># Option 2: pass pre-computed embeddings</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">compute_embeddings</span><span class="p">(</span><span class="n">transformers_model</span><span class="p">,</span> <span class="n">embeddings_field</span><span class="o">=</span><span class="s2">&quot;embeddings&quot;</span><span class="p">)</span>

<span class="n">fob</span><span class="o">.</span><span class="n">compute_similarity</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="s2">&quot;embeddings&quot;</span><span class="p">,</span> <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;sim2&quot;</span><span class="p">)</span>
<span class="n">fob</span><span class="o">.</span><span class="n">compute_visualization</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="s2">&quot;embeddings&quot;</span><span class="p">,</span> <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;vis2&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>Because <code class="code docutils literal notranslate"><span class="pre">transformers</span></code> zero-shot models can be used to embed text, they can
also be used to construct similarity indexes on your datasets which support
natural language queries.</p>
<p>To use this functionality, you must pass the model by <strong>name</strong> into the brain
method, along with any necessary keyword arguments that must be passed to
<a class="reference internal" href="../api/fiftyone.zoo.html#fiftyone.zoo.load_zoo_model" title="fiftyone.zoo.load_zoo_model"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_zoo_model()</span></code></a> to load the correct
model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.brain</span> <span class="k">as</span> <span class="nn">fob</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">,</span> <span class="n">max_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="n">fob</span><span class="o">.</span><span class="n">compute_similarity</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;zero_shot_sim&quot;</span><span class="p">,</span>
    <span class="n">model</span><span class="o">=</span><span class="s2">&quot;zero-shot-classification-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;BAAI/AltCLIP&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sort_by_similarity</span><span class="p">(</span><span class="s2">&quot;A photo of a dog&quot;</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">25</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="huggingface-hub">
<span id="id2"></span><h2>Hugging Face Hub<a class="headerlink" href="#huggingface-hub" title="Permalink to this headline">¶</a></h2>
<p>FiftyOne integrates with the
<a class="reference external" href="https://huggingface.co/docs/hub/index">Hugging Face Hub</a> to allow you to
push datasets to and load datasets from the Hub with ease. This integration
simplifies the process of sharing datasets with the machine learning  and
computer vision community, and allows you to easily access and work with many
of the most popular vision and multimodal datasets available!</p>
<div class="section" id="huggingface-hub-setup">
<span id="id4"></span><h3>Setup<a class="headerlink" href="#huggingface-hub-setup" title="Permalink to this headline">¶</a></h3>
<p>To push datasets to and load datasets from the
<a class="reference external" href="https://huggingface.co/docs/hub/index">Hugging Face Hub</a>, you will need the
<a class="reference external" href="https://github.com/huggingface/huggingface_hub">Hugging Face Hub Python client</a>,
which you can install via PyPI:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span>pip<span class="w"> </span>install<span class="w"> </span><span class="s2">&quot;huggingface_hub&gt;=0.20.0&quot;</span>
</pre></div>
</div>
<p>To push a dataset to the Hub, and in some cases, to access a dataset on
the hub, you will need to have a
<a class="reference external" href="https://huggingface.co/join">Hugging Face Hub account</a>.</p>
<p>Hugging Face handles authentication via tokens, which you can obtain by
logging into your account and navigating to the
<a class="reference external" href="https://huggingface.co/settings/tokens">Access Tokens</a> section of your
profile. At the bottom of this page, you can create a new token with write or
read access to the Hub. Once you have your token, you can set it as an
environment variable:</p>
<div class="highlight-shell notranslate"><div class="highlight"><pre><span></span><span class="nb">export</span><span class="w"> </span><span class="nv">HF_TOKEN</span><span class="o">=</span><span class="s2">&quot;&lt;your-token-here&gt;&quot;</span>
</pre></div>
</div>
</div>
<div class="section" id="pushing-datasets-to-the-hub">
<span id="huggingface-hub-push-dataset"></span><h3>Pushing datasets to the Hub<a class="headerlink" href="#pushing-datasets-to-the-hub" title="Permalink to this headline">¶</a></h3>
<p>If you are working with a dataset in FiftyOne and you want to quickly share it
with others, you can do so via the
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a>
function, which takes two positional arguments:</p>
<ul class="simple">
<li><p>the FiftyOne sample collection (a <a class="reference internal" href="../api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset" title="fiftyone.core.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> or <a class="reference internal" href="../api/fiftyone.core.view.html#fiftyone.core.view.DatasetView" title="fiftyone.core.view.DatasetView"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetView</span></code></a>)</p></li>
<li><p>the <code class="code docutils literal notranslate"><span class="pre">repo_name</span></code>, which will be combined with your Hugging Face username or
organization name to construct the <code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> where the sample collection
will be uploaded.</p></li>
</ul>
<p>As you will see, this simple function allows you to push datasets and filtered
views containing images, videos, point clouds, and other multimodal data to the
Hugging Face Hub, providing you with incredible flexibility in the process.</p>
<div class="section" id="basic-usage">
<span id="huggingface-hub-push-dataset-basic"></span><h4>Basic usage<a class="headerlink" href="#basic-usage" title="Permalink to this headline">¶</a></h4>
<p>The basic recipe for pushing a FiftyOne dataset to the Hub is just two lines of
code. As a starting point, let’s use the example
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart"><span class="std std-ref">Quickstart dataset</span></a> dataset from the
<a class="reference internal" href="../dataset_zoo/index.html#dataset-zoo"><span class="std std-ref">FiftyOne Dataset Zoo</span></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>To push the dataset to the Hugging Face Hub, all you need to do is call
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a> with the dataset
and the desired <code class="code docutils literal notranslate"><span class="pre">repo_name</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>

<span class="n">push_to_hub</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;my-quickstart-dataset&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>When you run this code, a few things happen:</p>
<ul class="simple">
<li><p>The dataset and its media files are exported to a temporary directory and
uploaded to the specified Hugging Face repo.</p></li>
<li><p>A <code class="code docutils literal notranslate"><span class="pre">fiftyone.yml</span></code> config file for the dataset is generated and uploaded to
the repo, which contains all of the necessary information so that the dataset
can be loaded with
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a>.</p></li>
<li><p>A Hugging Face
<a class="reference external" href="https://huggingface.co/docs/hub/en/datasets-cards">Dataset Card</a>
for the dataset is auto-generated, providing tags, metadata, license info,
and a code snippet illustrating how to load the dataset from the hub.</p></li>
</ul>
<p>Your dataset will be available on the Hub at the following URL:</p>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>https://huggingface.co/datasets/&lt;your-username-or-org-name&gt;/my-quickstart-dataset
</pre></div>
</div>
<p>Pushing a <a class="reference internal" href="../api/fiftyone.core.view.html#fiftyone.core.view.DatasetView" title="fiftyone.core.view.DatasetView"><code class="xref py py-class docutils literal notranslate"><span class="pre">DatasetView</span></code></a> to the Hub works in exactly the same way. For example,
if you want to push a filtered view of the <code class="code docutils literal notranslate"><span class="pre">quickstart</span></code> dataset containing only
predictions with high confidence, you can do so by creating the view as usual,
and then passing that in to
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>

<span class="c1"># Create view with high confidence predictions</span>
<span class="n">view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span> <span class="n">F</span><span class="p">(</span><span class="s2">&quot;confidence&quot;</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mf">0.95</span><span class="p">)</span>

<span class="c1"># Push view to the Hub as a new dataset</span>
<span class="n">push_to_hub</span><span class="p">(</span><span class="n">view</span><span class="p">,</span> <span class="s2">&quot;my-quickstart-high-conf&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>When you do so, note that the view is exported as a new dataset, and other
details from the original dataset are not included.</p>
<p>FiftyOne is a <em>visual</em> toolkit, so when you push a dataset to the Hub, you can
optionally include a preview (image, gif, or video) of the dataset, that will be
displayed on the dataset page. To do this, you can pass the <code class="code docutils literal notranslate"><span class="pre">preview_path</span></code>
argument to <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a>, with
either a relative or absolute path to the preview file on your local machine:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Screenshot and save the preview image to a file...</span>

<span class="n">push_to_hub</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="s2">&quot;my-quickstart-with-preview&quot;</span><span class="p">,</span>
    <span class="n">preview_path</span><span class="o">=</span><span class="s2">&quot;/path/to/preview.jpg&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The preview file will be uploaded to the Hub along with the dataset, and will be
displayed on the dataset card!</p>
<img alt="Pushing a dataset to the Hugging Face Hub with a preview image" class="align-center" src="../_images/hf_data_card_preview.jpg" />
</div>
<div class="section" id="pushing-large-datasets">
<span id="huggingface-hub-push-large-dataset"></span><h4>Pushing large datasets<a class="headerlink" href="#pushing-large-datasets" title="Permalink to this headline">¶</a></h4>
<p>Large datasets with many samples require a bit more care when pushing to the
Hub. Hugging Face limits the number of files that can be uploaded in a single
directory to 10000, so if your dataset contains more than 10000 samples, the
data will need to be split into multiple directories. FiftyOne handles this
automatically when pushing large datasets to the Hub, but you can manually
configure the number of samples per directory by passing the <code class="code docutils literal notranslate"><span class="pre">chunk_size</span></code>
argument to <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>

<span class="c1"># Limit to 100 images per directory</span>
<span class="n">push_to_hub</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;my-large-dataset&quot;</span><span class="p">,</span> <span class="n">chunk_size</span><span class="o">=</span><span class="mi">100</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">chunk_size</span></code> argument is currently only supported when exporting in
<a class="reference internal" href="../user_guide/export_datasets.html#fiftyonedataset-export"><span class="std std-ref">FiftyOneDataset format</span></a> (the default).</p>
</div>
</div>
<div class="section" id="advanced-usage">
<span id="huggingface-hub-push-dataset-advanced"></span><h4>Advanced usage<a class="headerlink" href="#advanced-usage" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a> function
provides a number of optional arguments that allow you to customize how your
dataset is pushed to the Hub, including whether the dataset is public or private,
what license it is released under, and more.</p>
<p>FiftyOne’s <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a>
function supports the Hugging Face Hub API arguments <code class="code docutils literal notranslate"><span class="pre">private</span></code> and <code class="code docutils literal notranslate"><span class="pre">exist_ok</span></code>.</p>
<ul class="simple">
<li><p><strong>private</strong> <em>(bool)</em>: Whether the dataset should be private. If <code class="code docutils literal notranslate"><span class="pre">True</span></code>, the
dataset will be private and only accessible to you. If <code class="code docutils literal notranslate"><span class="pre">False</span></code>, the dataset
will be public and accessible to anyone with the link. Defaults to <code class="code docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><dl class="simple">
<dt><strong>exist_ok</strong> <em>(bool)</em>: Whether to overwrite an existing dataset with the same</dt><dd><p><code class="code docutils literal notranslate"><span class="pre">repo_name</span></code>. If <code class="code docutils literal notranslate"><span class="pre">True</span></code>, the existing dataset will be overwritten. If <code class="code docutils literal notranslate"><span class="pre">False</span></code>,
an error will be raised if a dataset with the same <code class="code docutils literal notranslate"><span class="pre">repo_name</span></code> already
exists. Defaults to <code class="code docutils literal notranslate"><span class="pre">False</span></code>.</p>
</dd>
</dl>
</li>
</ul>
<p>For example, to push a dataset to the Hub as private, you can do the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>

<span class="n">push_to_hub</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;my-private-dataset&quot;</span><span class="p">,</span> <span class="n">private</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>You can also specify the <code class="code docutils literal notranslate"><span class="pre">tags</span></code>, <code class="code docutils literal notranslate"><span class="pre">license</span></code>, and <code class="code docutils literal notranslate"><span class="pre">description</span></code> of the dataset,
all of which will propagate to the <code class="code docutils literal notranslate"><span class="pre">fiftyone.yml</span></code> config file and the Hugging
Face Dataset Card. For example, to push a video action recognition dataset with
an MIT license and a description, you can do the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart-video&quot;</span><span class="p">)</span>

<span class="n">push_to_hub</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="s2">&quot;my-action-recognition-dataset&quot;</span><span class="p">,</span>
    <span class="n">tags</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;video&quot;</span><span class="p">,</span> <span class="s2">&quot;action-recognition&quot;</span><span class="p">],</span>
    <span class="n">license</span><span class="o">=</span><span class="s2">&quot;mit&quot;</span><span class="p">,</span>
    <span class="n">description</span><span class="o">=</span><span class="s2">&quot;A dataset of videos for action recognition tasks&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>The pushed dataset will be available on the Hub and the dataset page will look
like this:</p>
<img alt="Pushing a dataset to the Hugging Face Hub with advanced options" class="align-center" src="../_images/hf_push_advanced_example.jpg" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">tags</span></code> argument can be a string or a list of strings. The tag <code class="code docutils literal notranslate"><span class="pre">fiftyone</span></code>
is automatically added to all datasets pushed with FiftyOne, communicating
that the dataset was created with FiftyOne and can be loaded with the
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a> function.</p>
</div>
<p>The license is specified as a string. For a list of supported licenses, see the
<a class="reference external" href="https://huggingface.co/docs/hub/en/repositories-licenses">Hugging Face Hub documentation</a>.</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">description</span></code> argument can be used for whatever you like. When the dataset
is loaded from the Hub, this description will be accessible via the dataset’s
<a class="reference internal" href="../api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset.description" title="fiftyone.core.dataset.Dataset.description"><code class="xref py py-meth docutils literal notranslate"><span class="pre">description</span></code></a> property.</p>
<p>Additionally, you can specify the “format” of the uploaded dataset. By default,
the format is the standard <a class="reference internal" href="../user_guide/dataset_creation/datasets.html#fiftyonedataset-import"><span class="std std-ref">FiftyOneDataset</span></a> format,
but you can also specify the data is uploaded in any of these
<a class="reference internal" href="../user_guide/dataset_creation/datasets.html#supported-import-formats"><span class="std std-ref">common formats</span></a>. For example, to push the
quickstart dataset in <a class="reference internal" href="../user_guide/dataset_creation/datasets.html#cocodetectiondataset-import"><span class="std std-ref">COCO</span></a> format, with a
Creative Commons Attribution 4.0 license, you can do the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>
<span class="kn">import</span> <span class="nn">fiftyone.types</span> <span class="k">as</span> <span class="nn">fot</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>
<span class="n">dataset_type</span> <span class="o">=</span> <span class="n">fot</span><span class="o">.</span><span class="n">dataset_types</span><span class="o">.</span><span class="n">COCODetectionDataset</span>

<span class="n">push_to_hub</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="s2">&quot;quickstart-coco&quot;</span><span class="p">,</span>
    <span class="n">dataset_type</span><span class="o">=</span><span class="n">dataset_type</span><span class="p">,</span>
    <span class="n">license</span><span class="o">=</span><span class="s2">&quot;cc-by-4.0&quot;</span><span class="p">,</span>
    <span class="n">label_fields</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span>  <span class="c1"># convert all label fields, not just ground truth</span>
<span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>The <code class="code docutils literal notranslate"><span class="pre">label_fields</span></code> argument is used to specify which label fields to convert
to the specified dataset type. By default when using some dataset formats,
only the <code class="code docutils literal notranslate"><span class="pre">ground_truth</span></code> label field is converted. If you want to convert all
label fields, you can set <code class="code docutils literal notranslate"><span class="pre">label_fields=&quot;*&quot;</span></code>. If you want to convert specific
label fields, you can pass a list of field names.</p>
</div>
<p>Additionally, you can specify the minimum version of FiftyOne required to load
the dataset by passing the <code class="code docutils literal notranslate"><span class="pre">min_fiftyone_version</span></code> argument. This is useful when
the dataset utilizes features that are only available in versions above a certain
release. For example, to specify that the dataset requires <code class="code docutils literal notranslate"><span class="pre">fiftyone&gt;=0.23.0</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">push_to_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>

<span class="n">push_to_hub</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="s2">&quot;quickstart-min-version&quot;</span><span class="p">,</span>
    <span class="n">min_fiftyone_version</span><span class="o">=</span><span class="s2">&quot;0.23.0&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="loading-datasets-from-the-hub">
<span id="huggingface-hub-load-dataset"></span><h3>Loading datasets from the Hub<a class="headerlink" href="#loading-datasets-from-the-hub" title="Permalink to this headline">¶</a></h3>
<p>To load a dataset from the Hugging Face Hub, you can use the
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a> function.
This function supports loading datasets in any of the
<a class="reference internal" href="../user_guide/dataset_creation/datasets.html#supported-import-formats"><span class="std std-ref">common formats</span></a> supported by FiftyOne, as well
as image-based datasets stored via <a class="reference external" href="https://parquet.apache.org/">Parquet</a> files,
as is common with datasets from the
<a class="reference external" href="https://huggingface.co/docs/datasets/en/index">datasets</a> library which have
been uploaded to the Hugging Face Hub. Below, we will walk through all of the
ways you can load datasets from the Hub.</p>
<p>In its simplest usage, the
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a> function
only requires the <code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> of the dataset you want to load. For example, to
load the <a class="reference internal" href="#huggingface-hub-push-dataset-advanced"><span class="std std-ref">private dataset</span></a> that we
pushed to the Hub earlier, you can do the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span><span class="s2">&quot;&lt;username-or-org&gt;/my-private-dataset&quot;</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>As long as you have an environment variable <code class="code docutils literal notranslate"><span class="pre">HF_TOKEN</span></code> set with your Hugging
Face token (with read access), you can load private or gated datasets that you have
access to from the Hub.</p>
</div>
<div class="section" id="loading-datasets-from-repo-configs">
<span id="huggingface-hub-load-dataset-from-repo-config"></span><h4>Loading datasets from repo configs<a class="headerlink" href="#loading-datasets-from-repo-configs" title="Permalink to this headline">¶</a></h4>
<p>When you push a dataset to the Hub using
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a>, a <code class="code docutils literal notranslate"><span class="pre">fiftyone.yml</span></code>
config file is generated and uploaded to the repo. This file contains all of the
information necessary to load the dataset from the Hugging Face Hub. More
generally, any repo on the Hugging Face Hub that contains a <code class="code docutils literal notranslate"><span class="pre">fiftyone.yml</span></code> or
<code class="code docutils literal notranslate"><span class="pre">fiftyone.yaml</span></code> file (assuming the file is correctly formatted) can be loaded
using the <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a>
function by passing the <code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> of the dataset, without needing to specify any
additional arguments.</p>
<p>For example, to load the <code class="code docutils literal notranslate"><span class="pre">quickstart</span></code> dataset that we pushed to the Hub earlier,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span><span class="s2">&quot;&lt;username&gt;/my-quickstart-dataset&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="code docutils literal notranslate"><span class="pre">&lt;username&gt;</span></code> is your Hugging Face username or organization name.</p>
</div>
<div class="section" id="loading-datasets-from-local-configs">
<span id="huggingface-hub-load-dataset-from-local-config"></span><h4>Loading datasets from local configs<a class="headerlink" href="#loading-datasets-from-local-configs" title="Permalink to this headline">¶</a></h4>
<p>If the repo was uploaded to the Hugging Face Hub via FiftyOne’s
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.push_to_hub" title="fiftyone.utils.huggingface.push_to_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">push_to_hub()</span></code></a> function, then
the <code class="code docutils literal notranslate"><span class="pre">fiftyone.yml</span></code> config file will be generated and uploaded to the repo.
However, some common datasets like
<a class="reference external" href="https://huggingface.co/datasets/ylecun/mnist">mnist</a> were uploaded to the Hub
using the <code class="code docutils literal notranslate"><span class="pre">datasets</span></code> library and do not contain a <code class="code docutils literal notranslate"><span class="pre">fiftyone.yml</span></code> or
<code class="code docutils literal notranslate"><span class="pre">fiftyone.yaml</span></code> file. If you know how the dataset is structured, you can load
the dataset by passing the path to a local yaml config file that describes the
dataset via the <code class="code docutils literal notranslate"><span class="pre">config_file</span></code> keyword argument.</p>
<p>For example, to load the <code class="code docutils literal notranslate"><span class="pre">mnist</span></code> dataset from the Hub, you might have a local
yaml config file like this:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">format</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ParquetFilesDataset</span>
<span class="nt">classification_fields</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">label</span>
</pre></div>
</div>
<p>To load the dataset from the Hub, you can pass the <code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> of the dataset and
the path to the local yaml config file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;ylecun/mnist&quot;</span><span class="p">,</span>
    <span class="n">config_file</span><span class="o">=</span><span class="s2">&quot;/path/to/mnist.yml&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>For a comprehensive list of the supported fields in the yaml config file, see
<a class="reference internal" href="#huggingface-hub-load-dataset-config-kwargs"><span class="std std-ref">Supported config fields</span></a>.</p>
</div>
<div class="section" id="loading-datasets-with-config-kwargs">
<span id="huggingface-hub-load-dataset-from-kwargs"></span><h4>Loading datasets with config kwargs<a class="headerlink" href="#loading-datasets-with-config-kwargs" title="Permalink to this headline">¶</a></h4>
<p>In addition to loading datasets from repo configs and local configs, you can
also load datasets from the Hub by passing the necessary config arguments
directly to <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a>.
This is useful when you want to load a dataset from the Hub that does not have
a <code class="code docutils literal notranslate"><span class="pre">fiftyone.yml</span></code> or <code class="code docutils literal notranslate"><span class="pre">fiftyone.yaml</span></code> file, and the structure of the dataset is
simple enough that you can specify the necessary arguments directly.</p>
<p>For example, to load the <code class="code docutils literal notranslate"><span class="pre">mnist</span></code> dataset from the Hub, you can pass the <code class="code docutils literal notranslate"><span class="pre">format</span></code>
and <code class="code docutils literal notranslate"><span class="pre">classification_fields</span></code> arguments directly:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;ylecun/mnist&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;ParquetFilesDataset&quot;</span><span class="p">,</span>
    <span class="n">classification_fields</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>This will tell FiftyOne that the data is stored in Parquet files, and that the
<code class="code docutils literal notranslate"><span class="pre">label</span></code> field should be treated as a classification field, to be converted into
a <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Classification" title="fiftyone.core.labels.Classification"><code class="xref py py-class docutils literal notranslate"><span class="pre">Classification</span></code></a> label field in the dataset.</p>
</div>
<div class="section" id="supported-config-fields">
<span id="huggingface-hub-load-dataset-config-kwargs"></span><h4>Supported config fields<a class="headerlink" href="#supported-config-fields" title="Permalink to this headline">¶</a></h4>
<p>Whether you are loading a dataset from a repo config, a local config file, or
passing the config arguments directly, you can specify a number of fields.</p>
<p>Broadly speaking, these fields fall into three categories: format specification,
media field specification, and label field specification.</p>
<p>Let’s look at these categories in more detail:</p>
<p><strong>Format specification</strong>:</p>
<ul class="simple">
<li><p><strong>format</strong> <em>(str)</em>: The format of the dataset. This can be any of the
<a class="reference internal" href="../user_guide/dataset_creation/datasets.html#supported-import-formats"><span class="std std-ref">common formats</span></a> supported by FiftyOne — just
pass the name of the format as a string. For example, to load a dataset in the
<a class="reference internal" href="../user_guide/dataset_creation/datasets.html#cocodetectiondataset-import"><span class="std std-ref">COCO</span></a> format, you can pass
<code class="code docutils literal notranslate"><span class="pre">format=&quot;COCODetectionDataset&quot;</span></code>. To specify that the dataset is stored in
Parquet files, you can pass <code class="code docutils literal notranslate"><span class="pre">format=&quot;ParquetFilesDataset&quot;</span></code> (or simply
<code class="code docutils literal notranslate"><span class="pre">format=&quot;parquet&quot;</span></code> for short). This is the only required field.</p></li>
<li><p><strong>name</strong> <em>(str)</em>: The name of the FiftyOne <a class="reference internal" href="../api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset" title="fiftyone.core.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> to be created. If the
<code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> is cumbersome, this can be used to specify a simpler default name.
For example, for this <a class="reference external" href="https://huggingface.co/datasets/keremberke/aerial-sheep-object-detection">sheep dataset</a>
rather than using the <code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> <code class="code docutils literal notranslate"><span class="pre">keremberke/aerial-sheep-object-detection</span></code>, you
can specify <code class="code docutils literal notranslate"><span class="pre">name=&quot;sheep-detection&quot;</span></code>.</p></li>
<li><p><strong>subsets</strong> <em>(str or list)</em>: The subset or subsets of the Hugging Face
dataset that are <em>compatible</em> with this config, and are <em>available</em> to be
loaded. In Hugging Face, the “dataset” in a repo can contain multiple
“subsets”, which may or may not have the same schema. Take the
<a class="reference external" href="https://huggingface.co/datasets/ufldl-stanford/svhn">Street View House Numbers</a> dataset for
example. This dataset has two subsets: <code class="code docutils literal notranslate"><span class="pre">&quot;cropped_digits&quot;</span></code> and <code class="code docutils literal notranslate"><span class="pre">&quot;full_numbers&quot;</span></code>.
The <code class="code docutils literal notranslate"><span class="pre">cropped_digits</span></code> subset contains classification labels, while the
<code class="code docutils literal notranslate"><span class="pre">full_numbers</span></code> subset contains detection labels. A single config would not be
able to specify the schema for both subsets, so you can specify the subset you
want to load (or if you are the dataset author, which subset you want to <em>allow</em>
people to load in this way) with the <code class="code docutils literal notranslate"><span class="pre">subsets</span></code> field. For example, to load the
<code class="code docutils literal notranslate"><span class="pre">cropped_digits</span></code> subset of the SVHN dataset, you can pass
<code class="code docutils literal notranslate"><span class="pre">subsets=&quot;cropped_digits&quot;</span></code>. Note that this is not a required field, and by
default all subsets are loaded. Also note that subsets are distinct from splits
in the dataset, which are handled by the <code class="code docutils literal notranslate"><span class="pre">splits</span></code> field (see below).</p></li>
<li><p><strong>splits</strong> <em>(str or list)</em>: The split or splits of the Hugging Face dataset that
are <em>compatible</em> with this config, and are <em>available</em> to be loaded. As is
standard for machine learning, many datasets are split into training, validation,
and test sets. The specific names of these splits may vary from dataset to
dataset, but <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a>
identifies the names of all splits and by default, will assume that all of
these splits are to be loaded. If you only want to load a specific split or
splits, you can specify them with the <code class="code docutils literal notranslate"><span class="pre">splits</span></code> field. For example, to load the
training split of the <a class="reference external" href="https://huggingface.co/datasets//uoft-cs/cifar10">CIFAR10</a>
dataset, you can pass <code class="code docutils literal notranslate"><span class="pre">splits=&quot;train&quot;</span></code>. If you want to load multiple splits,
you can pass them as a list, e.g., <code class="code docutils literal notranslate"><span class="pre">splits=[&quot;train&quot;,</span> <span class="pre">&quot;test&quot;]</span></code>. Note that this
is not a required field, and by default all splits are loaded.</p></li>
</ul>
<p><strong>Media field specification</strong>:</p>
<p>While not all Parquet datasets contain media fields, all FiftyOne <a class="reference internal" href="../api/fiftyone.core.sample.html#fiftyone.core.sample.Sample" title="fiftyone.core.sample.Sample"><code class="xref py py-class docutils literal notranslate"><span class="pre">Sample</span></code></a> objects
must be connected to at least one media file. The following fields can be used
to configure the media fields in the Hugging Face dataset that should be converted
to FiftyOne media fields:</p>
<ul class="simple">
<li><p><strong>filepath</strong> <em>(str)</em>: In FiftyOne, <code class="code docutils literal notranslate"><span class="pre">filepath</span></code> is
<a class="reference external" href="https://docs.voxel51.com/user_guide/using_datasets.html#fields">a default field</a>
that is used to store the path to the primary media file for each sample in
the dataset. For Hugging Face parquet datasets, primary media fields for image
datasets are typically stored in the <code class="code docutils literal notranslate"><span class="pre">image</span></code> columns, so this is where
FiftyOne’s <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a>
looks by default. If the primary media field is stored in a different column,
you can specify the column name with the key <code class="code docutils literal notranslate"><span class="pre">filepath</span></code>. For example, the
<a class="reference external" href="https://huggingface.co/datasets/kakaobrain/coyo-700m">COYO-700M dataset</a>
has the primary media field referenced in the <code class="code docutils literal notranslate"><span class="pre">url</span></code> column. Specifying
<code class="code docutils literal notranslate"><span class="pre">filepath=&quot;url&quot;</span></code> will tell FiftyOne to look in the <code class="code docutils literal notranslate"><span class="pre">url</span></code> column for the
primary media file path. Images will be downloaded from the corresponding URLs
and saved to disk.</p></li>
<li><p><strong>thumbnail_path</strong> <em>(str)</em>: The field containing the path to a thumbnail image
for each sample in the dataset, if such a field exists. If a <code class="code docutils literal notranslate"><span class="pre">thumbnail_path</span></code>
is specified, this media file will be shown in the sample grid in the FiftyOne
App. This can be useful for quickly visualizing the dataset when the primary
media field contains large (e.g., high-resolution) images. For more information
on thumbnail images, see <a class="reference internal" href="../user_guide/app.html#app-multiple-media-fields"><span class="std std-ref">this section</span></a>.</p></li>
<li><p><strong>additional_media_fields</strong> <em>(dict)</em>: If each sample has multiple associated media
files that you may want to visualize in the FiftyOne App, you can specify
these non-default media fields in the <code class="code docutils literal notranslate"><span class="pre">additional_media_fields</span></code> dictionary,
where the keys are the column names in the Hugging Face dataset and the values
are the names of the fields in the FiftyOne <a class="reference internal" href="../api/fiftyone.core.dataset.html#fiftyone.core.dataset.Dataset" title="fiftyone.core.dataset.Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">Dataset</span></code></a> that will store the
paths. Note that this is <em>not</em> the same as <a class="reference internal" href="../user_guide/groups.html#groups"><span class="std std-ref">grouped datasets</span></a>.</p></li>
</ul>
<p><strong>Label field specification</strong>:</p>
<p>FiftyOne’s Hugging Face Hub integration currently supports converting labels of
type <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Classification" title="fiftyone.core.labels.Classification"><code class="xref py py-class docutils literal notranslate"><span class="pre">Classification</span></code></a>, <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detections" title="fiftyone.core.labels.Detections"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detections</span></code></a>, and <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Segmentation" title="fiftyone.core.labels.Segmentation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Segmentation</span></code></a> from Hugging Face
Parquet datasets to FiftyOne label fields. The following fields can be used to
specify the label fields in the Hugging Face dataset that should be converted to
FiftyOne label fields:</p>
<ul class="simple">
<li><p><strong>classification_fields</strong> <em>(str or list)</em>: The column or columns in the Hugging
Face dataset that should be converted to FiftyOne <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Classification" title="fiftyone.core.labels.Classification"><code class="xref py py-class docutils literal notranslate"><span class="pre">Classification</span></code></a> label fields.
contain classification labels. For example, if the dataset contains a <code class="code docutils literal notranslate"><span class="pre">label</span></code>
field that contains classification labels, you can specify
<code class="code docutils literal notranslate"><span class="pre">classification_fields=&quot;label&quot;</span></code>. If the dataset contains multiple
classification fields, you can specify them as a list, e.g.,
<code class="code docutils literal notranslate"><span class="pre">classification_fields=[&quot;label1&quot;,</span> <span class="pre">&quot;label2&quot;]</span></code>. This is not a required field,
and if the dataset does not contain classification labels, you can omit it.</p></li>
<li><p><strong>detection_fields</strong> <em>(str or list)</em>: The column or columns in the Hugging Face
dataset that should be converted to FiftyOne <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Detections" title="fiftyone.core.labels.Detections"><code class="xref py py-class docutils literal notranslate"><span class="pre">Detections</span></code></a> label fields. If the
dataset contains detection labels, you can specify the column name or names
here. For example, if the dataset contains a <code class="code docutils literal notranslate"><span class="pre">detections</span></code> field that contains
detection labels, you can specify <code class="code docutils literal notranslate"><span class="pre">detection_fields=&quot;detections&quot;</span></code>. If the
dataset contains multiple detection fields, you can specify them as a list,
e.g., <code class="code docutils literal notranslate"><span class="pre">detection_fields=[&quot;detections1&quot;,</span> <span class="pre">&quot;detections2&quot;]</span></code>. This is not a required
field, and if the dataset does not contain detection labels, you can omit it.</p></li>
<li><p><strong>mask_fields</strong> <em>(str or list)</em>: The column or columns in the Hugging Face dataset
that should be converted to FiftyOne <a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Segmentation" title="fiftyone.core.labels.Segmentation"><code class="xref py py-class docutils literal notranslate"><span class="pre">Segmentation</span></code></a> label fields. The column
in the Hugging Face dataset must contain an image or the URL for an image that
can be used as a segmentation mask. If necessary, the images will be downloaded
and saved to disk. If the dataset contains mask labels, you can specify the
column name or names here. For example, if the dataset contains a <code class="code docutils literal notranslate"><span class="pre">masks</span></code> field
that contains mask labels, you can specify <code class="code docutils literal notranslate"><span class="pre">mask_fields=&quot;masks&quot;</span></code>. This is not
a required field, and if the dataset does not contain mask labels, you can
omit it.</p></li>
</ul>
</div>
<div class="section" id="configuring-the-download-process">
<span id="huggingface-hub-load-dataset-download"></span><h4>Configuring the download process<a class="headerlink" href="#configuring-the-download-process" title="Permalink to this headline">¶</a></h4>
<p>When loading datasets from the Hugging Face Hub, FiftyOne will download the
<em>all</em> of the data specified by the <code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> and the config. If no splits or
subsets are listed in the config, this means that all samples across all splits
and subsets will be downloaded. This can be a time-consuming process, especially
for large datasets, and sometimes you may only want to download a fixed number
of samples to get started exploring the dataset.</p>
<p>FiftyOne’s <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a>
function supports a variety of arguments that allow you to control the download
process, from the maximum number of samples to be downloaded to the batch size
to use when making requests to the Datasets Server. Here are the supported
arguments:</p>
<ul class="simple">
<li><p><strong>max_samples</strong> <em>(int)</em>: The number of samples to download from the dataset.
If not specified, all samples will be downloaded.</p></li>
<li><p><strong>batch_size</strong> <em>(int)</em>: The batch size to use when making requests to the
Datasets Server. Defaults to 100, which is the max batch size allowed by the
Datasets Server.</p></li>
<li><p><strong>num_workers</strong> <em>(int)</em>: The number of worker to use when downloading
media files. If not specified, the number of workers will be resolved by
looking at your <a class="reference internal" href="../user_guide/config.html#configuring-fiftyone"><span class="std std-ref">FiftyOne Config</span></a>.</p></li>
<li><p><strong>splits</strong> <em>(str or list)</em>: The split or splits of the Hugging Face dataset
that you want to download. This overrides the <code class="code docutils literal notranslate"><span class="pre">splits</span></code> field in the config.</p></li>
<li><p><strong>subsets</strong> <em>(str or list)</em>: The subset or subsets of the Hugging Face dataset
that you want to download. This overrides the <code class="code docutils literal notranslate"><span class="pre">subsets</span></code> field in the config.</p></li>
<li><p><strong>overwrite</strong> <em>(bool)</em>: Whether to overwrite existing an existing dataset
with the same name. If <code class="code docutils literal notranslate"><span class="pre">True</span></code>, the existing dataset will be overwritten. If
<code class="code docutils literal notranslate"><span class="pre">False</span></code>, an error will be raised if a dataset with the same name already
exists. Defaults to <code class="code docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>persistent</strong> <em>(bool)</em>: Whether to persist the dataset to the underlying
database after it is loaded. If <code class="code docutils literal notranslate"><span class="pre">True</span></code>, the dataset will be available for
loading in future FiftyOne sessions by passing the dataset’s name into
FiftyOne’s
<code class="xref py py-func docutils literal notranslate"><span class="pre">load_dataset()</span></code> function.
Defaults to <code class="code docutils literal notranslate"><span class="pre">False</span></code>.</p></li>
<li><p><strong>revision</strong> <em>(str)</em>: The revision (specified by a commit hash to the Hugging
Face repo) of the dataset to load. If not specified, the latest revision will
be loaded.</p></li>
</ul>
</div>
<div class="section" id="basic-examples">
<span id="huggingface-hub-load-dataset-basic-examples"></span><h4>Basic examples<a class="headerlink" href="#basic-examples" title="Permalink to this headline">¶</a></h4>
<p>Okay, so <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a> is
<em>very</em> powerful, and can be used in a <em>ton</em> of ways. All of this flexibility
can be a bit overwhelming, so let’s walk through a few examples to show you how
easy it is in practice to load datasets from the Hugging Face Hub.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>To make these downloads as fast as possible, we recommend setting the
<code class="code docutils literal notranslate"><span class="pre">max_samples</span></code> argument to a reasonable number, like 1000, to get a feel for
the dataset. If you like what you see, you can always download more samples!</p>
</div>
<p><strong>Classification Datasets</strong></p>
<p>Let’s start by loading the
<a class="reference external" href="https://huggingface.co/datasets/ylecun/mnist">MNIST</a> dataset into FiftyOne. All you
need to do is pass the <code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> of the dataset — in this case <code class="code docutils literal notranslate"><span class="pre">&quot;ylecun/mnist&quot;</span></code> — to
<a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a>, specify the
format as <code class="code docutils literal notranslate"><span class="pre">&quot;parquet&quot;</span></code>, and specify the <code class="code docutils literal notranslate"><span class="pre">classification_fields</span></code> as <code class="code docutils literal notranslate"><span class="pre">&quot;label&quot;</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;ylecun/mnist&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">classification_fields</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>The same exact syntax works for the <a class="reference external" href="https://huggingface.co/datasets/cifar10">CIFAR-10</a>
and <a class="reference external" href="https://huggingface.co/datasets/zalando-datasets/fashion_mnist">FashionMNIST</a> datasets,
which are also available on the Hub. In fact, you can load any of the following
classification datasets from the Hub using the same syntax, just by changing the
<code class="code docutils literal notranslate"><span class="pre">repo_id</span></code>:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/datasets/uoft-cs/cifar10">CIFAR-10</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;uoft-cs/cifar10&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/ILSVRC/imagenet-1k">ImageNet</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;ILSVRC/imagenet-1k&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/zalando-datasets/fashion_mnist">FashionMNIST</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;zalando-datasets/fashion_mnist&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/zh-plus/tiny-imagenet">Tiny ImageNet</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;zh-plus/tiny-imagenet&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/ethz/food101">Food-101</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;ethz/food101&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/sasha/dog-food">Dog Food</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;sasha/dog-food&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/songweig/imagenet_sketch">ImageNet-Sketch</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;songweig/imagenet_sketch&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/nelorth/oxford-flowers">Oxford Flowers</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;nelorth/oxford-flowers&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/microsoft/cats_vs_dogs">Cats vs. Dogs</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;microsoft/cats_vs_dogs&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/timm/objectnet">ObjectNet-1.0</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;timm/objectnet&quot;</span></code>)</p></li>
</ul>
<p>A very similar syntax can be used to load classification datasets that contain
<em>multiple</em> classification fields, such as
<a class="reference external" href="https://huggingface.co/datasets/uoft-cs/cifar100">CIFAR-100</a> and the
<a class="reference external" href="https://huggingface.co/datasets/huggan/wikiart">WikiArt</a> dataset. For example,
to load the CIFAR-100 dataset, you can specify the <code class="code docutils literal notranslate"><span class="pre">classification_fields</span></code> as
<code class="code docutils literal notranslate"><span class="pre">[&quot;coarse_label&quot;,</span> <span class="pre">&quot;fine_label&quot;]</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;uoft-cs/cifar100&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">classification_fields</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;coarse_label&quot;</span><span class="p">,</span> <span class="s2">&quot;fine_label&quot;</span><span class="p">],</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>To load the <a class="reference external" href="https://huggingface.co/datasets/huggan/wikiart">WikiArt</a> dataset,
you can specify the <code class="code docutils literal notranslate"><span class="pre">classification_fields</span></code> as <code class="code docutils literal notranslate"><span class="pre">[&quot;artist&quot;,</span> <span class="pre">&quot;genre&quot;,</span> <span class="pre">&quot;style&quot;]</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;huggan/wikiart&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">classification_fields</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;artist&quot;</span><span class="p">,</span> <span class="s2">&quot;genre&quot;</span><span class="p">,</span> <span class="s2">&quot;style&quot;</span><span class="p">],</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>As touched upon earlier, you can also load a classification <em>subset</em> of a
dataset. For example, to load the <code class="code docutils literal notranslate"><span class="pre">cropped_digits</span></code> subset of the
<a class="reference external" href="https://huggingface.co/datasets/svhn">Street View House Numbers</a> dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;ufldl-stanford/svhn&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">classification_fields</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
    <span class="n">subsets</span><span class="o">=</span><span class="s2">&quot;cropped_digits&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Detection Datasets</strong></p>
<p>Loading detection datasets from the Hub is just as easy. For example, to load
the <a class="reference external" href="https://huggingface.co/datasets/detection-datasets/coco">MS COCO</a>
dataset, you can specify the <code class="code docutils literal notranslate"><span class="pre">detection_fields</span></code> as <code class="code docutils literal notranslate"><span class="pre">&quot;objects&quot;</span></code>, which is the
standard column name for detection features in Hugging Face datasets:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;detection-datasets/coco&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">detection_fields</span><span class="o">=</span><span class="s2">&quot;objects&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>The same syntax works for many other popular detection datasets on the Hub,
including:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/datasets/rishitdagli/cppe-5">CPPE - 5</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;rishitdagli/cppe-5&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/CUHK-CSE/wider_face">WIDER FACE</a> (use <code class="code docutils literal notranslate"><span class="pre">&quot;CUHK-CSE/wider_face&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/keremberke/license-plate-object-detection">License Plate Object Detection</a>
(use <code class="code docutils literal notranslate"><span class="pre">&quot;keremberke/license-plate-object-detection&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/keremberke/aerial-sheep-object-detection">Aerial Sheep Object Detection</a>
(use <code class="code docutils literal notranslate"><span class="pre">&quot;keremberke/aerial-sheep-object-detection&quot;</span></code>)</p></li>
</ul>
<p>Some detection datasets have their detections stored under a column with a
different name. For example, the <code class="code docutils literal notranslate"><span class="pre">full_numbers</span></code> subset of the
<a class="reference external" href="https://huggingface.co/datasets/ufldl-stanford/svhn">Street View House Numbers</a> dataset
stores its detections under the column <code class="code docutils literal notranslate"><span class="pre">digits</span></code>. To load this subset, you can
specify the <code class="code docutils literal notranslate"><span class="pre">detection_fields</span></code> as <code class="code docutils literal notranslate"><span class="pre">&quot;digits&quot;</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;ufldl-stanford/svhn&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">detection_fields</span><span class="o">=</span><span class="s2">&quot;digits&quot;</span><span class="p">,</span>
    <span class="n">subsets</span><span class="o">=</span><span class="s2">&quot;full_numbers&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Not <em>all</em> detection datasets on the Hub are stored in a format that is
currently supported by FiftyOne. For instance, the
<a class="reference external" href="https://huggingface.co/datasets/detection-datasets/fashionpedia">Fashionpedia</a>
dataset has detections stored in Pascal VOC format, which is not the <a class="reference external" href="https://huggingface.co/docs/transformers/en/tasks/object_detection">standard
Hugging Face format</a>.</p>
</div>
<p><strong>Segmentation Datasets</strong></p>
<p>Loading segmentation datasets from the Hub is also a breeze. For example, to
load the “instance_segmentation” subset from
<a class="reference external" href="https://huggingface.co/datasets/zhoubolei/scene_parse_150">SceneParse150</a>, all you
need to do is specify the <code class="code docutils literal notranslate"><span class="pre">mask_fields</span></code> as <code class="code docutils literal notranslate"><span class="pre">&quot;annotation&quot;</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;zhoubolei/scene_parse150&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">subsets</span><span class="o">=</span><span class="s2">&quot;instance_segmentation&quot;</span><span class="p">,</span>
    <span class="n">mask_fields</span><span class="o">=</span><span class="s2">&quot;annotation&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>Many other segmentation datasets on the Hub can be loaded in the same way, such
as <a class="reference external" href="https://huggingface.co/datasets/nateraw/ade20k-tiny">ADE 20K Tiny</a>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;nateraw/ade20k-tiny&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">mask_fields</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># only 20 samples in the dataset</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>In other cases, because there are now <em>multiple</em> image columns — one for the
sample image and one for the mask — the naming convention for the dataset might
be different, and you may need to explicitly specify the <code class="code docutils literal notranslate"><span class="pre">filepath</span></code>. For
example, to load the
<a class="reference external" href="https://huggingface.co/datasets/segments/sidewalk-semantic">Sidewalk Semantic</a>
dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="c1"># Note: you need access to the dataset to load it!</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;segments/sidewalk-semantic&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">filepath</span><span class="o">=</span><span class="s2">&quot;pixel_values&quot;</span><span class="p">,</span>
    <span class="n">mask_fields</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Once you have the dataset loaded into FiftyOne, you may want to set the
dataset’s <a class="reference external" href="storing-mask-targets">mask targets</a> to specify the names of
the classes represented in the segmentation masks.</p>
</div>
<p><strong>Unlabelled Image Datasets</strong></p>
<p>Some datasets on the Hub contain images and metadata in the form of features,
but do not explicitly contain classification, detection, or segmentation labels.
This is common for text-to-image tasks, as well as captioning and visual question
answering tasks. These datasets can also be converted and loaded into FiftyOne!
Once the dataset is loaded into FiftyOne, you can process the data and generate
labels for whatever tasks you are interested in.</p>
<p>Let’s look at a few examples:</p>
<p>For <a class="reference external" href="https://huggingface.co/datasets/poloclub/diffusiondb">DiffusionDB</a>, you
can load the dataset as follows:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;poloclub/diffusiondb&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>Here are some other popular datasets on the Hub that can be loaded following the
same syntax:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://huggingface.co/datasets/m1guelpf/nouns">Nouns</a>: (use <code class="code docutils literal notranslate"><span class="pre">&quot;m1guelpf/nouns&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/jmhessel/newyorker_caption_contest">New Yorker Caption Contest</a>:
(use <code class="code docutils literal notranslate"><span class="pre">&quot;jmhessel/newyorker_caption_contest&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/project-sloth/captcha-images">Captcha Dataset</a>:
(use <code class="code docutils literal notranslate"><span class="pre">&quot;project-sloth/captcha-images&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/AI4Math/MathVista">MathVista</a>: (use <code class="code docutils literal notranslate"><span class="pre">&quot;AI4Math/MathVista&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/TextVQA">TextVQA</a>: (use <code class="code docutils literal notranslate"><span class="pre">&quot;textvqa&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/flaviagiammarino/vqa-rad">VQA-RAD</a>: (use <code class="code docutils literal notranslate"><span class="pre">&quot;flaviagiammarino/vqa-rad&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/derek-thomas/ScienceQA">ScienceQA</a>: (use <code class="code docutils literal notranslate"><span class="pre">&quot;derek-thomas/ScienceQA&quot;</span></code>)</p></li>
<li><p><a class="reference external" href="https://huggingface.co/datasets/flaviagiammarino/path-vqa">PathVQA</a>: (use <code class="code docutils literal notranslate"><span class="pre">&quot;flaviagiammarino/path-vqa&quot;</span></code>)</p></li>
</ul>
<p>Many other popular datasets on the Hub can be loaded in the same way, with slight
modifications to <code class="code docutils literal notranslate"><span class="pre">filepath</span></code> or other arguments as needed. Here are a few examples:</p>
<p>For <a class="reference external" href="https://huggingface.co/datasets/kakaobrain/coyo-700m">COYO-700M</a>, we just
need to specify the <code class="code docutils literal notranslate"><span class="pre">filepath</span></code> as <code class="code docutils literal notranslate"><span class="pre">&quot;url&quot;</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;kakaobrain/coyo-700m&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">filepath</span><span class="o">=</span><span class="s2">&quot;url&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>For <a class="reference external" href="https://huggingface.co/datasets/kdexd/red_caps">RedCaps</a>, we instead use
<code class="code docutils literal notranslate"><span class="pre">&quot;image_url&quot;</span></code> as the <code class="code docutils literal notranslate"><span class="pre">filepath</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;kdexd/red_caps&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">filepath</span><span class="o">=</span><span class="s2">&quot;image_url&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>For <a class="reference external" href="https://huggingface.co/datasets/MMMU/MMMU">MMMU</a>
(A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for
Expert AGI), we use <code class="code docutils literal notranslate"><span class="pre">&quot;image_1&quot;</span></code> as the <code class="code docutils literal notranslate"><span class="pre">filepath</span></code>:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;MMMU/MMMU&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">filepath</span><span class="o">=</span><span class="s2">&quot;image_1&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="advanced-examples">
<span id="huggingface-hub-load-dataset-advanced-examples"></span><h4>Advanced examples<a class="headerlink" href="#advanced-examples" title="Permalink to this headline">¶</a></h4>
<p>The <a class="reference internal" href="../api/fiftyone.utils.huggingface.html#fiftyone.utils.huggingface.load_from_hub" title="fiftyone.utils.huggingface.load_from_hub"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_from_hub()</span></code></a> function
also allows us to load datasets in much more complex formats, as well as with
more advanced configurations. Let’s walk through a few examples to show you how
to leverage the full power of FiftyOne’s Hugging Face Hub integration.</p>
<p><strong>Loading Datasets from Revisions</strong></p>
<p>When you load a dataset from the Hugging Face Hub, you are loading the latest
revision of the dataset. However, you can also load a specific revision of the
dataset by specifying the <code class="code docutils literal notranslate"><span class="pre">revision</span></code> argument. For example, to load the last
revision of DiffusionDB before NSFW scores were added, you can specify this via:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;poloclub/diffusiondb&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">subset</span><span class="o">=</span><span class="s2">&quot;2m_random_1k&quot;</span><span class="p">,</span> <span class="c1">## just one of the subsets</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">revision</span><span class="o">=</span><span class="s2">&quot;5fa48ba66a44822d82d024d195fbe918e6c42ca6&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Loading Datasets with Multiple Media Fields</strong></p>
<p>Some datasets on the Hub contain multiple media fields for each sample. Take
<a class="reference external" href="https://huggingface.co/datasets/magicbrush">MagicBrush</a> for example, which
contains a <code class="code docutils literal notranslate"><span class="pre">&quot;source_img&quot;</span></code> and a <code class="code docutils literal notranslate"><span class="pre">&quot;target_img&quot;</span></code> for each sample, in addition
to a segmentation mask denoting the area of the source image to be modified. To
load this dataset, you can specify the <code class="code docutils literal notranslate"><span class="pre">filepath</span></code> as <code class="code docutils literal notranslate"><span class="pre">&quot;source_img&quot;</span></code> and the
target image via <code class="code docutils literal notranslate"><span class="pre">additional_media_fields</span></code>. Because this is getting a bit more
complex, we’ll create a local yaml config file to specify the dataset format:</p>
<div class="highlight-yaml notranslate"><div class="highlight"><pre><span></span><span class="nt">format</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">ParquetFilesDataset</span>
<span class="nt">name</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">magicbrush</span>
<span class="nt">filepath</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">source_img</span>
<span class="nt">additional_media_fields</span><span class="p">:</span>
<span class="w">    </span><span class="nt">target_img</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">target_img</span>
<span class="nt">mask_fields</span><span class="p">:</span><span class="w"> </span><span class="l l-Scalar l-Scalar-Plain">mask_img</span>
</pre></div>
</div>
<p>Now, you can load the dataset using the local yaml config file:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;osunlp/MagicBrush&quot;</span><span class="p">,</span>
    <span class="n">config_file</span><span class="o">=</span><span class="s2">&quot;/path/to/magicbrush.yml&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Customizing the Download Process</strong></p>
<p>When loading datasets from the Hub, you can customize the download process by
specifying the <code class="code docutils literal notranslate"><span class="pre">batch_size</span></code>, <code class="code docutils literal notranslate"><span class="pre">num_workers</span></code>, and <code class="code docutils literal notranslate"><span class="pre">overwrite</span></code> arguments. For
example, to download the <code class="code docutils literal notranslate"><span class="pre">full_numbers</span></code> subset of the <a class="reference external" href="https://huggingface.co/datasets/ufldl-stanford/svhn">Street View House Numbers</a> dataset with a batch size of 50 and 4
workers, you can do the following:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;ufldl-stanford/svhn&quot;</span><span class="p">,</span>
    <span class="nb">format</span><span class="o">=</span><span class="s2">&quot;parquet&quot;</span><span class="p">,</span>
    <span class="n">detection_fields</span><span class="o">=</span><span class="s2">&quot;digits&quot;</span><span class="p">,</span>
    <span class="n">subsets</span><span class="o">=</span><span class="s2">&quot;full_numbers&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Loading Private or Gated Datasets</strong></p>
<p>Like public datasets, you can also load private or gated datasets from the Hub,
as long as you have the necessary permissions. If your Hugging Face token is
set as an environment variable <code class="code docutils literal notranslate"><span class="pre">HF_TOKEN</span></code>, this is as simple as specifying the
<code class="code docutils literal notranslate"><span class="pre">repo_id</span></code> of the dataset. If you don’t have your token set, or you need to use
a specific token for a specific dataset, you can specify the <code class="code docutils literal notranslate"><span class="pre">token</span></code> argument.
You can do so following this recipe:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="kn">import</span> <span class="n">load_from_hub</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">load_from_hub</span><span class="p">(</span>
    <span class="s2">&quot;my-private-dataset-repo-id&quot;</span><span class="p">,</span>
    <span class="n">token</span><span class="o">=</span><span class="s2">&quot;&lt;my-secret-token&gt;&quot;</span><span class="p">,</span>
    <span class="o">...</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="ultralytics.html" class="btn btn-neutral float-right" title="Ultralytics Integration" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="lancedb.html" class="btn btn-neutral" title="LanceDB Integration" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  
</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Hugging Face Integration</a><ul>
<li><a class="reference internal" href="#transformers-library">Transformers Library</a><ul>
<li><a class="reference internal" href="#setup">Setup</a></li>
<li><a class="reference internal" href="#inference">Inference</a><ul>
<li><a class="reference internal" href="#image-classification">Image classification</a></li>
<li><a class="reference internal" href="#object-detection">Object detection</a></li>
<li><a class="reference internal" href="#semantic-segmentation">Semantic segmentation</a></li>
<li><a class="reference internal" href="#monocular-depth-estimation">Monocular depth estimation</a></li>
<li><a class="reference internal" href="#zero-shot-classification">Zero-shot classification</a></li>
<li><a class="reference internal" href="#zero-shot-object-detection">Zero-shot object detection</a></li>
<li><a class="reference internal" href="#batch-inference">Batch inference</a></li>
</ul>
</li>
<li><a class="reference internal" href="#embeddings">Embeddings</a><ul>
<li><a class="reference internal" href="#image-embeddings">Image embeddings</a></li>
<li><a class="reference internal" href="#text-embeddings">Text embeddings</a></li>
<li><a class="reference internal" href="#batch-embeddings">Batch embeddings</a></li>
<li><a class="reference internal" href="#patch-embeddings">Patch embeddings</a></li>
</ul>
</li>
<li><a class="reference internal" href="#brain-methods">Brain methods</a></li>
</ul>
</li>
<li><a class="reference internal" href="#huggingface-hub">Hugging Face Hub</a><ul>
<li><a class="reference internal" href="#huggingface-hub-setup">Setup</a></li>
<li><a class="reference internal" href="#pushing-datasets-to-the-hub">Pushing datasets to the Hub</a><ul>
<li><a class="reference internal" href="#basic-usage">Basic usage</a></li>
<li><a class="reference internal" href="#pushing-large-datasets">Pushing large datasets</a></li>
<li><a class="reference internal" href="#advanced-usage">Advanced usage</a></li>
</ul>
</li>
<li><a class="reference internal" href="#loading-datasets-from-the-hub">Loading datasets from the Hub</a><ul>
<li><a class="reference internal" href="#loading-datasets-from-repo-configs">Loading datasets from repo configs</a></li>
<li><a class="reference internal" href="#loading-datasets-from-local-configs">Loading datasets from local configs</a></li>
<li><a class="reference internal" href="#loading-datasets-with-config-kwargs">Loading datasets with config kwargs</a></li>
<li><a class="reference internal" href="#supported-config-fields">Supported config fields</a></li>
<li><a class="reference internal" href="#configuring-the-download-process">Configuring the download process</a></li>
<li><a class="reference internal" href="#basic-examples">Basic examples</a></li>
<li><a class="reference internal" href="#advanced-examples">Advanced examples</a></li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
         <script src="../_static/js/voxel51-website.js"></script>
         <script src="../_static/js/custom.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


  <div class="container-fluid docs-tutorials-resources" id="docs-tutorials-resources">
    <div class="container">
      <div class="row">
        <div class="col-md-4 text-center">
          <h2>Docs</h2>
          <p>Access comprehensive developer documentation for PyTorch</p>
          <a class="with-right-arrow" href="https://pytorch.org/docs/stable/index.html">View Docs</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Tutorials</h2>
          <p>Get in-depth tutorials for beginners and advanced developers</p>
          <a class="with-right-arrow" href="https://pytorch.org/tutorials">View Tutorials</a>
        </div>

        <div class="col-md-4 text-center">
          <h2>Resources</h2>
          <p>Find development resources and get your questions answered</p>
          <a class="with-right-arrow" href="https://pytorch.org/resources">View Resources</a>
        </div>
      </div>
    </div>
  </div>

  <footer class="site-footer">
    <div class="container footer-container">
      <div class="footer-logo-wrapper">
        <a href="https://pytorch.org/" class="footer-logo"></a>
      </div>

      <div class="footer-links-wrapper">
        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/">PyTorch</a></li>
            <li><a href="https://pytorch.org/get-started">Get Started</a></li>
            <li><a href="https://pytorch.org/features">Features</a></li>
            <li><a href="https://pytorch.org/ecosystem">Ecosystem</a></li>
            <li><a href="https://pytorch.org/blog/">Blog</a></li>
            <li><a href="https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md">Contributing</a></li>
          </ul>
        </div>

        <div class="footer-links-col">
          <ul>
            <li class="list-title"><a href="https://pytorch.org/resources">Resources</a></li>
            <li><a href="https://pytorch.org/tutorials">Tutorials</a></li>
            <li><a href="https://pytorch.org/docs/stable/index.html">Docs</a></li>
            <li><a href="https://discuss.pytorch.org" target="_blank">Discuss</a></li>
            <li><a href="https://github.com/pytorch/pytorch/issues" target="_blank">Github Issues</a></li>
            <li><a href="https://pytorch.org/assets/brand-guidelines/PyTorch-Brand-Guidelines.pdf" target="_blank">Brand Guidelines</a></li>
          </ul>
        </div>

        <div class="footer-links-col follow-us-col">
          <ul>
            <li class="list-title">Stay Connected</li>
            <li>
              <div id="mc_embed_signup">
                <form
                  action="https://twitter.us14.list-manage.com/subscribe/post?u=75419c71fe0a935e53dfa4a3f&id=91d0dccd39"
                  method="post"
                  id="mc-embedded-subscribe-form"
                  name="mc-embedded-subscribe-form"
                  class="email-subscribe-form validate"
                  target="_blank"
                  novalidate>
                  <div id="mc_embed_signup_scroll" class="email-subscribe-form-fields-wrapper">
                    <div class="mc-field-group">
                      <label for="mce-EMAIL" style="display:none;">Email Address</label>
                      <input type="email" value="" name="EMAIL" class="required email" id="mce-EMAIL" placeholder="Email Address">
                    </div>

                    <div id="mce-responses" class="clear">
                      <div class="response" id="mce-error-response" style="display:none"></div>
                      <div class="response" id="mce-success-response" style="display:none"></div>
                    </div>    <!-- real people should not fill this in and expect good things - do not remove this or risk form bot signups-->

                    <div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_75419c71fe0a935e53dfa4a3f_91d0dccd39" tabindex="-1" value=""></div>

                    <div class="clear">
                      <input type="submit" value="" name="subscribe" id="mc-embedded-subscribe" class="button email-subscribe-button">
                    </div>
                  </div>
                </form>
              </div>

            </li>
          </ul>

          <div class="footer-social-icons">
            <a href="https://www.facebook.com/pytorch" target="_blank" class="facebook"></a>
            <a href="https://twitter.com/pytorch" target="_blank" class="twitter"></a>
            <a href="https://www.youtube.com/pytorch" target="_blank" class="youtube"></a>
          </div>
        </div>
      </div>
    </div>
  </footer>


  

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->


  <div class="mobile-main-menu">
    <div class="container-fluid">
      <div class="container">
        <div class="mobile-main-menu-header-container">
          <a class="header-logo" href="https://pytorch.org/" aria-label="PyTorch"></a>
          <a class="main-menu-close-button" href="#" data-behavior="close-mobile-menu"></a>
        </div>
      </div>
    </div>

    <div class="mobile-main-menu-links-container">
      <div class="main-menu">
        <ul>
          <li>
            <a href="https://pytorch.org/get-started">Get Started</a>
          </li>

          <li>
            <a href="https://pytorch.org/features">Features</a>
          </li>

          <li>
            <a href="https://pytorch.org/ecosystem">Ecosystem</a>
          </li>

          <li>
            <a href="https://pytorch.org/mobile">Mobile</a>
          </li>

          <li>
            <a href="https://pytorch.org/hub">PyTorch Hub</a>
          </li>

          <li>
            <a href="https://pytorch.org/blog/">Blog</a>
          </li>

          <li>
            <a href="https://pytorch.org/tutorials">Tutorials</a>
          </li>

          <li class="active">
            <a href="https://pytorch.org/docs/stable/index.html">Docs</a>
          </li>

          <li>
            <a href="https://pytorch.org/resources">Resources</a>
          </li>

          <li>
            <a href="https://github.com/pytorch/pytorch">Github</a>
          </li>
        </ul>
      </div>
    </div>
  </div>


  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>