


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Open Images Integration &mdash; FiftyOne 1.3.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/voxel51-website.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="ActivityNet Integration" href="activitynet.html" />
    <link rel="prev" title="COCO Integration" href="coco.html" />
<meta property="og:image" content="https://voxel51.com/wp-content/uploads/2024/03/3.24_webpages_Home_AV.png" />

<link
  href="https://fonts.googleapis.com/css?family=Palanquin:400,600,700,800"
  rel="stylesheet"
/>
<link
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
  rel="stylesheet"
/>
<script src="https://tag.clearbitscripts.com/v1/pk_b9ed71c8234edd4f77326bcbfab5a4ca/tags.js"></script>


  
  <script src="../_static/js/modernizr.min.js"></script>

  
</head>


<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <nav id="nav__main" class="nav__main">
    <div class="nav__main__logo">
      <a href="https://voxel51.com/">
        <img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
        />
      </a>
    </div>

    <div class="nav__spacer desktop_only"></div>

    <div id="nav__main__mobilebutton--on">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-bars"></i>
      </a>
    </div>

    <div id="nav__main__mobilebutton--off">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-times"></i>
      </a>
    </div>

    <div id="nav__main__items">
      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Products</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/fiftyone/">Open Source</a></li>
            <li><a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a></li>
            <li><a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a></li>
            <li><a href="https://voxel51.com/success-stories/">Success Stories</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Learn</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://community.voxel51.com">Community Discord</a></li>
            <li><a href="https://voxel51.com/blog/">Blog</a></li>
            <li><a href="https://voxel51.com/computer-vision-events/">Events</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item">
        <a href="https://docs.voxel51.com/">Docs</a>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Company</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/ourstory/">About Us</a></li>
            <li><a href="https://voxel51.com/jobs/">Careers</a></li>
            <li><a href="https://voxel51.com/talk-to-sales/">Talk to Sales</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item mobile_only">
        <a href="https://github.com/voxel51/fiftyone">GitHub</a>
      </div>

      <div class="nav__item desktop_only" id="octocat">
        <!-- https://buttons.github.io -->
        <a
          class="github-button"
          href="https://github.com/voxel51/fiftyone"
          data-color-scheme="no-preference: dark_high_contrast; light: dark_high_contrast; dark: dark_high_contrast;"
          data-size="large"
          data-show-count="true"
          aria-label="Star voxel51/fiftyone on GitHub"
          >Star</a
        >
      </div>

      <div class="nav__item full_nav_only">
        <a class="button-primary" href="https://voxel51.com/schedule-teams-workshop/" target="_blank"
          >Schedule a workshop</a
        >
      </div>
    </div>
  </nav>
</div>



<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

           <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../teams/index.html">FiftyOne Teams ðŸš€</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/index.html">Environments</a></li>
<li class="toctree-l1"><a class="reference internal" href="../tutorials/index.html">Tutorials</a></li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/index.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheets/index.html">Cheat Sheets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_zoo/index.html">Dataset Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brain.html">FiftyOne Brain</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Integrations</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="coco.html"> COCO</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#"> Open Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="activitynet.html"> ActivityNet</a></li>
<li class="toctree-l2"><a class="reference internal" href="cvat.html"> CVAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="labelstudio.html"> Label Studio</a></li>
<li class="toctree-l2"><a class="reference internal" href="v7.html"> V7</a></li>
<li class="toctree-l2"><a class="reference internal" href="labelbox.html"> Labelbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="qdrant.html"> Qdrant</a></li>
<li class="toctree-l2"><a class="reference internal" href="redis.html"> Redis</a></li>
<li class="toctree-l2"><a class="reference internal" href="pinecone.html"> Pinecone</a></li>
<li class="toctree-l2"><a class="reference internal" href="mongodb.html"> MongoDB</a></li>
<li class="toctree-l2"><a class="reference internal" href="elasticsearch.html"> Elasticsearch</a></li>
<li class="toctree-l2"><a class="reference internal" href="milvus.html"> Milvus</a></li>
<li class="toctree-l2"><a class="reference internal" href="lancedb.html"> LanceDB</a></li>
<li class="toctree-l2"><a class="reference internal" href="huggingface.html"> Hugging Face</a></li>
<li class="toctree-l2"><a class="reference internal" href="ultralytics.html"> Ultralytics</a></li>
<li class="toctree-l2"><a class="reference internal" href="albumentations.html"> Albumentations</a></li>
<li class="toctree-l2"><a class="reference internal" href="super_gradients.html"> SuperGradients</a></li>
<li class="toctree-l2"><a class="reference internal" href="openclip.html"> OpenCLIP</a></li>
<li class="toctree-l2"><a class="reference internal" href="pytorch_hub.html"> PyTorch Hub</a></li>
<li class="toctree-l2"><a class="reference internal" href="lightning_flash.html"> Lightning Flash</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../plugins/index.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/fiftyone.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deprecation.html">Deprecation Notices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>
 
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">FiftyOne Integrations</a> &gt;</li>
        
      <li>Open Images Integration</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Contents
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content style-external-links">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  <div class="section" id="open-images-integration">
<span id="open-images"></span><h1>Open Images Integration<a class="headerlink" href="#open-images-integration" title="Permalink to this headline">Â¶</a></h1>
<p>Weâ€™ve collaborated with the team behind the
<a class="reference external" href="https://storage.googleapis.com/openimages/web/download.html">Open Images Dataset</a>
to make it easy to download, visualize, and evaluate on the Open Images dataset
natively in FiftyOne!</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check out <a class="reference internal" href="../tutorials/open_images.html"><span class="doc">this tutorial</span></a> to see how you can
use FiftyOne to download and evaluate models on Open Images.</p>
</div>
<img alt="open-images-v6" class="align-center" src="../_images/open-images-v6.png" />
<div class="section" id="loading-open-images">
<span id="id1"></span><h2>Loading Open Images<a class="headerlink" href="#loading-open-images" title="Permalink to this headline">Â¶</a></h2>
<p>The FiftyOne Dataset Zoo provides support for loading the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-open-images-v6"><span class="std std-ref">Open Images V6</span></a> and
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-open-images-v7"><span class="std std-ref">Open Images V7</span></a> datasets.</p>
<p>Like all other zoo datasets, you can use
<a class="reference internal" href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_zoo_dataset()</span></code></a> to download
and load an Open Images V7 split into FiftyOne:</p>
<div class="highlight-python notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal">1</span>
<span class="normal">2</span>
<span class="normal">3</span>
<span class="normal">4</span>
<span class="normal">5</span>
<span class="normal">6</span>
<span class="normal">7</span></pre></div></td><td class="code"><div><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># Download and load the validation split of Open Images V7</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;open-images-v7&quot;</span><span class="p">,</span> <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div></td></tr></table></div>
</div>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOne supports loading annotations for classification, detection,
segmentation, and visual relationship tasks for the 600 boxable classes
(<a class="reference external" href="https://storage.googleapis.com/openimages/web/factsfigures.html">cf. dataset overview</a>).</p>
<p>By default, all label types are loaded, but you can customize this via the
optional <code class="docutils literal notranslate"><span class="pre">label_types</span></code> argument (see below for details).</p>
</div>
<p>In addition, FiftyOne provides parameters that can be used to efficiently
download specific subsets of the Open Images dataset, allowing you to quickly
explore different slices of the dataset without downloading the entire split.</p>
<p>When performing partial downloads, FiftyOne will use existing downloaded data
first if possible before resorting to downloading additional data from the web.</p>
<div class="highlight-python notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span>
<span class="normal">39</span>
<span class="normal">40</span>
<span class="normal">41</span>
<span class="normal">42</span>
<span class="normal">43</span>
<span class="normal">44</span>
<span class="normal">45</span>
<span class="normal">46</span>
<span class="normal">47</span>
<span class="normal">48</span>
<span class="normal">49</span>
<span class="normal">50</span>
<span class="normal">51</span>
<span class="normal">52</span>
<span class="normal">53</span>
<span class="normal">54</span>
<span class="normal">55</span>
<span class="normal">56</span>
<span class="normal">57</span>
<span class="normal">58</span>
<span class="normal">59</span>
<span class="normal">60</span></pre></div></td><td class="code"><div><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1">#</span>
<span class="c1"># Load 50 random samples from the validation split of Open Images V7.</span>
<span class="c1">#</span>
<span class="c1"># Only the required images will be downloaded (if necessary).</span>
<span class="c1"># By default, all label types are loaded</span>
<span class="c1">#</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">&quot;open-images-v7&quot;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1">#</span>
<span class="c1"># Load detections and classifications for 25 samples from the</span>
<span class="c1"># validation split of Open Images V6 that contain fedoras and pianos</span>
<span class="c1">#</span>
<span class="c1"># Images that contain all `label_types` and `classes` will be</span>
<span class="c1"># prioritized first, followed by images that contain at least one of</span>
<span class="c1"># the required `classes`. If there are not enough images matching</span>
<span class="c1"># `classes` in the split to meet `max_samples`, only the available</span>
<span class="c1"># images will be loaded.</span>
<span class="c1">#</span>
<span class="c1"># Images will only be downloaded if necessary</span>
<span class="c1">#</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">&quot;open-images-v6&quot;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span>
    <span class="n">label_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;detections&quot;</span><span class="p">,</span> <span class="s2">&quot;classifications&quot;</span><span class="p">],</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Fedora&quot;</span><span class="p">,</span> <span class="s2">&quot;Piano&quot;</span><span class="p">],</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">session</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>

<span class="c1">#</span>
<span class="c1"># Load classifications and point labels for all samples from the</span>
<span class="c1"># validation split of Open Images V7 with class &quot;Turtle&quot; or &quot;Tortoise&quot;.</span>
<span class="c1">#</span>
<span class="c1"># If there are not enough images matching classes` in the split to</span>
<span class="c1"># meet `max_samples`, only the available images will be loaded.</span>
<span class="c1">#</span>
<span class="c1"># Images will only be downloaded if necessary</span>
<span class="c1">#</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">&quot;open-images-v7&quot;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s2">&quot;validation&quot;</span><span class="p">,</span>
    <span class="n">label_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;points&quot;</span><span class="p">,</span> <span class="s2">&quot;classifications&quot;</span><span class="p">],</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Turtle&quot;</span><span class="p">,</span> <span class="s2">&quot;Tortoise&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">session</span><span class="o">.</span><span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span>
</pre></div></td></tr></table></div>
</div>
<p>The following parameters are available to configure a partial download of Open
Images V6 or Open Images V7 by passing them to
<a class="reference internal" href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_zoo_dataset()</span></code></a>:</p>
<ul class="simple">
<li><p><strong>split</strong> (<em>None</em>) and <strong>splits</strong> (<em>None</em>): a string or list of strings,
respectively, specifying the splits to load. Supported values are
<code class="docutils literal notranslate"><span class="pre">(&quot;train&quot;,</span> <span class="pre">&quot;test&quot;,</span> <span class="pre">&quot;validation&quot;)</span></code>. If neither is provided, all available
splits are loaded</p></li>
<li><p><strong>label_types</strong> (<em>None</em>): a label type or list of label types to load.
Supported values for Open Images V6 are
<code class="docutils literal notranslate"><span class="pre">(&quot;detections&quot;,</span> <span class="pre">&quot;classifications&quot;,</span> <span class="pre">&quot;relationships&quot;,</span> <span class="pre">&quot;segmentations&quot;)</span></code>.
Open Images V7 also supports <code class="code docutils literal notranslate"><span class="pre">&quot;points&quot;</span></code> labels. By default, all labels types are loaded</p></li>
<li><p><strong>classes</strong> (<em>None</em>): a string or list of strings specifying required
classes to load. If provided, only samples containing at least one instance
of a specified class will be loaded. You can use
<a class="reference internal" href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_classes" title="fiftyone.utils.openimages.get_classes"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_classes()</span></code></a> and
<a class="reference internal" href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_segmentation_classes" title="fiftyone.utils.openimages.get_segmentation_classes"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_segmentation_classes()</span></code></a>
to see the available classes and segmentation classes, respectively</p></li>
<li><p><strong>attrs</strong> (<em>None</em>): a string or list of strings specifying required
relationship attributes to load. This parameter is only applicable if
<code class="docutils literal notranslate"><span class="pre">label_types</span></code> contains <code class="docutils literal notranslate"><span class="pre">&quot;relationships&quot;</span></code>. If provided, only samples
containing at least one instance of a specified attribute will be loaded.
You can use
<a class="reference internal" href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.get_attributes" title="fiftyone.utils.openimages.get_attributes"><code class="xref py py-func docutils literal notranslate"><span class="pre">get_attributes()</span></code></a>
to see the available attributes</p></li>
<li><p><strong>image_ids</strong> (<em>None</em>): a list of specific image IDs to load. The IDs can
be specified either as <code class="docutils literal notranslate"><span class="pre">&lt;split&gt;/&lt;image-id&gt;</span></code> or <code class="docutils literal notranslate"><span class="pre">&lt;image-id&gt;</span></code> strings.
Alternatively, you can provide the path to a TXT (newline-separated), JSON,
or CSV file containing the list of image IDs to load in either of the first
two formats</p></li>
<li><p><strong>include_id</strong> (<em>True</em>): whether to include the Open Images ID of each
sample in the loaded labels</p></li>
<li><p><strong>only_matching</strong> (<em>False</em>): whether to only load labels that match the
<code class="docutils literal notranslate"><span class="pre">classes</span></code> or <code class="docutils literal notranslate"><span class="pre">attrs</span></code> requirements that you provide (True), or to load
all labels for samples that match the requirements (False)</p></li>
<li><p><strong>num_workers</strong> (<em>None</em>): the number of processes to use when downloading
individual images. By default, <code class="code docutils literal notranslate"><span class="pre">multiprocessing.cpu_count()</span></code> is used</p></li>
<li><p><strong>shuffle</strong> (<em>False</em>): whether to randomly shuffle the order in which
samples are chosen for partial downloads</p></li>
<li><p><strong>seed</strong> (<em>None</em>): a random seed to use when shuffling</p></li>
<li><p><strong>max_samples</strong> (<em>None</em>): a maximum number of samples to load per split. If
<code class="docutils literal notranslate"><span class="pre">label_types</span></code>, <code class="docutils literal notranslate"><span class="pre">classes</span></code>, and/or <code class="docutils literal notranslate"><span class="pre">attrs</span></code> are also specified, first
priority will be given to samples that contain all of the specified label
types, classes, and/or attributes, followed by samples that contain at
least one of the specified labels types or classes. The actual number of
samples loaded may be less than this maximum value if the dataset does not
contain sufficient samples matching your requirements</p></li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See
<a class="reference internal" href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.OpenImagesV6Dataset" title="fiftyone.zoo.datasets.base.OpenImagesV6Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenImagesV7Dataset</span></code></a>
,
<a class="reference internal" href="../api/fiftyone.zoo.datasets.base.html#fiftyone.zoo.datasets.base.OpenImagesV7Dataset" title="fiftyone.zoo.datasets.base.OpenImagesV7Dataset"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenImagesV7Dataset</span></code></a>
and <a class="reference internal" href="../api/fiftyone.utils.openimages.html#fiftyone.utils.openimages.OpenImagesDatasetImporter" title="fiftyone.utils.openimages.OpenImagesDatasetImporter"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenImagesDatasetImporter</span></code></a>
for complete descriptions of the optional keyword arguments that you can
pass to <a class="reference internal" href="../api/fiftyone.zoo.datasets.html#fiftyone.zoo.datasets.load_zoo_dataset" title="fiftyone.zoo.datasets.load_zoo_dataset"><code class="xref py py-func docutils literal notranslate"><span class="pre">load_zoo_dataset()</span></code></a>.</p>
</div>
</div>
<div class="section" id="open-images-style-evaluation">
<span id="open-images-evaluation"></span><h2>Open Images-style evaluation<a class="headerlink" href="#open-images-style-evaluation" title="Permalink to this headline">Â¶</a></h2>
<p>The <a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
method provides builtin support for running
<a class="reference external" href="https://storage.googleapis.com/openimages/web/evaluation.html">Open Images-style evaluation</a>.</p>
<p>In order to run Open Images-style evaluation, simply set the <code class="docutils literal notranslate"><span class="pre">method</span></code>
parameter to <code class="docutils literal notranslate"><span class="pre">&quot;open-images&quot;</span></code>.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOneâ€™s implementation of Open Images-style evaluation matches the
reference implementation available via the
<a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/object_detection">TF Object Detection API</a>.</p>
</div>
<div class="section" id="overview">
<h3>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">Â¶</a></h3>
<p>Open Images-style evaluation provides additional features not found in
<a class="reference internal" href="../user_guide/evaluation.html#evaluating-detections-coco"><span class="std std-ref">COCO-style evaluation</span></a> that you may find
useful when evaluating your custom datasets.</p>
<p>The two primary differences are:</p>
<ul class="simple">
<li><p><strong>Non-exhaustive image labeling:</strong> positive and negative sample-level
<a class="reference internal" href="../api/fiftyone.core.labels.html#fiftyone.core.labels.Classifications" title="fiftyone.core.labels.Classifications"><code class="xref py py-class docutils literal notranslate"><span class="pre">Classifications</span></code></a> fields can be provided to indicate which object classes
were considered when annotating the image. Predicted objects whose classes
are not included in the sample-level labels for a sample are ignored.
The names of these fields can be specified via the <code class="docutils literal notranslate"><span class="pre">pos_label_field</span></code> and
<code class="docutils literal notranslate"><span class="pre">neg_label_field</span></code> parameters</p></li>
<li><p><strong>Class hierarchies:</strong> If your dataset includes a
<a class="reference external" href="https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy_visualizer/circle.html">class hierarchy</a>,
you can configure this evaluation protocol to automatically expand ground
truth and/or predicted leaf classes so that all levels of the hierarchy can
be <a class="reference external" href="https://storage.googleapis.com/openimages/web/evaluation.html">correctly evaluated</a>.
You can provide a label hierarchy via the <code class="docutils literal notranslate"><span class="pre">hierarchy</span></code> parameter. By
default, if you provide a hierarchy, then image-level label fields and
ground truth detections will be expanded to incorporate parent classes
(child classes for negative image-level labels). You can disable this
feature by setting the <code class="docutils literal notranslate"><span class="pre">expand_gt_hierarchy</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code>.
Alternatively, you can expand predictions by setting the
<code class="docutils literal notranslate"><span class="pre">expand_pred_hierarchy</span></code> parameter to <code class="docutils literal notranslate"><span class="pre">True</span></code></p></li>
</ul>
<p>In addition, note that:</p>
<ul class="simple">
<li><p>Like <a class="reference external" href="http://host.robots.ox.ac.uk/pascal/VOC/voc2010/devkit_doc_08-May-2010.pdf">VOC-style evaluation</a>,
only one IoU (default = 0.5) is used to calculate mAP. You can customize
this value via the <code class="docutils literal notranslate"><span class="pre">iou</span></code> parameter</p></li>
<li><p>When dealing with crowd objects, Open Images-style evaluation dictates that
if a crowd is matched with multiple predictions, each counts as one true
positive when computing mAP</p></li>
</ul>
<p>When you specify an <code class="docutils literal notranslate"><span class="pre">eval_key</span></code> parameter, a number of helpful fields will be
populated on each sample and its predicted/ground truth objects:</p>
<ul>
<li><p>True positive (TP), false positive (FP), and false negative (FN) counts
for the each sample are saved in top-level fields of each sample:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_tp</span>
<span class="n">FP</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fp</span>
<span class="n">FN</span><span class="p">:</span> <span class="n">sample</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_fn</span>
</pre></div>
</div>
</li>
<li><p>The fields listed below are populated on each individual object instance;
these fields tabulate the TP/FP/FN status of the object, the ID of the
matching object (if any), and the matching IoU:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">TP</span><span class="o">/</span><span class="n">FP</span><span class="o">/</span><span class="n">FN</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span>
      <span class="n">ID</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_id</span>
     <span class="n">IoU</span><span class="p">:</span> <span class="nb">object</span><span class="o">.&lt;</span><span class="n">eval_key</span><span class="o">&gt;</span><span class="n">_iou</span>
</pre></div>
</div>
</li>
</ul>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>See <a class="reference internal" href="../api/fiftyone.utils.eval.openimages.html#fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig" title="fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">OpenImagesEvaluationConfig</span></code></a> for complete descriptions of the optional
keyword arguments that you can pass to
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>
when running Open Images-style evaluation.</p>
</div>
</div>
<div class="section" id="example-evaluation">
<h3>Example evaluation<a class="headerlink" href="#example-evaluation" title="Permalink to this headline">Â¶</a></h3>
<p>The example below demonstrates Open Images-style detection evaluation on the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-quickstart"><span class="std std-ref">quickstart dataset</span></a> from the Dataset Zoo:</p>
<div class="highlight-python notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span>
<span class="normal">36</span>
<span class="normal">37</span>
<span class="normal">38</span></pre></div></td><td class="code"><div><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="c1"># Evaluate the objects in the `predictions` field with respect to the</span>
<span class="c1"># objects in the `ground_truth` field</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;open-images&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;eval&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Get the 10 most common classes in the dataset</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">&quot;ground_truth.detections.label&quot;</span><span class="p">)</span>
<span class="n">classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">counts</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)[:</span><span class="mi">10</span><span class="p">]</span>

<span class="c1"># Print a classification report for the top-10 classes</span>
<span class="n">results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>

<span class="c1"># Print some statistics about the total TP/FP/FN counts</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;TP: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;eval_tp&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FP: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;eval_fp&quot;</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;FN: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="s2">&quot;eval_fn&quot;</span><span class="p">))</span>

<span class="c1"># Create a view that has samples with the most false positives first, and</span>
<span class="c1"># only includes false positive boxes in the `predictions` field</span>
<span class="n">view</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">dataset</span>
    <span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">&quot;eval_fp&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span><span class="s2">&quot;predictions&quot;</span><span class="p">,</span> <span class="n">F</span><span class="p">(</span><span class="s2">&quot;eval&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;fp&quot;</span><span class="p">)</span>
<span class="p">)</span>

<span class="c1"># Visualize results in the App</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">view</span><span class="p">)</span>
</pre></div></td></tr></table></div>
</div>
<div class="highlight-text notranslate"><div class="highlight"><pre><span></span>               precision    recall  f1-score   support

       person       0.25      0.86      0.39       378
         kite       0.27      0.75      0.40        75
          car       0.18      0.80      0.29        61
         bird       0.20      0.51      0.28        51
       carrot       0.09      0.74      0.16        47
         boat       0.09      0.46      0.16        37
    surfboard       0.17      0.73      0.28        30
     airplane       0.36      0.83      0.50        24
traffic light       0.32      0.79      0.45        24
      giraffe       0.36      0.91      0.52        23

    micro avg       0.21      0.79      0.34       750
    macro avg       0.23      0.74      0.34       750
 weighted avg       0.23      0.79      0.36       750
</pre></div>
</div>
<img alt="quickstart-evaluate-detections-oi" class="align-center" src="../_images/quickstart_evaluate_detections_oi.png" />
</div>
<div class="section" id="map-and-pr-curves">
<h3>mAP and PR curves<a class="headerlink" href="#map-and-pr-curves" title="Permalink to this headline">Â¶</a></h3>
<p>You can easily compute mean average precision (mAP) and precision-recall (PR)
curves using the results object returned by
<a class="reference internal" href="../api/fiftyone.core.collections.html#fiftyone.core.collections.SampleCollection.evaluate_detections" title="fiftyone.core.collections.SampleCollection.evaluate_detections"><code class="xref py py-meth docutils literal notranslate"><span class="pre">evaluate_detections()</span></code></a>:</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>FiftyOneâ€™s implementation of Open Images-style evaluation matches the
reference implementation available via the
<a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/object_detection">TF Object Detection API</a>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span></pre></div></td><td class="code"><div><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>

<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;open-images&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mAP</span><span class="p">())</span>
<span class="c1"># 0.599</span>

<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_pr_curves</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;person&quot;</span><span class="p">,</span> <span class="s2">&quot;dog&quot;</span><span class="p">,</span> <span class="s2">&quot;car&quot;</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></td></tr></table></div>
</div>
<img alt="oi-pr-curve" class="align-center" src="../_images/oi_pr_curve.png" />
</div>
<div class="section" id="confusion-matrices">
<h3>Confusion matrices<a class="headerlink" href="#confusion-matrices" title="Permalink to this headline">Â¶</a></h3>
<p>You can also easily generate <a class="reference internal" href="../user_guide/evaluation.html#confusion-matrices"><span class="std std-ref">confusion matrices</span></a> for
the results of Open Images-style evaluations.</p>
<p>In order for the confusion matrix to capture anything other than false
positive/negative counts, you will likely want to set the
<a class="reference internal" href="../api/fiftyone.utils.eval.openimages.html#fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig" title="fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">classwise</span></code></a>
parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code> during evaluation so that predicted objects can be
matched with ground truth objects of different classes.</p>
<div class="highlight-python notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span></pre></div></td><td class="code"><div><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>

<span class="c1"># Perform evaluation, allowing objects to be matched between classes</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;open-images&quot;</span><span class="p">,</span>
    <span class="n">classwise</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Generate a confusion matrix for the specified classes</span>
<span class="n">plot</span> <span class="o">=</span> <span class="n">results</span><span class="o">.</span><span class="n">plot_confusion_matrix</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;car&quot;</span><span class="p">,</span> <span class="s2">&quot;truck&quot;</span><span class="p">,</span> <span class="s2">&quot;motorcycle&quot;</span><span class="p">])</span>
<span class="n">plot</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div></td></tr></table></div>
</div>
<img alt="oi-confusion-matrix" class="align-center" src="../_images/oi_confusion_matrix.png" />
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Did you know? <a class="reference internal" href="../user_guide/evaluation.html#confusion-matrices"><span class="std std-ref">Confusion matrices</span></a> can be
attached to your <a class="reference internal" href="../api/fiftyone.core.session.html#fiftyone.core.session.Session" title="fiftyone.core.session.Session"><code class="xref py py-class docutils literal notranslate"><span class="pre">Session</span></code></a> object and dynamically explored using FiftyOneâ€™s
<a class="reference internal" href="../user_guide/plots.html#interactive-plots"><span class="std std-ref">interactive plotting features</span></a>!</p>
</div>
</div>
</div>
<div class="section" id="open-images-challenge">
<span id="id4"></span><h2>Open Images Challenge<a class="headerlink" href="#open-images-challenge" title="Permalink to this headline">Â¶</a></h2>
<p>Since FiftyOneâ€™s implementation of Open Images-style evaluation matches the
reference implementation from the
<a class="reference external" href="https://github.com/tensorflow/models/tree/master/research/object_detection">TF Object Detection API</a>
used in the
<a class="reference external" href="https://storage.googleapis.com/openimages/web/evaluation.html">Open Images detection challenges</a>.
you can use it to compute the official mAP for your model while also enjoying
the benefits of working in the FiftyOne ecosystem, including
<a class="reference internal" href="../user_guide/using_views.html#using-views"><span class="std std-ref">using views</span></a> to manipulate your dataset and visually
exploring your modelâ€™s predictions in the <a class="reference internal" href="../user_guide/app.html#fiftyone-app"><span class="std std-ref">FiftyOne App</span></a>!</p>
<p>In order to compute the official Open Images mAP for a model, your dataset
<strong>must</strong> include the appropriate positive and negative sample-level labels, and
you must provide the class hierarchy. Fortunately, when you load the Open
Images dataset
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-open-images-v6"><span class="std std-ref">from the FiftyOne Dataset Zoo</span></a>, all of the
necessary information is automatically loaded for you!</p>
<p>The example snippet below loads the
<a class="reference internal" href="../dataset_zoo/datasets.html#dataset-zoo-open-images-v6"><span class="std std-ref">Open Images V6</span></a> dataset and runs the
official Open Images evaluation protocol on some mock model predictions:</p>
<div class="highlight-python notranslate"><div class="highlight"><table class="highlighttable"><tr><td class="linenos"><div class="linenodiv"><pre><span class="normal"> 1</span>
<span class="normal"> 2</span>
<span class="normal"> 3</span>
<span class="normal"> 4</span>
<span class="normal"> 5</span>
<span class="normal"> 6</span>
<span class="normal"> 7</span>
<span class="normal"> 8</span>
<span class="normal"> 9</span>
<span class="normal">10</span>
<span class="normal">11</span>
<span class="normal">12</span>
<span class="normal">13</span>
<span class="normal">14</span>
<span class="normal">15</span>
<span class="normal">16</span>
<span class="normal">17</span>
<span class="normal">18</span>
<span class="normal">19</span>
<span class="normal">20</span>
<span class="normal">21</span>
<span class="normal">22</span>
<span class="normal">23</span>
<span class="normal">24</span>
<span class="normal">25</span>
<span class="normal">26</span>
<span class="normal">27</span>
<span class="normal">28</span>
<span class="normal">29</span>
<span class="normal">30</span>
<span class="normal">31</span>
<span class="normal">32</span>
<span class="normal">33</span>
<span class="normal">34</span>
<span class="normal">35</span></pre></div></td><td class="code"><div><pre><span></span><span class="kn">import</span> <span class="nn">random</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># Load some samples from the Open Images V6 dataset from the zoo</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">&quot;open-images-v6&quot;</span><span class="p">,</span>
    <span class="s2">&quot;validation&quot;</span><span class="p">,</span>
    <span class="n">max_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
    <span class="n">label_types</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;detections&quot;</span><span class="p">,</span> <span class="s2">&quot;classifications&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="c1"># Generate some fake predictions</span>
<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="p">:</span>
    <span class="n">predictions</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;detections&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">detection</span> <span class="ow">in</span> <span class="n">predictions</span><span class="o">.</span><span class="n">detections</span><span class="p">:</span>
        <span class="n">detection</span><span class="o">.</span><span class="n">confidence</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span>

    <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;predictions&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">predictions</span>
    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="c1"># Evaluate your predictions via the official Open Images protocol</span>
<span class="n">results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">&quot;predictions&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;detections&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;open-images&quot;</span><span class="p">,</span>
    <span class="n">pos_label_field</span><span class="o">=</span><span class="s2">&quot;positive_labels&quot;</span><span class="p">,</span>
    <span class="n">neg_label_field</span><span class="o">=</span><span class="s2">&quot;negative_labels&quot;</span><span class="p">,</span>
    <span class="n">hierarchy</span><span class="o">=</span><span class="n">dataset</span><span class="o">.</span><span class="n">info</span><span class="p">[</span><span class="s2">&quot;hierarchy&quot;</span><span class="p">],</span>

<span class="p">)</span>

<span class="c1"># The official mAP for the results</span>
<span class="nb">print</span><span class="p">(</span><span class="n">results</span><span class="o">.</span><span class="n">mAP</span><span class="p">())</span>
</pre></div></td></tr></table></div>
</div>
<p>Most models trained on Open Images return the predictions for every class in
the hierarchy. However, if your model does not, then you can set the
<a class="reference internal" href="../api/fiftyone.utils.eval.openimages.html#fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig" title="fiftyone.utils.eval.openimages.OpenImagesEvaluationConfig"><code class="xref py py-class docutils literal notranslate"><span class="pre">expand_pred_hierarchy</span></code></a>
parameter to <code class="docutils literal notranslate"><span class="pre">False</span></code> to automatically generate predictions for parent classes
in the hierarchy for evaluation purposes.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>Check out <a class="reference internal" href="../recipes/adding_detections.html"><span class="doc">this recipe</span></a> to learn how to
add your modelâ€™s predictions to a FiftyOne Dataset.</p>
</div>
</div>
<div class="section" id="map-protocol">
<span id="open-images-map"></span><h2>mAP protocol<a class="headerlink" href="#map-protocol" title="Permalink to this headline">Â¶</a></h2>
<p>The Open Images mAP protocol is similar to <a class="reference internal" href="coco.html#coco-map"><span class="std std-ref">COCO-style mAP</span></a>,
with the primary differences being support for image-level labels, class
hierarchies, and differences in the way that objects are matched to crowds.</p>
<p>The steps to compute Open Images-style mAP are detailed below.</p>
<p><strong>Preprocessing</strong></p>
<ul class="simple">
<li><p>Filter ground truth and predicted objects by class
(unless <code class="docutils literal notranslate"><span class="pre">classwise=False</span></code>)</p></li>
<li><p>Expand the ground truth predictions by duplicating every object and
positive image-level label and modifying the class to include all parent
classes in the class hierarchy. Negative image-level labels are expanded to
include all child classes in the hierarchy for every label in the image</p></li>
<li><p>Sort predicted objects by confidence so that high confidence objects are
matched first</p></li>
<li><p>Sort ground truth objects so that objects with <code class="docutils literal notranslate"><span class="pre">IsGroupOf=True</span></code> (the name
of this attribute can be customized via the <code class="docutils literal notranslate"><span class="pre">iscrowd</span></code> parameter) are
matched last</p></li>
<li><p>Compute IoU between every ground truth and predicted object within the same
class (and between classes if <code class="docutils literal notranslate"><span class="pre">classwise=False</span></code>) in each image</p></li>
<li><p>Compute IoU between predictions and crowd objects as the intersection of
both boxes divided by the area of the prediction only. A prediction fully
inside the crowd box has an IoU of 1</p></li>
</ul>
<p><strong>Matching</strong></p>
<p>Once IoUs have been computed, predictions and ground truth objects are matched
to compute true positives, false positives, and false negatives:</p>
<ul class="simple">
<li><p>For each class, start with the highest confidence prediction, match it to
the ground truth object that it overlaps with the highest IoU. A prediction
only matches if the IoU is above the specified <code class="docutils literal notranslate"><span class="pre">iou</span></code> threshold
(default = 0.5)</p></li>
<li><p>If a prediction matched to a non-crowd gt object, it will not match to a
crowd even if the IoU is higher</p></li>
<li><p>Multiple predictions can match to the same crowd ground truth object, but
only one counts as a true positive, the others are ignored (unlike COCO).
If the crowd is not matched by any prediction, it is a false negative</p></li>
<li><p>(Unlike COCO) If a prediction maximally overlaps with a non-crowd ground
truth object that has already been matched with a higher confidence
prediction, the prediction is marked as a false positive</p></li>
<li><p>If <code class="docutils literal notranslate"><span class="pre">classwise=False</span></code>, predictions can only match to crowds if they are of
the same class</p></li>
</ul>
<p><strong>Computing mAP</strong></p>
<ul class="simple">
<li><p>(Unlike COCO) Only one IoU threshold (default = 0.5) is used to compute mAP</p></li>
<li><p>The next 6 steps are computed separately for each class:</p></li>
<li><p>Construct an array of true positives and false positives, sorted by
confidence</p></li>
<li><p>Compute the cumulative sum of this TP FP array</p></li>
<li><p>Compute precision array by elementwise dividing the TP-FP-sum array by
the total number of predictions up to that point</p></li>
<li><p>Compute recall array by elementwise dividing the TP-FP-sum array with the
total number of ground truth objects for the class</p></li>
<li><p>Ensure that precision is a non-increasing array</p></li>
<li><p>Add values <code class="docutils literal notranslate"><span class="pre">0</span></code> and <code class="docutils literal notranslate"><span class="pre">1</span></code> to precision and recall arrays</p></li>
<li><p>(Unlike COCO) Precision values are not interpolated and all recall values
are used to compute AP. This means that every class will produce a
different number of precision and recall values depending on the number of
true and false positives existing for that class</p></li>
<li><p>For every class that contains at least one ground truth object, compute the
AP by averaging the precision values. Then compute mAP by averaging the AP
values for each class</p></li>
</ul>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="activitynet.html" class="btn btn-neutral float-right" title="ActivityNet Integration" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="coco.html" class="btn btn-neutral" title="COCO Integration" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  
</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Open Images Integration</a><ul>
<li><a class="reference internal" href="#loading-open-images">Loading Open Images</a></li>
<li><a class="reference internal" href="#open-images-style-evaluation">Open Images-style evaluation</a><ul>
<li><a class="reference internal" href="#overview">Overview</a></li>
<li><a class="reference internal" href="#example-evaluation">Example evaluation</a></li>
<li><a class="reference internal" href="#map-and-pr-curves">mAP and PR curves</a></li>
<li><a class="reference internal" href="#confusion-matrices">Confusion matrices</a></li>
</ul>
</li>
<li><a class="reference internal" href="#open-images-challenge">Open Images Challenge</a></li>
<li><a class="reference internal" href="#map-protocol">mAP protocol</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
         <script src="../_static/js/voxel51-website.js"></script>
         <script src="../_static/js/custom.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


<footer class="footer-wrapper" id="docs-tutorials-resources">
  <div class="footer pytorch-container">
    <div class="footer__logo">
      <a href="https://voxel51.com/"
        ><img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
      /></a>
    </div>

    <div class="footer__address">
      330 E Liberty St<br />
      Ann Arbor, MI 48104<br />
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>

    <!--
    <div class="footer__contact">
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>
    -->

    <div class="footer__links">
      <div class="footer__links--col2">
        <p class="nav__item--brand">Products</p>
        <a href="https://voxel51.com/fiftyone/">FiftyOne</a>
        <a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a>
        <a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a>
        <a href="https://voxel51.com/success-stories/">Success Stories</a>
      </div>
      <div class="footer__links--col3">
        <p class="nav__item--brand">Resources</p>
        <a href="https://voxel51.com/blog/">Blog</a>
        <a href="https://docs.voxel51.com/">Docs</a>
        <a href="https://github.com/voxel51/">GitHub</a>
        <a href="https://community.voxel51.com">Discord</a>
        <a href="https://voxel51.com/ourstory/">About Us</a>
        <a href="https://voxel51.com/computer-vision-events/">Events</a>
        <a href="https://voxel51.com/jobs/">Careers</a>
        <a href="https://voxel51.com/press/">Press</a>
      </div>
    </div>

    <div class="footer__icons">
      <ul class="list-inline">
        <li>
          <a href="https://www.linkedin.com/company/voxel51/">
            <i class="fa-brands fa-linkedin"></i>
          </a>
        </li>
        <li>
          <a href="https://github.com/voxel51/">
            <i class="fa-brands fa-github"></i>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/voxel51">
            <i class="fa-brands fa-twitter"></i>
          </a>
        </li>
        <li>
          <a href="https://www.facebook.com/voxel51/">
            <i class="fa-brands fa-facebook"></i>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer__copyright">
      <ul class="list-inline">
        <li>&copy; 2024 Voxel51 Inc.</li>
        <li>
          <a href="https://voxel51.com/privacy/">Privacy Policy</a>
        </li>
        <li>
          <a href="https://voxel51.com/terms/">Terms of Service</a>
        </li>
      </ul>
    </div>
  </div>
</footer>

<!-- https://buttons.github.io -->
<script async defer src="https://buttons.github.io/buttons.js" ></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XD15NFRY3M" ></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "G-XD15NFRY3M");
</script>

<script>
  function open_modal(modal_id, modal_closer_id) {
    // Get the modal
    let the_modal = document.getElementById(modal_id);

    the_modal.style.display = "flex";
    document.body.style.overflow = "hidden";

    let the_modal_closer = document.getElementById(modal_closer_id);

    // When the user clicks on <span> (x), close the modal
    the_modal_closer &&
      (the_modal_closer.onclick = function () {
        the_modal.style.display = "none";
      });

    // When the user clicks anywhere outside of the modal, close it
    window.onclick = function (event) {
      if (event.target == the_modal) {
        the_modal.style.display = "none";
        document.body.style.overflow = "unset";
        window.onclick = undefined;
      }
    };
  }
</script>

<!-- bigpicture.io -->
<script>
  !(function (e, t, i) {
    var r = (e.bigPicture = e.bigPicture || []);
    if (!r.initialized)
      if (r.invoked)
        e.console &&
          console.error &&
          console.error("BigPicture.io snippet included twice.");
      else {
        (r.invoked = !0),
          (r.SNIPPET_VERSION = 1.5),
          (r.handler = function (e) {
            if (void 0 !== r.callback)
              try {
                return r.callback(e);
              } catch (e) {}
          }),
          (r.eventList = ["mousedown", "mouseup", "click", "submit"]),
          (r.methods = [
            "track",
            "identify",
            "page",
            "group",
            "alias",
            "integration",
            "ready",
            "intelReady",
            "consentReady",
            "on",
            "off",
          ]),
          (r.factory = function (e) {
            return function () {
              var t = Array.prototype.slice.call(arguments);
              return t.unshift(e), r.push(t), r;
            };
          });
        for (var n = 0; n < r.methods.length; n++) {
          var o = r.methods[n];
          r[o] = r.factory(o);
        }
        r.getCookie = function (e) {
          var i = ("; " + t.cookie).split("; " + e + "=");
          return 2 == i.length && i.pop().split(";").shift();
        };
        var c = (r.isEditor = (function () {
          try {
            return (
              e.self !== e.top &&
              (new RegExp("app" + i, "ig").test(t.referrer) ||
                "edit" == r.getCookie("_bpr_edit"))
            );
          } catch (e) {
            return !1;
          }
        })());
        r.init = function (n, o) {
          if (((r.projectId = n), (r._config = o), !c))
            for (var a = 0; a < r.eventList.length; a++)
              e.addEventListener(r.eventList[a], r.handler, !0);
          var s = t.createElement("script");
          s.async = !0;
          var d = c ? "/editor/editor" : "/public-" + n;
          (s.src = "//cdn" + i + d + ".js"),
            t.getElementsByTagName("head")[0].appendChild(s);
        };
      }
  })(window, document, ".bigpicture.io");
  bigPicture.init("1646");
</script>


  

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>