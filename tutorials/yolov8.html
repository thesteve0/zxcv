


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne &mdash; FiftyOne 1.3.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/voxel51-website.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Build a 3D self-driving dataset from scratch with OpenAI’s Point-E and FiftyOne" href="pointe.html" />
    <link rel="prev" title="Nearest Neighbor Embeddings Classification with Qdrant" href="qdrant.html" />
<meta property="og:image" content="https://voxel51.com/wp-content/uploads/2024/03/3.24_webpages_Home_AV.png" />

<link
  href="https://fonts.googleapis.com/css?family=Palanquin:400,600,700,800"
  rel="stylesheet"
/>
<link
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
  rel="stylesheet"
/>
<script src="https://tag.clearbitscripts.com/v1/pk_b9ed71c8234edd4f77326bcbfab5a4ca/tags.js"></script>


  
  <script src="../_static/js/modernizr.min.js"></script>

  
</head>


<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <nav id="nav__main" class="nav__main">
    <div class="nav__main__logo">
      <a href="https://voxel51.com/">
        <img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
        />
      </a>
    </div>

    <div class="nav__spacer desktop_only"></div>

    <div id="nav__main__mobilebutton--on">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-bars"></i>
      </a>
    </div>

    <div id="nav__main__mobilebutton--off">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-times"></i>
      </a>
    </div>

    <div id="nav__main__items">
      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Products</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/fiftyone/">Open Source</a></li>
            <li><a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a></li>
            <li><a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a></li>
            <li><a href="https://voxel51.com/success-stories/">Success Stories</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Learn</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://community.voxel51.com">Community Discord</a></li>
            <li><a href="https://voxel51.com/blog/">Blog</a></li>
            <li><a href="https://voxel51.com/computer-vision-events/">Events</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item">
        <a href="https://docs.voxel51.com/">Docs</a>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Company</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/ourstory/">About Us</a></li>
            <li><a href="https://voxel51.com/jobs/">Careers</a></li>
            <li><a href="https://voxel51.com/talk-to-sales/">Talk to Sales</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item mobile_only">
        <a href="https://github.com/voxel51/fiftyone">GitHub</a>
      </div>

      <div class="nav__item desktop_only" id="octocat">
        <!-- https://buttons.github.io -->
        <a
          class="github-button"
          href="https://github.com/voxel51/fiftyone"
          data-color-scheme="no-preference: dark_high_contrast; light: dark_high_contrast; dark: dark_high_contrast;"
          data-size="large"
          data-show-count="true"
          aria-label="Star voxel51/fiftyone on GitHub"
          >Star</a
        >
      </div>

      <div class="nav__item full_nav_only">
        <a class="button-primary" href="https://voxel51.com/schedule-teams-workshop/" target="_blank"
          >Schedule a workshop</a
        >
      </div>
    </div>
  </nav>
</div>



<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

           <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../teams/index.html">FiftyOne Teams 🚀</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/index.html">Environments</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pandas_comparison.html">pandas and FiftyOne</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluate_detections.html">Evaluating object detections</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluate_classifications.html">Evaluating a classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_embeddings.html">Using image embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="cvat_annotation.html">Annotating with CVAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="labelbox_annotation.html">Annotating with Labelbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="open_images.html">Working with Open Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="detectron2.html">Training with Detectron2</a></li>
<li class="toctree-l2"><a class="reference internal" href="uniqueness.html">Exploring image uniqueness</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_mistakes.html">Finding class mistakes</a></li>
<li class="toctree-l2"><a class="reference internal" href="detection_mistakes.html">Finding detection mistakes</a></li>
<li class="toctree-l2"><a class="reference internal" href="qdrant.html">Embeddings with Qdrant</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Fine-tuning YOLOv8 models</a></li>
<li class="toctree-l2"><a class="reference internal" href="pointe.html">3D point clouds with Point-E</a></li>
<li class="toctree-l2"><a class="reference internal" href="monocular_depth_estimation.html">Monocular depth estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="dimension_reduction.html">Dimensionality reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="zero_shot_classification.html">Zero-shot classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_augmentation.html">Data augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering.html">Clustering images</a></li>
<li class="toctree-l2"><a class="reference internal" href="small_object_detection.html">Detecting small objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="anomaly_detection.html">Anomaly detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/index.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheets/index.html">Cheat Sheets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_zoo/index.html">Dataset Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brain.html">FiftyOne Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../integrations/index.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins/index.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/fiftyone.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deprecation.html">Deprecation Notices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>
 
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">FiftyOne Tutorials</a> &gt;</li>
        
      <li>Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Contents
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content style-external-links">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<table class="fo-notebook-links" align="left">
    <td>
        <a target="_blank" href="https://colab.research.google.com/github/voxel51/fiftyone/blob/v1.3.0/docs/source/tutorials/yolov8.ipynb">
            <img src="../_static/images/icons/colab-logo-256px.png"> &nbsp; Run in Google Colab
        </a>
    </td>
    <td>
        <a target="_blank" href="https://github.com/voxel51/fiftyone/blob/v1.3.0/docs/source/tutorials/yolov8.ipynb">
            <img src="../_static/images/icons/github-logo-256px.png"> &nbsp; View source on GitHub
        </a>
    </td>
    <td>
        <a target="_blank" href="https://raw.githubusercontent.com/voxel51/fiftyone/v1.3.0/docs/source/tutorials/yolov8.ipynb" download>
            <img src="../_static/images/icons/cloud-icon-256px.png"> &nbsp; Download notebook
        </a>
    </td>
</table><div class="section" id="Fine-tune-YOLOv8-models-for-custom-use-cases-with-the-help-of-FiftyOne">
<h1>Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne<a class="headerlink" href="#Fine-tune-YOLOv8-models-for-custom-use-cases-with-the-help-of-FiftyOne" title="Permalink to this headline">¶</a></h1>
<p>Since its <a class="reference external" href="https://arxiv.org/abs/1506.02640">initial release back in 2015</a>, the You Only Look Once (YOLO) family of computer vision models has been one of the most popular in the field. In late 2022, <a class="reference external" href="https://github.com/ultralytics/ultralytics">Ultralytics</a> announced <a class="reference external" href="https://docs.ultralytics.com/#ultralytics-yolov8">YOLOv8</a>, which comes with a new
<a class="reference external" href="https://arxiv.org/abs/2206.08016#:~:text=Many%20networks%20have%20been%20proposed,before%20and%20demonstrates%20its%20effectiveness.">backbone</a>.</p>
<p>The basic YOLOv8 detection and segmentation models, however, are general purpose, which means for custom use cases they may not be suitable out of the box. With FiftyOne, we can visualize and evaluate YOLOv8 model predictions, and better understand where the model’s predictive power breaks down.</p>
<p>In this walkthrough, we will show you how to load YOLOv8 model predictions into FiftyOne, and use insights from model evaluation to fine-tune a YOLOv8 model for your custom use case.</p>
<p>Specifically, this walkthrough covers:</p>
<ul class="simple">
<li><p>Loading YOLOv8 model predictions into FiftyOne</p></li>
<li><p>Evaluating YOLOv8 model predictions</p></li>
<li><p>Curating a dataset for fine-tuning</p></li>
<li><p>Fine-tuning YOLOv8 models</p></li>
<li><p>Comparing the performance of out-of-the-box and fine-tuned YOLOv8 models.</p></li>
</ul>
<p><strong>So, what’s the takeaway?</strong></p>
<p>FiftyOne can help you to achieve better performance using YOLOv8 models on real-time inference tasks for custom use cases.</p>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">¶</a></h2>
<p>To get started, you need to install <a class="reference external" href="https://docs.voxel51.com/getting_started/install.html">FiftyOne</a> and <a class="reference external" href="https://github.com/ultralytics/ultralytics">Ultralytics</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>fiftyone<span class="w"> </span>ultralytics
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
</pre></div>
</div>
</div>
<p>We will import the YOLO object from Ultralytics and use this to instantiate pretrained detection and segmentation models in Python. Along with the YOLOv8 architecture, Ultralytics released a set of pretrained models, with different sizes, for classification, detection, and segmentation tasks.</p>
<p>For the purposes of illustration, we will use the smallest version, YOLOv8 Nano (YOLOv8n), but the same syntax will work for any of the pretrained models on the <a class="reference external" href="https://github.com/ultralytics/ultralytics">Ultralytics YOLOv8 GitHub repo</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>

<span class="n">detection_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">&quot;yolov8n.pt&quot;</span><span class="p">)</span>
<span class="n">seg_model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">&quot;yolov8n-seg.pt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>In Python, we can apply a YOLOv8 model to an individual image by passing the file path into the model call. For an image with file path <code class="docutils literal notranslate"><span class="pre">path/to/image.jpg</span></code>, running <code class="docutils literal notranslate"><span class="pre">detection_model(&quot;path/to/image.jpg&quot;)</span></code> will generate a list containing a single <code class="docutils literal notranslate"><span class="pre">ultralytics.yolo.engine.results.Results</span></code> object.</p>
<p>We can see this by applying the detection model to Ultralytics’ test image:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">results</span> <span class="o">=</span> <span class="n">detection_model</span><span class="p">(</span><span class="s2">&quot;https://ultralytics.com/images/bus.jpg&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>A similar result can be obtained if we apply the segmentation model to an image. These results contain bounding boxes, class confidence scores, and integers representing class labels. For a complete discussion of these results objects, see the Ultralytics YOLOv8 <a class="reference external" href="https://docs.ultralytics.com/reference/results/">Results API Reference</a>.</p>
<p>If we want to run tasks on all images in a directory, then we can do so from the command line with the YOLO Command Line Interface by specifying the task <code class="docutils literal notranslate"><span class="pre">[detect,</span> <span class="pre">segment,</span> <span class="pre">classify]</span></code> and mode <code class="docutils literal notranslate"><span class="pre">[train,</span> <span class="pre">val,</span> <span class="pre">predict,</span> <span class="pre">export]</span></code>, along with other arguments.</p>
<p>To run inference on a set of images, we must first put the data in the appropriate format. The best way to do so is to load your images into a FiftyOne <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>, and then export the dataset in <a class="reference external" href="https://docs.voxel51.com/user_guide/dataset_creation/datasets.html#yolov5dataset">YOLOv5Dataset</a> format, as YOLOv5 and YOLOv8 use the same data formats.</p>
<p>💡 FiftyOne’s Ultralytics Integration</p>
<p>If you just want to run inference on your FiftyOne dataset with an existing YOLOv8 model, you can do so by passing this <code class="docutils literal notranslate"><span class="pre">ultralytics.YOLO</span></code> model directly into your FiftyOne dataset’s <code class="docutils literal notranslate"><span class="pre">apply_model()</span></code> method:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="c1"># Load a dataset</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span><span class="s2">&quot;quickstart&quot;</span><span class="p">)</span>

<span class="c1"># Load a YOLOv8 model</span>
<span class="kn">from</span> <span class="nn">ultralytics</span> <span class="kn">import</span> <span class="n">YOLO</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">YOLO</span><span class="p">(</span><span class="s2">&quot;yolov8l.pt&quot;</span><span class="p">)</span>

<span class="c1"># Apply the model to the dataset</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;yolov8l&quot;</span><span class="p">)</span>

<span class="c1"># Launch the App to visualize the results</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>For more details, check out the <a class="reference external" href="https://docs.voxel51.com/integrations/ultralytics.html">FiftyOne Ultralytics Integration docs</a>!</p>
</div>
<div class="section" id="Load-YOLOv8-predictions-in-FiftyOne">
<h2>Load YOLOv8 predictions in FiftyOne<a class="headerlink" href="#Load-YOLOv8-predictions-in-FiftyOne" title="Permalink to this headline">¶</a></h2>
<p>In this walkthrough, we will look at YOLOv8’s predictions on a subset of the <a class="reference external" href="https://cocodataset.org/#home">MS COCO</a> dataset. This is the dataset on which these models were trained, which means that they are likely to show close to peak performance on this data. Additionally, working with COCO data makes it easy for us to map model outputs to class labels.</p>
<p>Load the images and ground truth object detections in COCO’s validation set from the <a class="reference external" href="https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html">FiftyOne Dataset Zoo</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s1">&#39;coco-2017&#39;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s1">&#39;validation&#39;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We then generate a mapping from YOLO class predictions to COCO class labels. <a class="reference external" href="https://cocodataset.org/#home">COCO has 91 classes</a>, and YOLOv8, just like YOLOv3 and YOLOv5, ignores all of the numeric classes and <a class="reference external" href="https://imageai.readthedocs.io/en/latest/detection/">focuses on the remaining 80</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coco_classes</span> <span class="o">=</span> <span class="p">[</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">default_classes</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">c</span><span class="o">.</span><span class="n">isnumeric</span><span class="p">()]</span>
</pre></div>
</div>
</div>
<div class="section" id="Generate-predictions">
<h3>Generate predictions<a class="headerlink" href="#Generate-predictions" title="Permalink to this headline">¶</a></h3>
<p>Export the dataset into a directory <code class="docutils literal notranslate"><span class="pre">coco_val</span></code> in YOLO format:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">export_yolo_data</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">export_dir</span><span class="p">,</span>
    <span class="n">classes</span><span class="p">,</span>
    <span class="n">label_field</span> <span class="o">=</span> <span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">split</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="p">):</span>

    <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span><span class="p">:</span>
        <span class="n">splits</span> <span class="o">=</span> <span class="n">split</span>
        <span class="k">for</span> <span class="n">split</span> <span class="ow">in</span> <span class="n">splits</span><span class="p">:</span>
            <span class="n">export_yolo_data</span><span class="p">(</span>
                <span class="n">samples</span><span class="p">,</span>
                <span class="n">export_dir</span><span class="p">,</span>
                <span class="n">classes</span><span class="p">,</span>
                <span class="n">label_field</span><span class="p">,</span>
                <span class="n">split</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">split</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">split_view</span> <span class="o">=</span> <span class="n">samples</span>
            <span class="n">split</span> <span class="o">=</span> <span class="s2">&quot;val&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">split_view</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">match_tags</span><span class="p">(</span><span class="n">split</span><span class="p">)</span>

        <span class="n">split_view</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="n">export_dir</span><span class="p">,</span>
            <span class="n">dataset_type</span><span class="o">=</span><span class="n">fo</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">YOLOv5Dataset</span><span class="p">,</span>
            <span class="n">label_field</span><span class="o">=</span><span class="n">label_field</span><span class="p">,</span>
            <span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">,</span>
            <span class="n">split</span><span class="o">=</span><span class="n">split</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">coco_val_dir</span> <span class="o">=</span> <span class="s2">&quot;coco_val&quot;</span>
<span class="n">export_yolo_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">coco_val_dir</span><span class="p">,</span> <span class="n">coco_classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Then run inference on these images:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>yolo<span class="w"> </span><span class="nv">task</span><span class="o">=</span>detect<span class="w"> </span><span class="nv">mode</span><span class="o">=</span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolov8n.pt<span class="w"> </span><span class="nv">source</span><span class="o">=</span>coco_val/images/val<span class="w"> </span><span class="nv">save_txt</span><span class="o">=</span>True<span class="w"> </span><span class="nv">save_conf</span><span class="o">=</span>True
</pre></div>
</div>
</div>
<p>Running this inference generates a directory <code class="docutils literal notranslate"><span class="pre">runs/detect/predict/labels</span></code>, which will contain a separate <code class="docutils literal notranslate"><span class="pre">.txt</span></code> file for each image in the dataset, and a line for each object detection.</p>
<p>Each line is in the form: an integer for the class label, a class confidence score, and four values representing the bounding box.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">label_file</span> <span class="o">=</span> <span class="s2">&quot;runs/detect/predict/labels/000000000139.txt&quot;</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">label_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">f</span><span class="o">.</span><span class="n">read</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
56 0.663281 0.619718 0.0640625 0.201878 0.265856
60 0.55625 0.619718 0.184375 0.225352 0.266771
74 0.710938 0.307512 0.01875 0.0469484 0.277868
60 0.860156 0.91784 0.279687 0.159624 0.278297
72 0.744531 0.539906 0.101562 0.295775 0.356417
75 0.888281 0.820423 0.0609375 0.241784 0.391675
58 0.385156 0.457746 0.0640625 0.084507 0.420693
56 0.609375 0.620892 0.090625 0.21831 0.50562
56 0.650781 0.619718 0.0859375 0.215962 0.508265
56 0.629687 0.619718 0.128125 0.220657 0.523211
0 0.686719 0.535211 0.0828125 0.333333 0.712339
56 0.505469 0.624413 0.0953125 0.230047 0.854189
62 0.125 0.502347 0.23125 0.225352 0.927385

</pre></div></div>
</div>
</div>
<div class="section" id="Load-detections">
<h3>Load detections<a class="headerlink" href="#Load-detections" title="Permalink to this headline">¶</a></h3>
<p>We can read a YOLOv8 detection prediction file with <span class="math notranslate nohighlight">\(N\)</span> detections into an <span class="math notranslate nohighlight">\((N, 6)\)</span> numpy array:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">read_yolo_detections_file</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
    <span class="n">detections</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">filepath</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([])</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">filepath</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">lines</span> <span class="o">=</span> <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">(</span><span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">]</span>

    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">lines</span><span class="p">:</span>
        <span class="n">detection</span> <span class="o">=</span> <span class="p">[</span><span class="nb">float</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">line</span><span class="p">]</span>
        <span class="n">detections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">detection</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">detections</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>From here, we need to convert these detections into FiftyOne’s <a class="reference external" href="https://docs.voxel51.com/user_guide/using_datasets.html#object-detection">Detections</a> format.</p>
<p>YOLOv8 represents bounding boxes in a centered format with coordinates <code class="docutils literal notranslate"><span class="pre">[center_x,</span> <span class="pre">center_y,</span> <span class="pre">width,</span> <span class="pre">height]</span></code>, whereas <a class="reference external" href="https://docs.voxel51.com/user_guide/using_datasets.html#object-detection">FiftyOne stores bounding boxes</a> in <code class="docutils literal notranslate"><span class="pre">[top-left-x,</span> <span class="pre">top-left-y,</span> <span class="pre">width,</span> <span class="pre">height]</span></code> format. We can make this conversion by “un-centering” the predicted bounding boxes:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_uncenter_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&#39;&#39;&#39;convert from center coords to corner coords&#39;&#39;&#39;</span>
    <span class="n">boxes</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">-=</span> <span class="n">boxes</span><span class="p">[:,</span> <span class="mi">2</span><span class="p">]</span><span class="o">/</span><span class="mf">2.</span>
    <span class="n">boxes</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">-=</span> <span class="n">boxes</span><span class="p">[:,</span> <span class="mi">3</span><span class="p">]</span><span class="o">/</span><span class="mf">2.</span>
</pre></div>
</div>
</div>
<p>Additionally, we can convert a list of class predictions (indices) to a list of class labels (strings) by passing in the class list:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">_get_class_labels</span><span class="p">(</span><span class="n">predicted_classes</span><span class="p">,</span> <span class="n">class_list</span><span class="p">):</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">(</span><span class="n">predicted_classes</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">class_list</span><span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="n">labels</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">labels</span>
</pre></div>
</div>
</div>
<p>Given the output of a <code class="docutils literal notranslate"><span class="pre">read_yolo_detections_file()</span></code> call, <code class="docutils literal notranslate"><span class="pre">yolo_detections</span></code>, we can generate the FiftyOne <code class="docutils literal notranslate"><span class="pre">Detections</span></code> object that captures this data:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convert_yolo_detections_to_fiftyone</span><span class="p">(</span>
    <span class="n">yolo_detections</span><span class="p">,</span>
    <span class="n">class_list</span>
    <span class="p">):</span>

    <span class="n">detections</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">yolo_detections</span><span class="o">.</span><span class="n">size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">fo</span><span class="o">.</span><span class="n">Detections</span><span class="p">(</span><span class="n">detections</span><span class="o">=</span><span class="n">detections</span><span class="p">)</span>

    <span class="n">boxes</span> <span class="o">=</span> <span class="n">yolo_detections</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">_uncenter_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">)</span>

    <span class="n">confs</span> <span class="o">=</span> <span class="n">yolo_detections</span><span class="p">[:,</span> <span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">_get_class_labels</span><span class="p">(</span><span class="n">yolo_detections</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">class_list</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">conf</span><span class="p">,</span> <span class="n">box</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">confs</span><span class="p">,</span> <span class="n">boxes</span><span class="p">):</span>
        <span class="n">detections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">fo</span><span class="o">.</span><span class="n">Detection</span><span class="p">(</span>
                <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
                <span class="n">bounding_box</span><span class="o">=</span><span class="n">box</span><span class="o">.</span><span class="n">tolist</span><span class="p">(),</span>
                <span class="n">confidence</span><span class="o">=</span><span class="n">conf</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">fo</span><span class="o">.</span><span class="n">Detections</span><span class="p">(</span><span class="n">detections</span><span class="o">=</span><span class="n">detections</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>The final ingredient is a function that takes in the file path of an image, and returns the file path of the corresponding YOLOv8 detection prediction text file.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_prediction_filepath</span><span class="p">(</span><span class="n">filepath</span><span class="p">,</span> <span class="n">run_number</span> <span class="o">=</span> <span class="mi">1</span><span class="p">):</span>
    <span class="n">run_num_string</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">run_number</span> <span class="o">!=</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">run_num_string</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">run_number</span><span class="p">)</span>
    <span class="n">filename</span> <span class="o">=</span> <span class="n">filepath</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;runs/detect/predict</span><span class="si">{</span><span class="n">run_num_string</span><span class="si">}</span><span class="s2">/labels/</span><span class="si">{</span><span class="n">filename</span><span class="si">}</span><span class="s2">.txt&quot;</span>
</pre></div>
</div>
</div>
<p>If you run multiple inference calls for the same task, the predictions results are stored in a directory with the next available integer appended to <code class="docutils literal notranslate"><span class="pre">predict</span></code> in the file path. You can account for this in the above function by passing in the <code class="docutils literal notranslate"><span class="pre">run_number</span></code> argument.</p>
<p>Putting the pieces together, we can write a function that adds these YOLOv8 detections to all of the samples in our dataset efficiently by batching the read and write operations to the underlying <a class="reference external" href="https://docs.voxel51.com/environments/index.html#connecting-to-a-localhost-database">MongoDB database</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">add_yolo_detections</span><span class="p">(</span>
    <span class="n">samples</span><span class="p">,</span>
    <span class="n">prediction_field</span><span class="p">,</span>
    <span class="n">prediction_filepath</span><span class="p">,</span>
    <span class="n">class_list</span>
    <span class="p">):</span>

    <span class="n">prediction_filepaths</span> <span class="o">=</span> <span class="n">samples</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="n">prediction_filepath</span><span class="p">)</span>
    <span class="n">yolo_detections</span> <span class="o">=</span> <span class="p">[</span><span class="n">read_yolo_detections_file</span><span class="p">(</span><span class="n">pf</span><span class="p">)</span> <span class="k">for</span> <span class="n">pf</span> <span class="ow">in</span> <span class="n">prediction_filepaths</span><span class="p">]</span>
    <span class="n">detections</span> <span class="o">=</span>  <span class="p">[</span><span class="n">convert_yolo_detections_to_fiftyone</span><span class="p">(</span><span class="n">yd</span><span class="p">,</span> <span class="n">class_list</span><span class="p">)</span> <span class="k">for</span> <span class="n">yd</span> <span class="ow">in</span> <span class="n">yolo_detections</span><span class="p">]</span>
    <span class="n">samples</span><span class="o">.</span><span class="n">set_values</span><span class="p">(</span><span class="n">prediction_field</span><span class="p">,</span> <span class="n">detections</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now we can rapidly add the detections in a few lines of code:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">filepaths</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="s2">&quot;filepath&quot;</span><span class="p">)</span>
<span class="n">prediction_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_prediction_filepath</span><span class="p">(</span><span class="n">fp</span><span class="p">)</span> <span class="k">for</span> <span class="n">fp</span> <span class="ow">in</span> <span class="n">filepaths</span><span class="p">]</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">set_values</span><span class="p">(</span>
    <span class="s2">&quot;yolov8n_det_filepath&quot;</span><span class="p">,</span>
    <span class="n">prediction_filepaths</span>
<span class="p">)</span>

<span class="n">add_yolo_detections</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="s2">&quot;yolov8n&quot;</span><span class="p">,</span>
    <span class="s2">&quot;yolov8n_det_filepath&quot;</span><span class="p">,</span>
    <span class="n">coco_classes</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now we can visualize these YOLOv8 model predictions on the samples in our dataset in the FiftyOne App:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="yolov8-base-predictions" src="../_images/yolov8_coco_val_predictions.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-segmentation-masks">
<h3>Load segmentation masks<a class="headerlink" href="#Load-segmentation-masks" title="Permalink to this headline">¶</a></h3>
<p>It is also worth noting that it is possible to convert YOLOv8 predictions directly from the output of a YOLO model call in Python, without first generating external prediction files and reading them in. Let’s see how this can be done for instance segmentations.</p>
<p>Like detections, YOLOv8 stores instance segmentations with centered bounding boxes. In addition, <a class="reference external" href="https://docs.ultralytics.com/reference/results/#masks-api-reference">YOLOv8 stores a mask</a> that covers the entire image, with only a rectangular region of that mask containing nonzero values. FiftyOne, on the other hand, <a class="reference external" href="https://docs.voxel51.com/user_guide/using_datasets.html#instance-segmentations">stores instance segmentations</a> at <code class="docutils literal notranslate"><span class="pre">Detection</span></code> labels with a mask that only covers the given
bounding box.</p>
<p>We can convert from YOLOv8 instance segmentations to FiftyOne instance segmentations with this <code class="docutils literal notranslate"><span class="pre">convert_yolo_segmentations_to_fiftyone()</span></code> function:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[32]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">convert_yolo_segmentations_to_fiftyone</span><span class="p">(</span>
    <span class="n">yolo_segmentations</span><span class="p">,</span>
    <span class="n">class_list</span>
    <span class="p">):</span>

    <span class="n">detections</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">boxes</span> <span class="o">=</span> <span class="n">yolo_segmentations</span><span class="o">.</span><span class="n">boxes</span><span class="o">.</span><span class="n">xywhn</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">boxes</span><span class="o">.</span><span class="n">shape</span> <span class="ow">or</span> <span class="n">yolo_segmentations</span><span class="o">.</span><span class="n">masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">fo</span><span class="o">.</span><span class="n">Detections</span><span class="p">(</span><span class="n">detections</span><span class="o">=</span><span class="n">detections</span><span class="p">)</span>

    <span class="n">_uncenter_boxes</span><span class="p">(</span><span class="n">boxes</span><span class="p">)</span>
    <span class="n">masks</span> <span class="o">=</span> <span class="n">yolo_segmentations</span><span class="o">.</span><span class="n">masks</span><span class="o">.</span><span class="n">masks</span>
    <span class="n">labels</span> <span class="o">=</span> <span class="n">_get_class_labels</span><span class="p">(</span><span class="n">yolo_segmentations</span><span class="o">.</span><span class="n">boxes</span><span class="o">.</span><span class="n">cls</span><span class="p">,</span> <span class="n">class_list</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">label</span><span class="p">,</span> <span class="n">box</span><span class="p">,</span> <span class="n">mask</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">masks</span><span class="p">):</span>
        <span class="c1">## convert to absolute indices to index mask</span>
        <span class="n">w</span><span class="p">,</span> <span class="n">h</span> <span class="o">=</span> <span class="n">mask</span><span class="o">.</span><span class="n">shape</span>
        <span class="n">tmp</span> <span class="o">=</span>  <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">box</span><span class="p">)</span>
        <span class="n">tmp</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">tmp</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">+=</span> <span class="n">tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">tmp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*=</span> <span class="n">h</span>
        <span class="n">tmp</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">*=</span> <span class="n">h</span>
        <span class="n">tmp</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="n">w</span>
        <span class="n">tmp</span><span class="p">[</span><span class="mi">3</span><span class="p">]</span> <span class="o">*=</span> <span class="n">w</span>
        <span class="n">tmp</span> <span class="o">=</span> <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="p">)</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">tmp</span><span class="p">]</span>
        <span class="n">y0</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="n">x1</span> <span class="o">=</span> <span class="n">tmp</span>
        <span class="n">sub_mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[</span><span class="n">x0</span><span class="p">:</span><span class="n">x1</span><span class="p">,</span> <span class="n">y0</span><span class="p">:</span><span class="n">y1</span><span class="p">]</span>

        <span class="n">detections</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
            <span class="n">fo</span><span class="o">.</span><span class="n">Detection</span><span class="p">(</span>
                <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">,</span>
                <span class="n">bounding_box</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">box</span><span class="p">),</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">sub_mask</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">bool</span><span class="p">)</span>
            <span class="p">)</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">fo</span><span class="o">.</span><span class="n">Detections</span><span class="p">(</span><span class="n">detections</span><span class="o">=</span><span class="n">detections</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Looping through all samples in the dataset, we can add the predictions from our <code class="docutils literal notranslate"><span class="pre">seg_model</span></code>, and then view these predicted masks in the FiftyOne App.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="yolov8-segmentation" src="../_images/yolov8_coco_val_segmentation.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Evaluate-YOLOv8-model-predictions">
<h2>Evaluate YOLOv8 model predictions<a class="headerlink" href="#Evaluate-YOLOv8-model-predictions" title="Permalink to this headline">¶</a></h2>
<p>Now that we have YOLOv8 predictions loaded onto the images in our dataset, we can evaluate the quality of these predictions using FiftyOne’s <a class="reference external" href="https://docs.voxel51.com/user_guide/evaluation.html">Evaluation API</a>.</p>
<p>To evaluate the object detections in the <code class="docutils literal notranslate"><span class="pre">yolov8_det</span></code> field relative to the <code class="docutils literal notranslate"><span class="pre">ground_truth</span></code> detections field, we can run:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">detection_results</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">&quot;yolov8n&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;eval&quot;</span><span class="p">,</span>
    <span class="n">compute_mAP</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="Compute-summary-statistics">
<h3>Compute summary statistics<a class="headerlink" href="#Compute-summary-statistics" title="Permalink to this headline">¶</a></h3>
<p>We can then get the <a class="reference external" href="https://jonathan-hui.medium.com/map-mean-average-precision-for-object-detection-45c121a31173">mean average precision</a> (mAP) of the model’s predictions:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mAP</span> <span class="o">=</span> <span class="n">detection_results</span><span class="o">.</span><span class="n">mAP</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;mAP = </span><span class="si">{</span><span class="n">mAP</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
mAP = 0.3121319189417518
</pre></div></div>
</div>
<p>We can also look at the model’s performance on the 20 most common object classes in the dataset, where it has seen the most examples so the statistics are most meaningful:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[45]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">counts</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">count_values</span><span class="p">(</span><span class="s2">&quot;ground_truth.detections.label&quot;</span><span class="p">)</span>

<span class="n">top20_classes</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span>
    <span class="n">counts</span><span class="p">,</span>
    <span class="n">key</span><span class="o">=</span><span class="n">counts</span><span class="o">.</span><span class="n">get</span><span class="p">,</span>
    <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span>
<span class="p">)[:</span><span class="mi">20</span><span class="p">]</span>

<span class="n">detection_results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">top20_classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
               precision    recall  f1-score   support

       person       0.85      0.68      0.76     11573
          car       0.71      0.52      0.60      1971
        chair       0.62      0.34      0.44      1806
         book       0.61      0.12      0.20      1182
       bottle       0.68      0.39      0.50      1051
          cup       0.61      0.44      0.51       907
 dining table       0.54      0.42      0.47       697
traffic light       0.66      0.36      0.46       638
         bowl       0.63      0.49      0.55       636
      handbag       0.48      0.12      0.19       540
         bird       0.79      0.39      0.52       451
         boat       0.58      0.29      0.39       430
        truck       0.57      0.35      0.44       415
        bench       0.58      0.27      0.37       413
     umbrella       0.65      0.52      0.58       423
          cow       0.81      0.61      0.70       397
       banana       0.68      0.34      0.45       397
       carrot       0.56      0.29      0.38       384
   motorcycle       0.77      0.58      0.66       379
     backpack       0.51      0.16      0.24       371

    micro avg       0.76      0.52      0.61     25061
    macro avg       0.64      0.38      0.47     25061
 weighted avg       0.74      0.52      0.60     25061

</pre></div></div>
</div>
<p>From the output of <code class="docutils literal notranslate"><span class="pre">print_report()</span></code>, we can see that this model performs decently well, but certainly has its limitations. While its precision is relatively good on average, it is lacking when it comes to recall. This is especially pronounced for certain classes like the <code class="docutils literal notranslate"><span class="pre">book</span></code> class.</p>
</div>
<div class="section" id="Inspect-individual-predictions">
<h3>Inspect individual predictions<a class="headerlink" href="#Inspect-individual-predictions" title="Permalink to this headline">¶</a></h3>
<p>Fortunately, we can dig deeper into these results with FiftyOne. Using the FiftyOne App, we can for instance filter by class for both ground truth and predicted detections so that only <code class="docutils literal notranslate"><span class="pre">book</span></code> detections appear in the samples.</p>
<p><img alt="yolov8-book-predictions" src="../_images/yolov8_coco_val_books_modal.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Scrolling through the samples in the sample grid, we can see that a lot of the time, COCO’s purported <em>ground truth</em> labels for the <code class="docutils literal notranslate"><span class="pre">book</span></code> class appear to be imperfect. Sometimes, individual books are bounded, other times rows or whole bookshelves are encompassed in a single box, and yet other times books are entirely unlabeled. Unless our desired computer vision application specifically requires good <code class="docutils literal notranslate"><span class="pre">book</span></code> detection, this should probably not be a point of concern when we are assessing the
quality of the model. After all, the quality of a model is limited by the quality of the data it is trained on - this is why data-centric approaches to computer vision are so important!</p>
<p>For other classes like the <code class="docutils literal notranslate"><span class="pre">bird</span></code> class, however, there appear to be challenges. One way to see this is to filter for <code class="docutils literal notranslate"><span class="pre">bird</span></code> ground truth detections and then convert to an <a class="reference external" href="https://docs.voxel51.com/api/fiftyone.core.patches.html#fiftyone.core.patches.EvaluationPatchesView">EvaluationPatchesView</a>. Some of these recall errors appear to be related to small features, where the resolution is poor.</p>
<p>In other cases though, quick inspection confirms that the object is clearly a bird. This means that there is likely room for improvement.</p>
<p><img alt="yolov8-base-bird_patches" src="../_images/yolov8_coco_val_bird_patch_view.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Curate-data-for-fine-tuning">
<h2>Curate data for fine-tuning<a class="headerlink" href="#Curate-data-for-fine-tuning" title="Permalink to this headline">¶</a></h2>
<p>For the remainder of this walkthrough, we will pretend that we are working for a bird conservancy group, putting computer vision models in the field to track and protect endangered species. Our goal is to fine-tune a YOLOv8 detection model to detect birds.</p>
<div class="section" id="Generate-test-set">
<h3>Generate test set<a class="headerlink" href="#Generate-test-set" title="Permalink to this headline">¶</a></h3>
<p>We will use the COCO validation dataset above as our test set. Since we are only concerned with detecting birds, we can filter out all non-<code class="docutils literal notranslate"><span class="pre">bird</span></code> ground truth detections using <code class="docutils literal notranslate"><span class="pre">filter_labels()</span></code>. We will also filter out the non- <code class="docutils literal notranslate"><span class="pre">bird</span></code> predictions, but will pass the <code class="docutils literal notranslate"><span class="pre">only_matches</span> <span class="pre">=</span> <span class="pre">False</span></code> argument into <code class="docutils literal notranslate"><span class="pre">filter_labels()</span></code> to make sure we keep images that have ground truth <code class="docutils literal notranslate"><span class="pre">bird</span></code> detections without YOLOv8n <code class="docutils literal notranslate"><span class="pre">bird</span></code> predictions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">test_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span>
    <span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="n">F</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;bird&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">filter_labels</span><span class="p">(</span>
    <span class="s2">&quot;yolov8n&quot;</span><span class="p">,</span>
    <span class="n">F</span><span class="p">(</span><span class="s2">&quot;label&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;bird&quot;</span><span class="p">,</span>
    <span class="n">only_matches</span><span class="o">=</span><span class="kc">False</span>
<span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="n">test_dataset</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;birds-test-dataset&quot;</span>
<span class="n">test_dataset</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>

<span class="c1">## set classes to just include birds</span>
<span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;bird&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>We then give the dataset a name, make it persistent, and save it to the underlying database. This test set has only 125 images, which we can visualize in the FiftyOne App.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="yolov8-birds-test-view" src="../_images/yolov8_bird_test_view.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>We can also run <code class="docutils literal notranslate"><span class="pre">evaluate_detections()</span></code> on this data to evaluate the YOLOv8n model’s performance on images with ground truth bird detections. We will store the results under the <code class="docutils literal notranslate"><span class="pre">base</span></code> evaluation key:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[49]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">base_bird_results</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">&quot;yolov8n&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;base&quot;</span><span class="p">,</span>
    <span class="n">compute_mAP</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Evaluating detections...
 100% |█████████████████| 125/125 [886.0ms elapsed, 0s remaining, 141.1 samples/s]
Performing IoU sweep...
 100% |█████████████████| 125/125 [619.1ms elapsed, 0s remaining, 201.9 samples/s]
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[54]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">mAP</span> <span class="o">=</span> <span class="n">base_bird_results</span><span class="o">.</span><span class="n">mAP</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Base mAP = </span><span class="si">{</span><span class="n">mAP</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Base mAP = 0.24897924786479841
</pre></div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">base_bird_results</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="n">classes</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

        bird       0.87      0.39      0.54       451

   micro avg       0.87      0.39      0.54       451
   macro avg       0.87      0.39      0.54       451
weighted avg       0.87      0.39      0.54       451

</pre></div></div>
</div>
<p>We note that while the recall is the same as in the initial evaluation report over the entire COCO validation split, the precision is higher. This means there are images that have YOLOv8n <code class="docutils literal notranslate"><span class="pre">bird</span></code> predictions but not ground truth <code class="docutils literal notranslate"><span class="pre">bird</span></code> detections.</p>
<p>The final step in preparing this test set is exporting the data into YOLOv8 format so we can run inference on just these samples with our fine-tuned model when we are done training. We will do so using the <code class="docutils literal notranslate"><span class="pre">export_yolo_data()</span></code> function we defined earlier.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">export_yolo_data</span><span class="p">(</span>
    <span class="n">test_dataset</span><span class="p">,</span>
    <span class="s2">&quot;birds_test&quot;</span><span class="p">,</span>
    <span class="n">classes</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Generate-training-set">
<h3>Generate training set<a class="headerlink" href="#Generate-training-set" title="Permalink to this headline">¶</a></h3>
<p>Now we choose the data on which we will fine-tune the base YOLOv8 model. Our goal is to generate a high-quality training dataset whose examples cover all expected scenarios in that subset.</p>
<p>In general, this is both an art and a science, and it can involve a variety of techniques, including</p>
<ul class="simple">
<li><p>pulling in data from other datasets</p></li>
<li><p>annotating more data that you’ve already collected with ground truth labels,</p></li>
<li><p>augmenting your data with tools like <a class="reference external" href="https://albumentations.ai/">Albumentations</a></p></li>
<li><p>generating synthetic data with <a class="reference external" href="https://blog.roboflow.com/synthetic-data-with-stable-diffusion-a-guide/">diffusion models</a> or <a class="reference external" href="https://towardsai.net/p/l/gans-for-synthetic-data-generation">GANs</a>.</p></li>
</ul>
<p>We’ll take the first approach and incorporate existing high-quality data from Google’s <a class="reference external" href="https://storage.googleapis.com/openimages/web/index.html">Open Images dataset</a>. For a thorough tutorial on how to work with Open Images data, see <a class="reference external" href="https://medium.com/voxel51/loading-open-images-v6-and-custom-datasets-with-fiftyone-18b5334851c3">Loading Open Images V6 and custom datasets with FiftyOne</a>.</p>
<p>The COCO training data on which YOLOv8 was trained contains <span class="math notranslate nohighlight">\(3,237\)</span> images with <code class="docutils literal notranslate"><span class="pre">bird</span></code> detections. Open Images is more expansive, with the train, test, and validation splits together housing <span class="math notranslate nohighlight">\(20k+\)</span> images with <code class="docutils literal notranslate"><span class="pre">Bird</span></code> detections.</p>
<p>Let’s create our training dataset. First, we’ll create a dataset, <code class="docutils literal notranslate"><span class="pre">train_dataset</span></code>, by loading the <code class="docutils literal notranslate"><span class="pre">bird</span></code> detection labels from the COCO train split using the <a class="reference external" href="https://docs.voxel51.com/user_guide/dataset_zoo/datasets.html">FiftyOne Dataset Zoo</a>, and cloning this into a new <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> object:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s1">&#39;coco-2017&#39;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s1">&#39;train&#39;</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="n">classes</span>
<span class="p">)</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span>

<span class="n">train_dataset</span><span class="o">.</span><span class="n">name</span> <span class="o">=</span> <span class="s2">&quot;birds-train-data&quot;</span>
<span class="n">train_dataset</span><span class="o">.</span><span class="n">persistent</span> <span class="o">=</span> <span class="kc">True</span>
<span class="n">train_dataset</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Then, we’ll load Open Images samples with <code class="docutils literal notranslate"><span class="pre">Bird</span></code> detection labels, passing in <code class="docutils literal notranslate"><span class="pre">only_matching=True</span></code> to only load the <code class="docutils literal notranslate"><span class="pre">Bird</span></code> labels. We then map these labels into COCO label format by changing <code class="docutils literal notranslate"><span class="pre">Bird</span></code> into <code class="docutils literal notranslate"><span class="pre">bird</span></code>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">oi_samples</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_dataset</span><span class="p">(</span>
    <span class="s2">&quot;open-images-v6&quot;</span><span class="p">,</span>
    <span class="n">classes</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Bird&quot;</span><span class="p">],</span>
    <span class="n">only_matching</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">label_types</span><span class="o">=</span><span class="s2">&quot;detections&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">map_labels</span><span class="p">(</span>
    <span class="s2">&quot;ground_truth&quot;</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;Bird&quot;</span><span class="p">:</span><span class="s2">&quot;bird&quot;</span><span class="p">}</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can add these new samples into our training dataset with <code class="docutils literal notranslate"><span class="pre">merge_samples()</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">train_dataset</span><span class="o">.</span><span class="n">merge_samples</span><span class="p">(</span><span class="n">oi_samples</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>This dataset contains <span class="math notranslate nohighlight">\(24,226\)</span> samples with <code class="docutils literal notranslate"><span class="pre">bird</span></code> labels, or more than seven times as many birds as the base YOLOv8n model was trained on. In the next section, we’ll demonstrate how to fine-tune the model on this data using the <a class="reference external" href="https://docs.ultralytics.com/reference/base_trainer/">YOLO Trainer class</a>.</p>
</div>
</div>
<div class="section" id="Fine-tune-a-YOLOv8-detection-model">
<h2>Fine-tune a YOLOv8 detection model<a class="headerlink" href="#Fine-tune-a-YOLOv8-detection-model" title="Permalink to this headline">¶</a></h2>
<p>The final step in preparing our data is splitting it into training and validation sets and exporting it into YOLO format. We will use an 80–20 train-val split, which we will select randomly using <a class="reference external" href="https://docs.voxel51.com/api/fiftyone.utils.random.html">FiftyOne’s random utils</a>.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.utils.random</span> <span class="k">as</span> <span class="nn">four</span>

<span class="c1">## delete existing tags to start fresh</span>
<span class="n">train_dataset</span><span class="o">.</span><span class="n">untag_samples</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">distinct</span><span class="p">(</span><span class="s2">&quot;tags&quot;</span><span class="p">))</span>

<span class="c1">## split into train and val</span>
<span class="n">four</span><span class="o">.</span><span class="n">random_split</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="p">{</span><span class="s2">&quot;train&quot;</span><span class="p">:</span> <span class="mf">0.8</span><span class="p">,</span> <span class="s2">&quot;val&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">}</span>
<span class="p">)</span>

<span class="c1">## export in YOLO format</span>
<span class="n">export_yolo_data</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="s2">&quot;birds_train&quot;</span><span class="p">,</span>
    <span class="n">classes</span><span class="p">,</span>
    <span class="n">split</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;train&quot;</span><span class="p">,</span> <span class="s2">&quot;val&quot;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now all that is left is to do the fine-tuning! We will use <a class="reference external" href="https://docs.ultralytics.com/cli/">YOLO command line syntax</a>, with <code class="docutils literal notranslate"><span class="pre">mode=train</span></code>. We will specify the initial weights as the starting point for training, the number of epochs, image size, and batch size.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>yolo<span class="w"> </span><span class="nv">task</span><span class="o">=</span>detect<span class="w"> </span><span class="nv">mode</span><span class="o">=</span>train<span class="w"> </span><span class="nv">model</span><span class="o">=</span>yolov8n.pt<span class="w"> </span><span class="nv">data</span><span class="o">=</span>birds_train/dataset.yaml<span class="w"> </span><span class="nv">epochs</span><span class="o">=</span><span class="m">60</span><span class="w"> </span><span class="nv">imgsz</span><span class="o">=</span><span class="m">640</span><span class="w"> </span><span class="nv">batch</span><span class="o">=</span><span class="m">16</span>
</pre></div>
</div>
</div>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span> Image sizes 640 train, 640 val
 Using 8 dataloader workers
 Logging results to runs/detect/train
 Starting training for 60 epochs...

 Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
 1/60       6.65G      1.392      1.627      1.345         22        640: 1
            Class     Images  Instances      Box(P          R      mAP50  m
              all       4845      12487      0.677      0.524      0.581      0.339

 Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
 2/60       9.58G      1.446      1.407      1.395         30        640: 1
            Class     Images  Instances      Box(P          R      mAP50  m
              all       4845      12487      0.669       0.47       0.54      0.316

 Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
 3/60       9.58G       1.54      1.493      1.462         29        640: 1
            Class     Images  Instances      Box(P          R      mAP50  m
              all       4845      12487      0.529      0.329      0.349      0.188

                                       ......

 Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
58/60       9.59G      1.263     0.9489      1.277         47        640: 1
            Class     Images  Instances      Box(P          R      mAP50  m
              all       4845      12487      0.751      0.631      0.708      0.446

 Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
59/60       9.59G      1.264     0.9476      1.277         29        640: 1
            Class     Images  Instances      Box(P          R      mAP50  m
              all       4845      12487      0.752      0.631      0.708      0.446

 Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size
60/60       9.59G      1.257     0.9456      1.274         41        640: 1
            Class     Images  Instances      Box(P          R      mAP50  m
              all       4845      12487      0.752      0.631      0.709      0.446
</pre></div>
</div>
<p>For this walkthrough, <span class="math notranslate nohighlight">\(60\)</span> epochs of training was sufficient to achieve convergence. If you are fine-tuning on a different dataset, you may need to change these parameters.</p>
<p>With fine-tuning complete, we can generate predictions on our test data with the “best” weights found during the training process, which are stored at <code class="docutils literal notranslate"><span class="pre">runs/detect/train/weights/best.pt</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>yolo<span class="w"> </span><span class="nv">task</span><span class="o">=</span>detect<span class="w"> </span><span class="nv">mode</span><span class="o">=</span>predict<span class="w"> </span><span class="nv">model</span><span class="o">=</span>runs/detect/train/weights/best.pt<span class="w"> </span><span class="nv">source</span><span class="o">=</span>birds_test/images/val<span class="w"> </span><span class="nv">save_txt</span><span class="o">=</span>True<span class="w"> </span><span class="nv">save_conf</span><span class="o">=</span>True
</pre></div>
</div>
</div>
<p>Then we can load these predictions onto our data and visualize the predictions in the FiftyOne App:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">filepaths</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">values</span><span class="p">(</span><span class="s2">&quot;filepath&quot;</span><span class="p">)</span>
<span class="n">prediction_filepaths</span> <span class="o">=</span> <span class="p">[</span><span class="n">get_prediction_filepath</span><span class="p">(</span><span class="n">fp</span><span class="p">,</span> <span class="n">run_number</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">fp</span> <span class="ow">in</span> <span class="n">filepaths</span><span class="p">]</span>

<span class="n">test_dataset</span><span class="o">.</span><span class="n">set_values</span><span class="p">(</span>
    <span class="s2">&quot;yolov8n_bird_det_filepath&quot;</span><span class="p">,</span>
    <span class="n">prediction_filepaths</span>
<span class="p">)</span>

<span class="n">add_yolo_detections</span><span class="p">(</span>
    <span class="n">birds_test_dataset</span><span class="p">,</span>
    <span class="s2">&quot;yolov8n_bird&quot;</span><span class="p">,</span>
    <span class="s2">&quot;yolov8n_bird_det_filepath&quot;</span><span class="p">,</span>
    <span class="n">classes</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">test_dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="yolov8-finetune-predictions" src="../_images/yolov8_finetune_predictions_app.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Assess-improvement-from-fine-tuning">
<h2>Assess improvement from fine-tuning<a class="headerlink" href="#Assess-improvement-from-fine-tuning" title="Permalink to this headline">¶</a></h2>
<p>On a holistic level, we can compare the performance of the fine-tuned model to the original, pretrained model by stacking their standard metrics against each other. The easiest way to get these metrics is with FiftyOne’s Evaluation API:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[55]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">finetune_bird_results</span> <span class="o">=</span> <span class="n">test_dataset</span><span class="o">.</span><span class="n">evaluate_detections</span><span class="p">(</span>
    <span class="s2">&quot;yolov8n_bird&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;finetune&quot;</span><span class="p">,</span>
    <span class="n">compute_mAP</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Evaluating detections...
 100% |█████████████████| 125/125 [954.4ms elapsed, 0s remaining, 131.0 samples/s]
Performing IoU sweep...
 100% |█████████████████| 125/125 [751.8ms elapsed, 0s remaining, 166.3 samples/s]
</pre></div></div>
</div>
<p>From this, we can immediately see improvement in the mean average precision (mAP):</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;yolov8n mAP: </span><span class="si">{}</span><span class="s2">.format(base_bird_results.mAP()))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;fine-tuned mAP: </span><span class="si">{}</span><span class="s2">.format(finetune_bird_results.mAP()))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
yolov8n mAP: 0.24897924786479841
fine-tuned mAP: 0.31339033693212076
</pre></div></div>
</div>
<p>Printing out a report, we can see that the recall has improved from <span class="math notranslate nohighlight">\(0.39\)</span> to <span class="math notranslate nohighlight">\(0.56\)</span>. This major improvement offsets a minor dip in precision, giving an overall higher F1 score (<span class="math notranslate nohighlight">\(0.67\)</span> compared to <span class="math notranslate nohighlight">\(0.54\)</span>).</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[56]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">finetune_bird_results</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

        bird       0.81      0.56      0.67       506

   micro avg       0.81      0.56      0.67       506
   macro avg       0.81      0.56      0.67       506
weighted avg       0.81      0.56      0.67       506

</pre></div></div>
</div>
<p>We can also look more closely at individual images to see where the fine-tuned model is having trouble. In particular, we can look at images with the most false negatives, or the most false positives:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fn_view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">&quot;eval_fn&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">fn_view</span>
</pre></div>
</div>
</div>
<p><img alt="yolov8-finetune-fp" src="../_images/yolov8_finetune_fp_predictions.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">fp_view</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">sort_by</span><span class="p">(</span><span class="s2">&quot;eval_fp&quot;</span><span class="p">,</span> <span class="n">reverse</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">fp_view</span>
</pre></div>
</div>
</div>
<p><img alt="yolov8-finetune_fn" src="../_images/yolov8_finetune_fn_predictions.png" /></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">freeze</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>Looking at both the false positives and false negatives, we can see that the model struggles to correctly handle small features. This poor performance could be in part due to quality of the data, as many of these features are grainy. It could also be due to the training parameters, as both the pre-training and fine-tuning for this model used an image size of <span class="math notranslate nohighlight">\(640\)</span> pixels, which might not allow for fine-grained details to be captured.</p>
<p>To further improve the model’s performance, we could try a variety of approaches, including:</p>
<ul class="simple">
<li><p>Using image augmentation to increase the proportion of images with small birds</p></li>
<li><p>Gathering and annotating more images with small birds</p></li>
<li><p>Increasing the image size during fine-tuning</p></li>
</ul>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¶</a></h2>
<p>While YOLOv8 represents a step forward for real-time object detection and segmentation models, out-of-the-box it’s aimed at general purpose uses. Before deploying the model, it is essential to understand how it performs on your data. Only then can you effectively fine-tune the YOLOv8 architecture to suit your specific needs.</p>
<p>You can use FiftyOne to visualize, evaluate, and better understand YOLOv8 model predictions. After all, while YOLO may only look once, a conscientious computer vision engineer or researcher certainly looks twice (or more)!</p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="pointe.html" class="btn btn-neutral float-right" title="Build a 3D self-driving dataset from scratch with OpenAI’s Point-E and FiftyOne" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="qdrant.html" class="btn btn-neutral" title="Nearest Neighbor Embeddings Classification with Qdrant" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  
</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Fine-tune YOLOv8 models for custom use cases with the help of FiftyOne</a><ul>
<li><a class="reference internal" href="#Setup">Setup</a></li>
<li><a class="reference internal" href="#Load-YOLOv8-predictions-in-FiftyOne">Load YOLOv8 predictions in FiftyOne</a><ul>
<li><a class="reference internal" href="#Generate-predictions">Generate predictions</a></li>
<li><a class="reference internal" href="#Load-detections">Load detections</a></li>
<li><a class="reference internal" href="#Load-segmentation-masks">Load segmentation masks</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Evaluate-YOLOv8-model-predictions">Evaluate YOLOv8 model predictions</a><ul>
<li><a class="reference internal" href="#Compute-summary-statistics">Compute summary statistics</a></li>
<li><a class="reference internal" href="#Inspect-individual-predictions">Inspect individual predictions</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Curate-data-for-fine-tuning">Curate data for fine-tuning</a><ul>
<li><a class="reference internal" href="#Generate-test-set">Generate test set</a></li>
<li><a class="reference internal" href="#Generate-training-set">Generate training set</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Fine-tune-a-YOLOv8-detection-model">Fine-tune a YOLOv8 detection model</a></li>
<li><a class="reference internal" href="#Assess-improvement-from-fine-tuning">Assess improvement from fine-tuning</a></li>
<li><a class="reference internal" href="#Summary">Summary</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
         <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
         <script src="../_static/js/voxel51-website.js"></script>
         <script src="../_static/js/custom.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


<footer class="footer-wrapper" id="docs-tutorials-resources">
  <div class="footer pytorch-container">
    <div class="footer__logo">
      <a href="https://voxel51.com/"
        ><img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
      /></a>
    </div>

    <div class="footer__address">
      330 E Liberty St<br />
      Ann Arbor, MI 48104<br />
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>

    <!--
    <div class="footer__contact">
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>
    -->

    <div class="footer__links">
      <div class="footer__links--col2">
        <p class="nav__item--brand">Products</p>
        <a href="https://voxel51.com/fiftyone/">FiftyOne</a>
        <a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a>
        <a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a>
        <a href="https://voxel51.com/success-stories/">Success Stories</a>
      </div>
      <div class="footer__links--col3">
        <p class="nav__item--brand">Resources</p>
        <a href="https://voxel51.com/blog/">Blog</a>
        <a href="https://docs.voxel51.com/">Docs</a>
        <a href="https://github.com/voxel51/">GitHub</a>
        <a href="https://community.voxel51.com">Discord</a>
        <a href="https://voxel51.com/ourstory/">About Us</a>
        <a href="https://voxel51.com/computer-vision-events/">Events</a>
        <a href="https://voxel51.com/jobs/">Careers</a>
        <a href="https://voxel51.com/press/">Press</a>
      </div>
    </div>

    <div class="footer__icons">
      <ul class="list-inline">
        <li>
          <a href="https://www.linkedin.com/company/voxel51/">
            <i class="fa-brands fa-linkedin"></i>
          </a>
        </li>
        <li>
          <a href="https://github.com/voxel51/">
            <i class="fa-brands fa-github"></i>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/voxel51">
            <i class="fa-brands fa-twitter"></i>
          </a>
        </li>
        <li>
          <a href="https://www.facebook.com/voxel51/">
            <i class="fa-brands fa-facebook"></i>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer__copyright">
      <ul class="list-inline">
        <li>&copy; 2024 Voxel51 Inc.</li>
        <li>
          <a href="https://voxel51.com/privacy/">Privacy Policy</a>
        </li>
        <li>
          <a href="https://voxel51.com/terms/">Terms of Service</a>
        </li>
      </ul>
    </div>
  </div>
</footer>

<!-- https://buttons.github.io -->
<script async defer src="https://buttons.github.io/buttons.js" ></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XD15NFRY3M" ></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "G-XD15NFRY3M");
</script>

<script>
  function open_modal(modal_id, modal_closer_id) {
    // Get the modal
    let the_modal = document.getElementById(modal_id);

    the_modal.style.display = "flex";
    document.body.style.overflow = "hidden";

    let the_modal_closer = document.getElementById(modal_closer_id);

    // When the user clicks on <span> (x), close the modal
    the_modal_closer &&
      (the_modal_closer.onclick = function () {
        the_modal.style.display = "none";
      });

    // When the user clicks anywhere outside of the modal, close it
    window.onclick = function (event) {
      if (event.target == the_modal) {
        the_modal.style.display = "none";
        document.body.style.overflow = "unset";
        window.onclick = undefined;
      }
    };
  }
</script>

<!-- bigpicture.io -->
<script>
  !(function (e, t, i) {
    var r = (e.bigPicture = e.bigPicture || []);
    if (!r.initialized)
      if (r.invoked)
        e.console &&
          console.error &&
          console.error("BigPicture.io snippet included twice.");
      else {
        (r.invoked = !0),
          (r.SNIPPET_VERSION = 1.5),
          (r.handler = function (e) {
            if (void 0 !== r.callback)
              try {
                return r.callback(e);
              } catch (e) {}
          }),
          (r.eventList = ["mousedown", "mouseup", "click", "submit"]),
          (r.methods = [
            "track",
            "identify",
            "page",
            "group",
            "alias",
            "integration",
            "ready",
            "intelReady",
            "consentReady",
            "on",
            "off",
          ]),
          (r.factory = function (e) {
            return function () {
              var t = Array.prototype.slice.call(arguments);
              return t.unshift(e), r.push(t), r;
            };
          });
        for (var n = 0; n < r.methods.length; n++) {
          var o = r.methods[n];
          r[o] = r.factory(o);
        }
        r.getCookie = function (e) {
          var i = ("; " + t.cookie).split("; " + e + "=");
          return 2 == i.length && i.pop().split(";").shift();
        };
        var c = (r.isEditor = (function () {
          try {
            return (
              e.self !== e.top &&
              (new RegExp("app" + i, "ig").test(t.referrer) ||
                "edit" == r.getCookie("_bpr_edit"))
            );
          } catch (e) {
            return !1;
          }
        })());
        r.init = function (n, o) {
          if (((r.projectId = n), (r._config = o), !c))
            for (var a = 0; a < r.eventList.length; a++)
              e.addEventListener(r.eventList[a], r.handler, !0);
          var s = t.createElement("script");
          s.async = !0;
          var d = c ? "/editor/editor" : "/public-" + n;
          (s.src = "//cdn" + i + d + ".js"),
            t.getElementsByTagName("head")[0].appendChild(s);
        };
      }
  })(window, document, ".bigpicture.io");
  bigPicture.init("1646");
</script>


  

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>