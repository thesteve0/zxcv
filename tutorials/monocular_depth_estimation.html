


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Monocular Depth Estimation with FiftyOne &mdash; FiftyOne 1.3.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/voxel51-website.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Visualizing Data with Dimensionality Reduction Techniques" href="dimension_reduction.html" />
    <link rel="prev" title="Build a 3D self-driving dataset from scratch with OpenAIâ€™s Point-E and FiftyOne" href="pointe.html" />
<meta property="og:image" content="https://voxel51.com/wp-content/uploads/2024/03/3.24_webpages_Home_AV.png" />

<link
  href="https://fonts.googleapis.com/css?family=Palanquin:400,600,700,800"
  rel="stylesheet"
/>
<link
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
  rel="stylesheet"
/>
<script src="https://tag.clearbitscripts.com/v1/pk_b9ed71c8234edd4f77326bcbfab5a4ca/tags.js"></script>


  
  <script src="../_static/js/modernizr.min.js"></script>

  
</head>


<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <nav id="nav__main" class="nav__main">
    <div class="nav__main__logo">
      <a href="https://voxel51.com/">
        <img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
        />
      </a>
    </div>

    <div class="nav__spacer desktop_only"></div>

    <div id="nav__main__mobilebutton--on">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-bars"></i>
      </a>
    </div>

    <div id="nav__main__mobilebutton--off">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-times"></i>
      </a>
    </div>

    <div id="nav__main__items">
      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Products</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/fiftyone/">Open Source</a></li>
            <li><a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a></li>
            <li><a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a></li>
            <li><a href="https://voxel51.com/success-stories/">Success Stories</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Learn</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://community.voxel51.com">Community Discord</a></li>
            <li><a href="https://voxel51.com/blog/">Blog</a></li>
            <li><a href="https://voxel51.com/computer-vision-events/">Events</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item">
        <a href="https://docs.voxel51.com/">Docs</a>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Company</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/ourstory/">About Us</a></li>
            <li><a href="https://voxel51.com/jobs/">Careers</a></li>
            <li><a href="https://voxel51.com/talk-to-sales/">Talk to Sales</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item mobile_only">
        <a href="https://github.com/voxel51/fiftyone">GitHub</a>
      </div>

      <div class="nav__item desktop_only" id="octocat">
        <!-- https://buttons.github.io -->
        <a
          class="github-button"
          href="https://github.com/voxel51/fiftyone"
          data-color-scheme="no-preference: dark_high_contrast; light: dark_high_contrast; dark: dark_high_contrast;"
          data-size="large"
          data-show-count="true"
          aria-label="Star voxel51/fiftyone on GitHub"
          >Star</a
        >
      </div>

      <div class="nav__item full_nav_only">
        <a class="button-primary" href="https://voxel51.com/schedule-teams-workshop/" target="_blank"
          >Schedule a workshop</a
        >
      </div>
    </div>
  </nav>
</div>



<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

           <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../teams/index.html">FiftyOne Teams ðŸš€</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/index.html">Environments</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pandas_comparison.html">pandas and FiftyOne</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluate_detections.html">Evaluating object detections</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluate_classifications.html">Evaluating a classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_embeddings.html">Using image embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="cvat_annotation.html">Annotating with CVAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="labelbox_annotation.html">Annotating with Labelbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="open_images.html">Working with Open Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="detectron2.html">Training with Detectron2</a></li>
<li class="toctree-l2"><a class="reference internal" href="uniqueness.html">Exploring image uniqueness</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_mistakes.html">Finding class mistakes</a></li>
<li class="toctree-l2"><a class="reference internal" href="detection_mistakes.html">Finding detection mistakes</a></li>
<li class="toctree-l2"><a class="reference internal" href="qdrant.html">Embeddings with Qdrant</a></li>
<li class="toctree-l2"><a class="reference internal" href="yolov8.html">Fine-tuning YOLOv8 models</a></li>
<li class="toctree-l2"><a class="reference internal" href="pointe.html">3D point clouds with Point-E</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Monocular depth estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="dimension_reduction.html">Dimensionality reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="zero_shot_classification.html">Zero-shot classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_augmentation.html">Data augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering.html">Clustering images</a></li>
<li class="toctree-l2"><a class="reference internal" href="small_object_detection.html">Detecting small objects</a></li>
<li class="toctree-l2"><a class="reference internal" href="anomaly_detection.html">Anomaly detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/index.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheets/index.html">Cheat Sheets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_zoo/index.html">Dataset Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brain.html">FiftyOne Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../integrations/index.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins/index.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/fiftyone.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deprecation.html">Deprecation Notices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>
 
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">FiftyOne Tutorials</a> &gt;</li>
        
      <li>Monocular Depth Estimation with FiftyOne</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Contents
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content style-external-links">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<table class="fo-notebook-links" align="left">
    <td>
        <a target="_blank" href="https://colab.research.google.com/github/voxel51/fiftyone/blob/v1.3.0/docs/source/tutorials/monocular_depth_estimation.ipynb">
            <img src="../_static/images/icons/colab-logo-256px.png"> &nbsp; Run in Google Colab
        </a>
    </td>
    <td>
        <a target="_blank" href="https://github.com/voxel51/fiftyone/blob/v1.3.0/docs/source/tutorials/monocular_depth_estimation.ipynb">
            <img src="../_static/images/icons/github-logo-256px.png"> &nbsp; View source on GitHub
        </a>
    </td>
    <td>
        <a target="_blank" href="https://raw.githubusercontent.com/voxel51/fiftyone/v1.3.0/docs/source/tutorials/monocular_depth_estimation.ipynb" download>
            <img src="../_static/images/icons/cloud-icon-256px.png"> &nbsp; Download notebook
        </a>
    </td>
</table><div class="section" id="Monocular-Depth-Estimation-with-FiftyOne">
<h1>Monocular Depth Estimation with FiftyOne<a class="headerlink" href="#Monocular-Depth-Estimation-with-FiftyOne" title="Permalink to this headline">Â¶</a></h1>
<p>In this walkthrough, youâ€™ll learn how to run monocular depth estimation models on your data using FiftyOne, Replicate, and Hugging Face libraries!</p>
<p>It covers the following:</p>
<ul class="simple">
<li><p>What is monocular depth estimation?</p></li>
<li><p>Downloading the SUN RGB-D dataset from source and loading it into FiftyOne</p></li>
<li><p>Running monocular depth estimation models on your data</p></li>
<li><p>Evaluating prediction performance</p></li>
<li><p>Visualizing the results in FiftyOne</p></li>
</ul>
<div class="section" id="What-is-Monocular-Depth-Estimation?">
<h2>What is Monocular Depth Estimation?<a class="headerlink" href="#What-is-Monocular-Depth-Estimation?" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference external" href="https://paperswithcode.com/task/monocular-depth-estimation">Monocular depth estimation</a> is the task of predicting the depth of a scene from <em>a single image</em>. Often, depth information is necessary for downstream tasks, such as 3D reconstruction or scene understanding. However, depth sensors are expensive and not always available.</p>
<p>This is a challenging task because depth is inherently ambiguous from a single image. The same scene can be projected onto the image plane in many different ways, and it is impossible to know which one is correct without additional information.</p>
<p>If you have multiple cameras, you can use <a class="reference external" href="https://paperswithcode.com/task/stereo-depth-estimation">stereo depth estimation</a> techniques. But in some real world scenarios, you may be constrained to a single camera. When this is the case, you must rely on other cues, such as object size, occlusion, and perspective.</p>
<div class="section" id="Applications">
<h3>Applications<a class="headerlink" href="#Applications" title="Permalink to this headline">Â¶</a></h3>
<p>Monocular depth estimation has many applications in computer vision. For example, it can be used for:</p>
<ul class="simple">
<li><p><strong>3D reconstruction</strong>: Given a single image, estimate the depth of the scene and reconstruct the 3D geometry of the scene.</p></li>
<li><p><strong>Scene understanding</strong>: Given a single image, estimate the depth of the scene and use it to understand the scene better.</p></li>
<li><p><strong>Autonomous driving</strong>: Given a single image, estimate the depth of the scene and use it to navigate the vehicle.</p></li>
<li><p><strong>Augmented reality</strong>: Given a single image, estimate the depth of the scene and use it to place virtual objects in the scene.</p></li>
</ul>
<p>Beyond these industry applications, the ability to extract high-quality depth information from a single image has found fascinating use cases in content creation and editing, for instance:</p>
<ul class="simple">
<li><p><strong>Image editing</strong>: Given a single image, estimate the depth of the scene and use it to apply depth-aware effects to the image.</p></li>
<li><p><strong>Image generation</strong>: Given a single image, estimate the depth of the scene and use it to generate a 3D model of the scene.</p></li>
<li><p><strong>Depth-map guided text-to-image generation</strong>: Given a single image, estimate the depth of the scene and use it to generate a new image that both adheres to your input text prompt and has the same depth map. (See <a class="reference external" href="https://huggingface.co/lllyasviel/sd-controlnet-depth">ControlNet</a>!)</p></li>
</ul>
</div>
</div>
<div class="section" id="Create-Dataset">
<h2>Create Dataset<a class="headerlink" href="#Create-Dataset" title="Permalink to this headline">Â¶</a></h2>
<p>First, we import all the necessary libraries, installing <code class="docutils literal notranslate"><span class="pre">fiftyone</span></code> if necessary:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>fiftyone
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">glob</span> <span class="kn">import</span> <span class="n">glob</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">import</span> <span class="nn">torch</span>

<span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>
<span class="kn">import</span> <span class="nn">fiftyone.brain</span> <span class="k">as</span> <span class="nn">fob</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span>
</pre></div>
</div>
</div>
<p>Download the SUN RGB-D dataset from <a class="reference external" href="https://rgbd.cs.princeton.edu/">here</a> and unzip it, or use the following command to download it directly:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>curl<span class="w"> </span>-o<span class="w"> </span>sunrgbd.zip<span class="w"> </span>https://rgbd.cs.princeton.edu/data/SUNRGBD.zip
</pre></div>
</div>
</div>
<p>and then unzip it:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>unzip<span class="w"> </span>sunrgbd.zip
</pre></div>
</div>
</div>
<p>The <a class="reference external" href="https://rgbd.cs.princeton.edu/">SUN RGB-D dataset</a> contains 10,335 RGB-D images, each of which has a corresponding RGB image, depth image, and camera intrinsics. It contains images from the <a class="reference external" href="https://cs.nyu.edu/~silberman/datasets/nyu_depth_v2.html">NYU depth v2</a>, Berkeley <a class="reference external" href="http://kinectdata.com/">B3DO</a>, and <a class="reference external" href="https://sun3d.cs.princeton.edu/">SUN3D</a> datasets. SUN RGB-D is <a class="reference external" href="https://paperswithcode.com/dataset/sun-rgb-d">one of the most popular</a> datasets for monocular depth
estimation and semantic segmentation tasks!</p>
<p>If you want to use the dataset for other tasks, you can fully convert the annotations and load them into your <code class="docutils literal notranslate"><span class="pre">fiftyone.Dataset</span></code>. However, for this tutorial, we will only be using the depth images, so we will only use the RGB images and the depth images (stored in the <code class="docutils literal notranslate"><span class="pre">depth_bfx</span></code> sub-directories).</p>
<p>Because we are just interested in getting the point across, weâ€™ll restrict ourselves to the first 20 samples.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Dataset</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="s2">&quot;SUNRGBD-20&quot;</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Load in images and ground truth data</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## restrict to 20 scenes</span>
<span class="n">scene_dirs</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="s2">&quot;SUNRGBD/k*/*/*&quot;</span><span class="p">)[:</span><span class="mi">20</span><span class="p">]</span>
</pre></div>
</div>
</div>
<p>We will be representing depth maps with FiftyOneâ€™s <a class="reference external" href="https://docs.voxel51.com/user_guide/using_datasets.html#heatmaps">Heatmap</a> labels. For a thorough guide to working with heatmaps in FiftyOne, check out these <a class="reference external" href="https://voxel51.com/blog/heatmaps-fiftyone-tips-and-tricks-october-6th-2023/">FiftyOne Heatmaps Tips and Tricks</a>!</p>
<p>We are going to store everything in terms of normalized, <em>relative</em> distances, where 255 represents the maximum distance in the scene and 0 represents the minimum distance in the scene. This is a common way to represent depth maps, although it is far from the only way to do so. If we were interested in <em>absolute</em> distances, we could store sample-wise parameters for the minimum and maximum distances in the scene, and use these to reconstruct the absolute distances from the relative distances.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">scene_dir</span> <span class="ow">in</span> <span class="n">scene_dirs</span><span class="p">:</span>
    <span class="c1">## Get image file path from scene directory</span>
    <span class="n">image_path</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">scene_dir</span><span class="si">}</span><span class="s2">/image/*&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1">## Get depth map file path from scene directory</span>
    <span class="n">depth_path</span> <span class="o">=</span> <span class="n">glob</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">scene_dir</span><span class="si">}</span><span class="s2">/depth_bfx/*&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">depth_map</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">depth_path</span><span class="p">))</span>
    <span class="n">depth_map</span> <span class="o">=</span> <span class="p">(</span><span class="n">depth_map</span> <span class="o">*</span> <span class="mi">255</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">depth_map</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
    <span class="n">sample</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Sample</span><span class="p">(</span>
        <span class="n">filepath</span><span class="o">=</span><span class="n">image_path</span><span class="p">,</span>
        <span class="n">gt_depth</span><span class="o">=</span><span class="n">fo</span><span class="o">.</span><span class="n">Heatmap</span><span class="p">(</span><span class="nb">map</span><span class="o">=</span><span class="n">depth_map</span><span class="p">),</span>
    <span class="p">)</span>

    <span class="n">samples</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">add_samples</span><span class="p">(</span><span class="n">samples</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [192.2ms elapsed, 0s remaining, 104.1 samples/s]
</pre></div></div>
</div>
<p>We can then visualize our images and depth maps in the <a class="reference external" href="https://docs.voxel51.com/user_guide/app.html">FiftyOne App</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">auto</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
<span class="c1">## then open tab to localhost:5151 in browser</span>
</pre></div>
</div>
</div>
<p><img alt="sun-rgbd-dataset" src="../_images/mde_gt_heatmaps.png" /></p>
<p>When working with depth maps, the color scheme and opacity of the heatmap are important. We can customize these as illustrated <a class="reference external" href="https://docs.voxel51.com/user_guide/app.html#color-schemes">here</a>.</p>
<p><img alt="color-customization" src="../_images/mde_color_customization.gif" /></p>
<div class="section" id="Ground-Truth?">
<h3>Ground Truth?<a class="headerlink" href="#Ground-Truth?" title="Permalink to this headline">Â¶</a></h3>
<p>Inspecting these RGB images and depth maps, we can see that there are some inaccuracies in the ground truth depth maps. For example, in this image, the dark rift through the center of the image is actually the <em>farthest</em> part of the scene, but the ground truth depth map shows it as the <em>closest</em> part of the scene:</p>
<p><img alt="gt-issue" src="../_images/mde_gt_issue.png" /></p>
</div>
</div>
<div class="section" id="Run-Monocular-Depth-Estimation-Models">
<h2>Run Monocular Depth Estimation Models<a class="headerlink" href="#Run-Monocular-Depth-Estimation-Models" title="Permalink to this headline">Â¶</a></h2>
<p>Now that we have our dataset loaded in, we can run monocular depth estimation models on it! For a long time, the state-of-the-art models for monocular depth estimation such as <a class="reference external" href="https://github.com/hufu6371/DORN">DORN</a> and <a class="reference external" href="https://github.com/ialhashim/DenseDepth">DenseDepth</a> were built with convolutional neural networks. Recently, however, both transformer-based models (<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/dpt">DPT</a>,
<a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/glpn">GLPN</a>) and diffusion-based models (<a class="reference external" href="https://huggingface.co/Bingxin/Marigold">Marigold</a>) have achieved remarkable results!</p>
<div class="section" id="DPT-(Transformer-Models)">
<h3>DPT (Transformer Models)<a class="headerlink" href="#DPT-(Transformer-Models)" title="Permalink to this headline">Â¶</a></h3>
<p>The first model weâ€™ll run is a Transformer-based model called <a class="reference external" href="https://huggingface.co/docs/transformers/model_doc/dpt">DPT</a>. The checkpoint below uses <a class="reference external" href="https://github.com/isl-org/MiDaS/tree/master">MiDaS</a>, which returns the <a class="reference external" href="https://pyimagesearch.com/2022/01/17/torch-hub-series-5-midas-model-on-depth-estimation/">inverse depth map</a>, so we have to invert it back to get a comparable depth map.</p>
</div>
<div class="section" id="Option-1:-Run-locally-with-Hugging-Face-Transformers">
<h3>Option 1: Run locally with Hugging Face <a class="reference external" href="https://huggingface.co/docs/transformers/index">Transformers</a><a class="headerlink" href="#Option-1:-Run-locally-with-Hugging-Face-Transformers" title="Permalink to this headline">Â¶</a></h3>
<p>If necessary, install <code class="docutils literal notranslate"><span class="pre">transformers</span></code>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>transformers
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoImageProcessor</span><span class="p">,</span> <span class="n">AutoModelForDepthEstimation</span>

<span class="n">image_processor</span> <span class="o">=</span> <span class="n">AutoImageProcessor</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Intel/dpt-hybrid-midas&quot;</span><span class="p">)</span>
<span class="n">dpt_model</span> <span class="o">=</span> <span class="n">AutoModelForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Intel/dpt-hybrid-midas&quot;</span><span class="p">)</span>

<span class="c1">## you can also us a different model:</span>
<span class="c1"># image_processor = AutoImageProcessor.from_pretrained(&quot;Intel/dpt-large&quot;)</span>
<span class="c1"># dpt_model = AutoModelForDepthEstimation.from_pretrained(&quot;Intel/dpt-large&quot;)</span>

<span class="k">def</span> <span class="nf">apply_dpt_model</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="p">):</span>
    <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">image_processor</span><span class="p">(</span><span class="n">images</span><span class="o">=</span><span class="n">image</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span>

    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">predicted_depth</span> <span class="o">=</span> <span class="n">outputs</span><span class="o">.</span><span class="n">predicted_depth</span>

    <span class="n">prediction</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">functional</span><span class="o">.</span><span class="n">interpolate</span><span class="p">(</span>
        <span class="n">predicted_depth</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>
        <span class="n">size</span><span class="o">=</span><span class="n">image</span><span class="o">.</span><span class="n">size</span><span class="p">[::</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;bicubic&quot;</span><span class="p">,</span>
        <span class="n">align_corners</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">output</span> <span class="o">=</span> <span class="n">prediction</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
    <span class="c1">## flip b/c MiDaS returns inverse depth</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">-</span> <span class="n">output</span> <span class="o">*</span> <span class="mi">255</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">output</span><span class="p">))</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>

    <span class="n">sample</span><span class="p">[</span><span class="n">label_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Heatmap</span><span class="p">(</span><span class="nb">map</span><span class="o">=</span><span class="n">formatted</span><span class="p">)</span>
    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">autosave</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">apply_dpt_model</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">dpt_model</span><span class="p">,</span> <span class="s2">&quot;dpt&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [15.1s elapsed, 0s remaining, 1.5 samples/s]
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="dpt-heatmaps" src="../_images/mde_dpt_heatmaps.png" /></p>
<div class="section" id="Interpolating-Depth-Maps">
<h4>Interpolating Depth Maps<a class="headerlink" href="#Interpolating-Depth-Maps" title="Permalink to this headline">Â¶</a></h4>
<p>In our <code class="docutils literal notranslate"><span class="pre">apply_dpt_model()</span></code> function, between the modelâ€™s forward pass and the heatmap generation, notice that we make a call to <code class="docutils literal notranslate"><span class="pre">torch.nn.functional.interpolate()</span></code>. This is because the modelâ€™s forward pass is run on a downsampled version of the image, and we want to return a heatmap that is the same size as the original image.</p>
<p>Why do we need to do this? If we just want to <em>look</em> at the heatmaps, this would not matter. But if we want to compare the ground truth depth maps to the modelâ€™s predictions on a per-pixel basis, we need to make sure that they are the same size.</p>
</div>
<div class="section" id="Hugging-Face-Transformers-Integration">
<h4>Hugging Face Transformers Integration<a class="headerlink" href="#Hugging-Face-Transformers-Integration" title="Permalink to this headline">Â¶</a></h4>
<p>In this example, we manually applied the <code class="docutils literal notranslate"><span class="pre">transformers</span></code> model to our data to generate heatmaps. In practice, we have made it even easier to apply transformer-based models (for monocular depth estimation as well as other tasks) to your data via FiftyOneâ€™s <a class="reference external" href="https://docs.voxel51.com/integrations/huggingface.html">Hugging Face Transformers Integration</a>!</p>
<p>You can load the transformer models via Hugging Faceâ€™s <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library, and then just apply them to FiftyOne datasets via the <code class="docutils literal notranslate"><span class="pre">apply_model()</span></code> method:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># DPT</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DPTForDepthEstimation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">DPTForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Intel/dpt-large&quot;</span><span class="p">)</span>

<span class="c1"># GLPN</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">GLPNForDepthEstimation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">GLPNForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;vinvino02/glpn-kitti&quot;</span><span class="p">)</span>

<span class="c1"># Depth Anything</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForDepthEstimation</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForDepthEstimation</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;LiheYoung/depth-anything-small-hf&quot;</span><span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;depth_predictions&quot;</span><span class="p">)</span>

<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Alternatively, you can load any Hugging Face Transformers model directly from the <a class="reference external" href="https://docs.voxel51.com/user_guide/model_zoo/index.html">FiftyOne Model Zoo</a> via the name <code class="docutils literal notranslate"><span class="pre">depth-estimation-transformer-torch</span></code>, and specifying the modelâ€™s location on the Hugging Face Hub (<code class="docutils literal notranslate"><span class="pre">repo_id</span></code>) via the <code class="docutils literal notranslate"><span class="pre">name_or_path</span></code> parameter. To load and apply <a class="reference external" href="https://huggingface.co/Intel/dpt-hybrid-midas">this DPT MiDaS hybrid model</a>, for instance, you would use the following:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;depth-estimation-transformer-torch&quot;</span><span class="p">,</span>
    <span class="n">name_or_path</span><span class="o">=</span><span class="s2">&quot;Intel/dpt-hybrid-midas&quot;</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">dataset</span><span class="o">.</span><span class="n">apply_model</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="o">=</span><span class="s2">&quot;dpt_hybrid_midas&quot;</span><span class="p">)</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</div>
<div class="section" id="Option-2:-Run-with-Replicate">
<h3>Option 2: Run with <a class="reference external" href="https://replicate.com/">Replicate</a><a class="headerlink" href="#Option-2:-Run-with-Replicate" title="Permalink to this headline">Â¶</a></h3>
<p>Install the <code class="docutils literal notranslate"><span class="pre">replicate</span></code> Python client if necessary:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>replicate
</pre></div>
</div>
</div>
<p>And set your API Token:</p>
<p>Then run the following command:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span><span class="nb">export</span><span class="w"> </span><span class="nv">REPLICATE_API_TOKEN</span><span class="o">=</span>r8_&lt;your_token_here&gt;
</pre></div>
</div>
</div>
<p>ðŸ’¡ It might take a minute for the model to load into memory on the server (cold-start problem), but once it does the prediction should only take a few seconds.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">replicate</span>
<span class="kn">import</span> <span class="nn">requests</span>

<span class="n">rgb_fp</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">first</span><span class="p">()</span><span class="o">.</span><span class="n">filepath</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">replicate</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
    <span class="s2">&quot;cjwbw/midas:a6ba5798f04f80d3b314de0f0a62277f21ab3503c60c84d4817de83c5edfdae0&quot;</span><span class="p">,</span>
    <span class="nb">input</span><span class="o">=</span><span class="p">{</span>
        <span class="s2">&quot;model_type&quot;</span><span class="p">:</span> <span class="s2">&quot;dpt_beit_large_512&quot;</span><span class="p">,</span>
        <span class="s2">&quot;image&quot;</span><span class="p">:</span><span class="nb">open</span><span class="p">(</span><span class="n">rgb_fp</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>
    <span class="p">}</span>
<span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Marigold-(Diffusion-Models)">
<h3>Marigold (Diffusion Models)<a class="headerlink" href="#Marigold-(Diffusion-Models)" title="Permalink to this headline">Â¶</a></h3>
<p>While diffusion is a very powerful approach to monocular depth estimation, it is also very computationally expensive and can take a while. I personally recommend going for option 2, where predictions with Replicate take about 15 seconds per image.</p>
</div>
<div class="section" id="Option-1:-Download-and-run-locally-with-Hugging-Face-Diffusers">
<h3>Option 1: Download and run locally with Hugging Face <a class="reference external" href="https://huggingface.co/docs/diffusers/index">Diffusers</a><a class="headerlink" href="#Option-1:-Download-and-run-locally-with-Hugging-Face-Diffusers" title="Permalink to this headline">Â¶</a></h3>
<p>Clone the Marigold GH repo:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>git<span class="w"> </span>clone<span class="w"> </span>https://github.com/prs-eth/Marigold.git
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">Marigold.marigold</span> <span class="kn">import</span> <span class="n">MarigoldPipeline</span>
<span class="n">pipe</span> <span class="o">=</span> <span class="n">MarigoldPipeline</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s2">&quot;Bingxin/Marigold&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Then prediction looks like:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rgb_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">dataset</span><span class="o">.</span><span class="n">first</span><span class="p">()</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span>
<span class="n">output</span> <span class="o">=</span> <span class="n">pipe</span><span class="p">(</span><span class="n">rgb_image</span><span class="p">)</span>
<span class="n">depth_image</span> <span class="o">=</span> <span class="n">output</span><span class="p">[</span><span class="s1">&#39;depth_colored&#39;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="Option-2:-Run-via-Replicate">
<h3>Option 2: Run via <a class="reference external" href="https://replicate.com/">Replicate</a><a class="headerlink" href="#Option-2:-Run-via-Replicate" title="Permalink to this headline">Â¶</a></h3>
<p>ðŸ’¡ It might take a minute for the model to load into memory on the server (cold-start problem), but once it does the prediction should only take a few seconds.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[29]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">replicate</span>
<span class="kn">import</span> <span class="nn">requests</span>
<span class="kn">import</span> <span class="nn">io</span>

<span class="k">def</span> <span class="nf">marigold_model</span><span class="p">(</span><span class="n">rgb_image</span><span class="p">):</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">replicate</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="s2">&quot;adirik/marigold:1a363593bc4882684fc58042d19db5e13a810e44e02f8d4c32afd1eb30464818&quot;</span><span class="p">,</span>
        <span class="nb">input</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;image&quot;</span><span class="p">:</span><span class="n">rgb_image</span>
        <span class="p">}</span>
    <span class="p">)</span>
    <span class="c1">## get the black and white depth map</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">requests</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">output</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span><span class="o">.</span><span class="n">content</span>
    <span class="k">return</span> <span class="n">response</span>

<span class="k">def</span> <span class="nf">apply_marigold_model</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">label_field</span><span class="p">):</span>
    <span class="n">rgb_image</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">rgb_image</span><span class="p">)</span>
    <span class="n">depth_image</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">io</span><span class="o">.</span><span class="n">BytesIO</span><span class="p">(</span><span class="n">response</span><span class="p">)))[:,</span> <span class="p">:,</span> <span class="mi">0</span><span class="p">]</span> <span class="c1">## all channels are the same</span>
    <span class="n">formatted</span> <span class="o">=</span> <span class="p">(</span><span class="mi">255</span> <span class="o">-</span> <span class="n">depth_image</span><span class="p">)</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s2">&quot;uint8&quot;</span><span class="p">)</span>
    <span class="n">sample</span><span class="p">[</span><span class="n">label_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Heatmap</span><span class="p">(</span><span class="nb">map</span><span class="o">=</span><span class="n">formatted</span><span class="p">)</span>
    <span class="n">sample</span><span class="o">.</span><span class="n">save</span><span class="p">()</span>

<span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">autosave</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="n">apply_marigold_model</span><span class="p">(</span><span class="n">sample</span><span class="p">,</span> <span class="n">marigold_model</span><span class="p">,</span> <span class="s2">&quot;marigold&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
 100% |â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20/20 [5.3m elapsed, 0s remaining, 0.1 samples/s]
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="marigold-heatmaps" src="../_images/mde_marigold_heatmaps.png" /></p>
</div>
</div>
<div class="section" id="Evaluate-Predictions">
<h2>Evaluate Predictions<a class="headerlink" href="#Evaluate-Predictions" title="Permalink to this headline">Â¶</a></h2>
<p>Now that we have predictions from multiple models, letâ€™s evaluate them! We will leverage sklearn to apply three simple metrics commonly used for monocular depth estimation: <a class="reference external" href="https://en.wikipedia.org/wiki/Root-mean-square_deviation">root mean squared error (RMSE)</a>, <a class="reference external" href="https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio">peak signal to noise ratio (PSNR)</a>, and <a class="reference external" href="https://en.wikipedia.org/wiki/Structural_similarity">structural similarity index (SSIM)</a>.</p>
<p>ðŸ’¡ Higher PSNR and SSIM scores indicate better predictions, while lower RMSE scores indicate better predictions.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[38]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">skimage.metrics</span> <span class="kn">import</span> <span class="n">peak_signal_noise_ratio</span><span class="p">,</span> <span class="n">mean_squared_error</span><span class="p">,</span> <span class="n">structural_similarity</span>

<span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">pred</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Compute root mean squared error between ground truth and prediction&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">mean_squared_error</span><span class="p">(</span><span class="n">gt</span><span class="p">,</span> <span class="n">pred</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">evaluate_depth</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">prediction_field</span><span class="p">,</span> <span class="n">gt_field</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">dataset</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">autosave</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">gt_map</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="n">gt_field</span><span class="p">]</span><span class="o">.</span><span class="n">map</span>
        <span class="n">pred</span> <span class="o">=</span> <span class="n">sample</span><span class="p">[</span><span class="n">prediction_field</span><span class="p">]</span>
        <span class="n">pred_map</span> <span class="o">=</span> <span class="n">pred</span><span class="o">.</span><span class="n">map</span>
        <span class="n">pred</span><span class="p">[</span><span class="s2">&quot;rmse&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rmse</span><span class="p">(</span><span class="n">gt_map</span><span class="p">,</span> <span class="n">pred_map</span><span class="p">)</span>
        <span class="n">pred</span><span class="p">[</span><span class="s2">&quot;psnr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">peak_signal_noise_ratio</span><span class="p">(</span><span class="n">gt_map</span><span class="p">,</span> <span class="n">pred_map</span><span class="p">)</span>
        <span class="n">pred</span><span class="p">[</span><span class="s2">&quot;ssim&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">structural_similarity</span><span class="p">(</span><span class="n">gt_map</span><span class="p">,</span> <span class="n">pred_map</span><span class="p">)</span>
        <span class="n">sample</span><span class="p">[</span><span class="n">prediction_field</span><span class="p">]</span> <span class="o">=</span> <span class="n">pred</span>

    <span class="c1">## add dynamic fields to dataset so we can view them in the App</span>
    <span class="n">dataset</span><span class="o">.</span><span class="n">add_dynamic_sample_fields</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">evaluate_depth</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;dpt&quot;</span><span class="p">,</span> <span class="s2">&quot;gt_depth&quot;</span><span class="p">)</span>
<span class="n">evaluate_depth</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="s2">&quot;marigold&quot;</span><span class="p">,</span> <span class="s2">&quot;gt_depth&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We can then compute average metrics across the entire dataset very easily:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[66]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean Error Metrics&quot;</span><span class="p">)</span>
<span class="k">for</span> <span class="n">model</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;dpt&quot;</span><span class="p">,</span> <span class="s2">&quot;marigold&quot;</span><span class="p">]:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;-&quot;</span><span class="o">*</span><span class="mi">50</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">metric</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;rmse&quot;</span><span class="p">,</span> <span class="s2">&quot;psnr&quot;</span><span class="p">,</span> <span class="s2">&quot;ssim&quot;</span><span class="p">]:</span>
        <span class="n">mean_metric_value</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Mean </span><span class="si">{</span><span class="n">metric</span><span class="si">}</span><span class="s2"> for </span><span class="si">{</span><span class="n">model</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">mean_metric_value</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Mean Error Metrics
--------------------------------------------------
Mean rmse for dpt: 49.8915828817003
Mean psnr for dpt: 14.805904629602551
Mean ssim for dpt: 0.8398022368184576
--------------------------------------------------
Mean rmse for marigold: 104.0061165272178
Mean psnr for marigold: 7.93015537185192
Mean ssim for marigold: 0.42766803372861134
</pre></div></div>
</div>
<p>All of the metrics seem to agree that DPT outperforms Marigold. However, it is important to note that these metrics are not perfect. For example, RMSE is very sensitive to outliers, and SSIM is not very sensitive to small errors. For a more thorough evaluation, we can filter by these metrics in the app in order to visualize what the model is doing well and what it is doing poorly â€” or where the metrics are failing to capture the modelâ€™s performance.</p>
<p>Toggling masks on and off is a great way to visualize the differences between the ground truth and the modelâ€™s predictions:</p>
<p><img alt="compare-heatmaps" src="../_images/mde_compare_heatmaps.gif" /></p>
</div>
<div class="section" id="Key-Challenges-with-Monocular-Depth-Estimation">
<h2>Key Challenges with Monocular Depth Estimation<a class="headerlink" href="#Key-Challenges-with-Monocular-Depth-Estimation" title="Permalink to this headline">Â¶</a></h2>
<p>Now that weâ€™ve explored some model predictions, letâ€™s quickly recap some of the key challenges with monocular depth estimation:</p>
<div class="section" id="Data-quality-and-quantity">
<h3>Data quality and quantity<a class="headerlink" href="#Data-quality-and-quantity" title="Permalink to this headline">Â¶</a></h3>
<p>Ground truth data is hard to come by, and is often noisy. For example, the SUN RGB-D dataset contains 10,335 RGB-D images, which is a lot, but it is still a relatively small dataset compared to other datasets such as ImageNet, which contains 1.2 million images. And in many cases, the ground truth data is noisy. For example, the ground truth depth maps in the SUN RGB-D dataset are generated by projecting the 3D point clouds onto the 2D image plane, and then computing the Euclidean distance
between the projected points and the camera. This process is inherently noisy, and the resulting depth maps are often noisy as well.</p>
</div>
<div class="section" id="Poor-generalization">
<h3>Poor generalization<a class="headerlink" href="#Poor-generalization" title="Permalink to this headline">Â¶</a></h3>
<p>Models often struggle to generalize to new environments. Outdoors, for example, is a very different environment than indoors, and models trained on indoor data often fail to generalize to outdoor data.</p>
</div>
<div class="section" id="Precarious-metrics">
<h3>Precarious metrics<a class="headerlink" href="#Precarious-metrics" title="Permalink to this headline">Â¶</a></h3>
<p>Metrics are not always a good indicator of model performance. For example, a model might have a low RMSE, but still produce very noisy depth maps. This is why it is important to look at the depth maps themselves, and not just the metrics!</p>
</div>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">Â¶</a></h2>
<p>In this walkthrough, we learned how to run monocular depth estimation models on your data using FiftyOne, Replicate, and Hugging Face libraries! We also learned how to evaluate the predictions using common metrics, and how to visualize the results in FiftyOne. In real-world applications, it is important to look at the depth maps themselves, and not just the metrics! It is also important to understand that model performance is limited by the quality, quantity, and diversity of data they are
trained on.</p>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="dimension_reduction.html" class="btn btn-neutral float-right" title="Visualizing Data with Dimensionality Reduction Techniques" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="pointe.html" class="btn btn-neutral" title="Build a 3D self-driving dataset from scratch with OpenAIâ€™s Point-E and FiftyOne" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  
</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Monocular Depth Estimation with FiftyOne</a><ul>
<li><a class="reference internal" href="#What-is-Monocular-Depth-Estimation?">What is Monocular Depth Estimation?</a><ul>
<li><a class="reference internal" href="#Applications">Applications</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Create-Dataset">Create Dataset</a><ul>
<li><a class="reference internal" href="#Ground-Truth?">Ground Truth?</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Run-Monocular-Depth-Estimation-Models">Run Monocular Depth Estimation Models</a><ul>
<li><a class="reference internal" href="#DPT-(Transformer-Models)">DPT (Transformer Models)</a></li>
<li><a class="reference internal" href="#Option-1:-Run-locally-with-Hugging-Face-Transformers">Option 1: Run locally with Hugging Face Transformers</a><ul>
<li><a class="reference internal" href="#Interpolating-Depth-Maps">Interpolating Depth Maps</a></li>
<li><a class="reference internal" href="#Hugging-Face-Transformers-Integration">Hugging Face Transformers Integration</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Option-2:-Run-with-Replicate">Option 2: Run with Replicate</a></li>
<li><a class="reference internal" href="#Marigold-(Diffusion-Models)">Marigold (Diffusion Models)</a></li>
<li><a class="reference internal" href="#Option-1:-Download-and-run-locally-with-Hugging-Face-Diffusers">Option 1: Download and run locally with Hugging Face Diffusers</a></li>
<li><a class="reference internal" href="#Option-2:-Run-via-Replicate">Option 2: Run via Replicate</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Evaluate-Predictions">Evaluate Predictions</a></li>
<li><a class="reference internal" href="#Key-Challenges-with-Monocular-Depth-Estimation">Key Challenges with Monocular Depth Estimation</a><ul>
<li><a class="reference internal" href="#Data-quality-and-quantity">Data quality and quantity</a></li>
<li><a class="reference internal" href="#Poor-generalization">Poor generalization</a></li>
<li><a class="reference internal" href="#Precarious-metrics">Precarious metrics</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Summary">Summary</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
         <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
         <script src="../_static/js/voxel51-website.js"></script>
         <script src="../_static/js/custom.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


<footer class="footer-wrapper" id="docs-tutorials-resources">
  <div class="footer pytorch-container">
    <div class="footer__logo">
      <a href="https://voxel51.com/"
        ><img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
      /></a>
    </div>

    <div class="footer__address">
      330 E Liberty St<br />
      Ann Arbor, MI 48104<br />
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>

    <!--
    <div class="footer__contact">
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>
    -->

    <div class="footer__links">
      <div class="footer__links--col2">
        <p class="nav__item--brand">Products</p>
        <a href="https://voxel51.com/fiftyone/">FiftyOne</a>
        <a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a>
        <a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a>
        <a href="https://voxel51.com/success-stories/">Success Stories</a>
      </div>
      <div class="footer__links--col3">
        <p class="nav__item--brand">Resources</p>
        <a href="https://voxel51.com/blog/">Blog</a>
        <a href="https://docs.voxel51.com/">Docs</a>
        <a href="https://github.com/voxel51/">GitHub</a>
        <a href="https://community.voxel51.com">Discord</a>
        <a href="https://voxel51.com/ourstory/">About Us</a>
        <a href="https://voxel51.com/computer-vision-events/">Events</a>
        <a href="https://voxel51.com/jobs/">Careers</a>
        <a href="https://voxel51.com/press/">Press</a>
      </div>
    </div>

    <div class="footer__icons">
      <ul class="list-inline">
        <li>
          <a href="https://www.linkedin.com/company/voxel51/">
            <i class="fa-brands fa-linkedin"></i>
          </a>
        </li>
        <li>
          <a href="https://github.com/voxel51/">
            <i class="fa-brands fa-github"></i>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/voxel51">
            <i class="fa-brands fa-twitter"></i>
          </a>
        </li>
        <li>
          <a href="https://www.facebook.com/voxel51/">
            <i class="fa-brands fa-facebook"></i>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer__copyright">
      <ul class="list-inline">
        <li>&copy; 2024 Voxel51 Inc.</li>
        <li>
          <a href="https://voxel51.com/privacy/">Privacy Policy</a>
        </li>
        <li>
          <a href="https://voxel51.com/terms/">Terms of Service</a>
        </li>
      </ul>
    </div>
  </div>
</footer>

<!-- https://buttons.github.io -->
<script async defer src="https://buttons.github.io/buttons.js" ></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XD15NFRY3M" ></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "G-XD15NFRY3M");
</script>

<script>
  function open_modal(modal_id, modal_closer_id) {
    // Get the modal
    let the_modal = document.getElementById(modal_id);

    the_modal.style.display = "flex";
    document.body.style.overflow = "hidden";

    let the_modal_closer = document.getElementById(modal_closer_id);

    // When the user clicks on <span> (x), close the modal
    the_modal_closer &&
      (the_modal_closer.onclick = function () {
        the_modal.style.display = "none";
      });

    // When the user clicks anywhere outside of the modal, close it
    window.onclick = function (event) {
      if (event.target == the_modal) {
        the_modal.style.display = "none";
        document.body.style.overflow = "unset";
        window.onclick = undefined;
      }
    };
  }
</script>

<!-- bigpicture.io -->
<script>
  !(function (e, t, i) {
    var r = (e.bigPicture = e.bigPicture || []);
    if (!r.initialized)
      if (r.invoked)
        e.console &&
          console.error &&
          console.error("BigPicture.io snippet included twice.");
      else {
        (r.invoked = !0),
          (r.SNIPPET_VERSION = 1.5),
          (r.handler = function (e) {
            if (void 0 !== r.callback)
              try {
                return r.callback(e);
              } catch (e) {}
          }),
          (r.eventList = ["mousedown", "mouseup", "click", "submit"]),
          (r.methods = [
            "track",
            "identify",
            "page",
            "group",
            "alias",
            "integration",
            "ready",
            "intelReady",
            "consentReady",
            "on",
            "off",
          ]),
          (r.factory = function (e) {
            return function () {
              var t = Array.prototype.slice.call(arguments);
              return t.unshift(e), r.push(t), r;
            };
          });
        for (var n = 0; n < r.methods.length; n++) {
          var o = r.methods[n];
          r[o] = r.factory(o);
        }
        r.getCookie = function (e) {
          var i = ("; " + t.cookie).split("; " + e + "=");
          return 2 == i.length && i.pop().split(";").shift();
        };
        var c = (r.isEditor = (function () {
          try {
            return (
              e.self !== e.top &&
              (new RegExp("app" + i, "ig").test(t.referrer) ||
                "edit" == r.getCookie("_bpr_edit"))
            );
          } catch (e) {
            return !1;
          }
        })());
        r.init = function (n, o) {
          if (((r.projectId = n), (r._config = o), !c))
            for (var a = 0; a < r.eventList.length; a++)
              e.addEventListener(r.eventList[a], r.handler, !0);
          var s = t.createElement("script");
          s.async = !0;
          var d = c ? "/editor/editor" : "/public-" + n;
          (s.src = "//cdn" + i + d + ".js"),
            t.getElementsByTagName("head")[0].appendChild(s);
        };
      }
  })(window, document, ".bigpicture.io");
  bigPicture.init("1646");
</script>


  

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>