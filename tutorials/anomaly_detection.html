


<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Anomaly Detection with FiftyOne and Anomalib &mdash; FiftyOne 1.3.0 documentation</title>
  

  
  
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="../_static/css/theme.css" type="text/css" />
  <!-- <link rel="stylesheet" href="../_static/pygments.css" type="text/css" /> -->
  <link rel="stylesheet" href="../_static/css/voxel51-website.css" type="text/css" />
  <link rel="stylesheet" href="../_static/css/custom.css" type="text/css" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="FiftyOne Recipes" href="../recipes/index.html" />
    <link rel="prev" title="Detecting Small Objects with SAHI" href="small_object_detection.html" />
<meta property="og:image" content="https://voxel51.com/wp-content/uploads/2024/03/3.24_webpages_Home_AV.png" />

<link
  href="https://fonts.googleapis.com/css?family=Palanquin:400,600,700,800"
  rel="stylesheet"
/>
<link
  href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css"
  rel="stylesheet"
/>
<script src="https://tag.clearbitscripts.com/v1/pk_b9ed71c8234edd4f77326bcbfab5a4ca/tags.js"></script>


  
  <script src="../_static/js/modernizr.min.js"></script>

  
</head>


<div class="container-fluid header-holder tutorials-header" id="header-holder">
  <nav id="nav__main" class="nav__main">
    <div class="nav__main__logo">
      <a href="https://voxel51.com/">
        <img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
        />
      </a>
    </div>

    <div class="nav__spacer desktop_only"></div>

    <div id="nav__main__mobilebutton--on">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-bars"></i>
      </a>
    </div>

    <div id="nav__main__mobilebutton--off">
      <a href="javascript:void(0);" onclick="navMobileButton()">
        <i class="fa-solid fa-times"></i>
      </a>
    </div>

    <div id="nav__main__items">
      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Products</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/fiftyone/">Open Source</a></li>
            <li><a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a></li>
            <li><a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a></li>
            <li><a href="https://voxel51.com/success-stories/">Success Stories</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Learn</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://community.voxel51.com">Community Discord</a></li>
            <li><a href="https://voxel51.com/blog/">Blog</a></li>
            <li><a href="https://voxel51.com/computer-vision-events/">Events</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item">
        <a href="https://docs.voxel51.com/">Docs</a>
      </div>

      <div class="nav__item nav__dropdown">
        <div class="nav__dropdown__trigger">Company</div>
        <div class="nav__item nav__dropdown__menu has-arrows">
          <div class="arrow-up--light-primary arrow-left20"></div>
          <ul>
            <li><a href="https://voxel51.com/ourstory/">About Us</a></li>
            <li><a href="https://voxel51.com/jobs/">Careers</a></li>
            <li><a href="https://voxel51.com/talk-to-sales/">Talk to Sales</a></li>
          </ul>
        </div>
      </div>

      <div class="nav__item mobile_only">
        <a href="https://github.com/voxel51/fiftyone">GitHub</a>
      </div>

      <div class="nav__item desktop_only" id="octocat">
        <!-- https://buttons.github.io -->
        <a
          class="github-button"
          href="https://github.com/voxel51/fiftyone"
          data-color-scheme="no-preference: dark_high_contrast; light: dark_high_contrast; dark: dark_high_contrast;"
          data-size="large"
          data-show-count="true"
          aria-label="Star voxel51/fiftyone on GitHub"
          >Star</a
        >
      </div>

      <div class="nav__item full_nav_only">
        <a class="button-primary" href="https://voxel51.com/schedule-teams-workshop/" target="_blank"
          >Schedule a workshop</a
        >
      </div>
    </div>
  </nav>
</div>



<body class="pytorch-body">

   

    

    <div class="table-of-contents-link-wrapper">
      <span>Table of Contents</span>
      <a href="#" class="toggle-table-of-contents" data-behavior="toggle-table-of-contents"></a>
    </div>

    <nav data-toggle="wy-nav-shift" class="pytorch-left-menu" id="pytorch-left-menu">
      <div class="pytorch-side-scroll">
        <div class="pytorch-menu pytorch-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          <div class="pytorch-left-menu-search">
            

            
              
              
            

            


  


<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../search.html" method="get">
    <input type="text" name="q" placeholder="Search Docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

            
          </div>

           <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../index.html">Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="../teams/index.html">FiftyOne Teams üöÄ</a></li>
<li class="toctree-l1"><a class="reference internal" href="../getting_started/install.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="../environments/index.html">Environments</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="index.html">Tutorials</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="pandas_comparison.html">pandas and FiftyOne</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluate_detections.html">Evaluating object detections</a></li>
<li class="toctree-l2"><a class="reference internal" href="evaluate_classifications.html">Evaluating a classifier</a></li>
<li class="toctree-l2"><a class="reference internal" href="image_embeddings.html">Using image embeddings</a></li>
<li class="toctree-l2"><a class="reference internal" href="cvat_annotation.html">Annotating with CVAT</a></li>
<li class="toctree-l2"><a class="reference internal" href="labelbox_annotation.html">Annotating with Labelbox</a></li>
<li class="toctree-l2"><a class="reference internal" href="open_images.html">Working with Open Images</a></li>
<li class="toctree-l2"><a class="reference internal" href="detectron2.html">Training with Detectron2</a></li>
<li class="toctree-l2"><a class="reference internal" href="uniqueness.html">Exploring image uniqueness</a></li>
<li class="toctree-l2"><a class="reference internal" href="classification_mistakes.html">Finding class mistakes</a></li>
<li class="toctree-l2"><a class="reference internal" href="detection_mistakes.html">Finding detection mistakes</a></li>
<li class="toctree-l2"><a class="reference internal" href="qdrant.html">Embeddings with Qdrant</a></li>
<li class="toctree-l2"><a class="reference internal" href="yolov8.html">Fine-tuning YOLOv8 models</a></li>
<li class="toctree-l2"><a class="reference internal" href="pointe.html">3D point clouds with Point-E</a></li>
<li class="toctree-l2"><a class="reference internal" href="monocular_depth_estimation.html">Monocular depth estimation</a></li>
<li class="toctree-l2"><a class="reference internal" href="dimension_reduction.html">Dimensionality reduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="zero_shot_classification.html">Zero-shot classification</a></li>
<li class="toctree-l2"><a class="reference internal" href="data_augmentation.html">Data augmentation</a></li>
<li class="toctree-l2"><a class="reference internal" href="clustering.html">Clustering images</a></li>
<li class="toctree-l2"><a class="reference internal" href="small_object_detection.html">Detecting small objects</a></li>
<li class="toctree-l2 current"><a class="current reference internal" href="#">Anomaly detection</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../recipes/index.html">Recipes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cheat_sheets/index.html">Cheat Sheets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../user_guide/index.html">User Guide</a></li>
<li class="toctree-l1"><a class="reference internal" href="../dataset_zoo/index.html">Dataset Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../model_zoo/index.html">Model Zoo</a></li>
<li class="toctree-l1"><a class="reference internal" href="../brain.html">FiftyOne Brain</a></li>
<li class="toctree-l1"><a class="reference internal" href="../integrations/index.html">Integrations</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plugins/index.html">Plugins</a></li>
<li class="toctree-l1"><a class="reference internal" href="../cli/index.html">CLI</a></li>
<li class="toctree-l1"><a class="reference internal" href="../api/fiftyone.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="../release-notes.html">Release Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../deprecation.html">Deprecation Notices</a></li>
<li class="toctree-l1"><a class="reference internal" href="../faq/index.html">FAQ</a></li>
</ul>
 
        </div>
      </div>
    </nav>

    <div class="pytorch-container">
      <div class="pytorch-page-level-bar" id="pytorch-page-level-bar">
        <div class="pytorch-breadcrumbs-wrapper">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="pytorch-breadcrumbs">
    
      <li>
        <a href="../index.html">
          
            Docs
          
        </a> &gt;
      </li>

        
          <li><a href="index.html">FiftyOne Tutorials</a> &gt;</li>
        
      <li>Anomaly Detection with FiftyOne and Anomalib</li>
    
    
      <li class="pytorch-breadcrumbs-aside">
        
            
        
      </li>
    
  </ul>

  
</div>
        </div>

        <div class="pytorch-shortcuts-wrapper" id="pytorch-shortcuts-wrapper">
          Contents
        </div>
      </div>

      <section data-toggle="wy-nav-shift" id="pytorch-content-wrap" class="pytorch-content-wrap">
        <div class="pytorch-content-left">

        
          
          <div class="rst-content style-external-links">
          
            <div role="main" class="main-content" itemscope="itemscope" itemtype="http://schema.org/Article">
             <article itemprop="articleBody" id="pytorch-article" class="pytorch-article">
              
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt .copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}
</style>
<table class="fo-notebook-links" align="left">
    <td>
        <a target="_blank" href="https://colab.research.google.com/github/voxel51/fiftyone/blob/v1.3.0/docs/source/tutorials/anomaly_detection.ipynb">
            <img src="../_static/images/icons/colab-logo-256px.png"> &nbsp; Run in Google Colab
        </a>
    </td>
    <td>
        <a target="_blank" href="https://github.com/voxel51/fiftyone/blob/v1.3.0/docs/source/tutorials/anomaly_detection.ipynb">
            <img src="../_static/images/icons/github-logo-256px.png"> &nbsp; View source on GitHub
        </a>
    </td>
    <td>
        <a target="_blank" href="https://raw.githubusercontent.com/voxel51/fiftyone/v1.3.0/docs/source/tutorials/anomaly_detection.ipynb" download>
            <img src="../_static/images/icons/cloud-icon-256px.png"> &nbsp; Download notebook
        </a>
    </td>
</table><div class="section" id="Anomaly-Detection-with-FiftyOne-and-Anomalib">
<h1>Anomaly Detection with FiftyOne and Anomalib<a class="headerlink" href="#Anomaly-Detection-with-FiftyOne-and-Anomalib" title="Permalink to this headline">¬∂</a></h1>
<p><img alt="Anomaly Detection Thumbnail" src="../_images/anomaly_detection_thumbnail.jpg" /></p>
<p>Anomaly detection (AD) is a crucial task in mission-critical applications such as fraud detection, network security, and medical diagnosis. Anomaly detection on <em>visual</em> data like images, videos, and satellite imagery, is a particularly challenging task due to the high dimensionality of the data and the complexity of the underlying patterns. Yet visual anomaly detection is essential for detecting defects in manufacturing, identifying suspicious activity in surveillance footage, and detecting
abnormalities in medical images.</p>
<p>In this walkthrough, you‚Äôll learn how to perform anomaly detection on visual data using <a class="reference external" href="https://fiftyone.ai/">FiftyOne</a> and <a class="reference external" href="https://github.com/openvinotoolkit/anomalib">Anomalib</a> from the OpenVINO‚Ñ¢ toolkit. We‚Äôll use the <a class="reference external" href="https://www.mvtec.com/company/research/datasets/mvtec-ad">MVTec AD dataset</a> for demonstration, which contains images of various objects with anomalies like scratches, dents, and holes.</p>
<p>The notebook covers the following:</p>
<ul class="simple">
<li><p>What is visual anomaly detection?</p></li>
<li><p>Loading the MVTec AD dataset in FiftyOne</p></li>
<li><p>Training an anomaly detection model with Anomalib</p></li>
<li><p>Evaluating anomaly detection models in FiftyOne</p></li>
</ul>
<div class="section" id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Permalink to this headline">¬∂</a></h2>
<div class="section" id="Install-dependencies">
<h3>Install dependencies<a class="headerlink" href="#Install-dependencies" title="Permalink to this headline">¬∂</a></h3>
<p>Make sure you are running this in a virtual environment with <code class="docutils literal notranslate"><span class="pre">python=3.10</span></code>.</p>
<div class="highlight-bash notranslate"><div class="highlight"><pre><span></span>conda<span class="w"> </span>create<span class="w"> </span>-n<span class="w"> </span>anomalib_env<span class="w"> </span><span class="nv">python</span><span class="o">=</span><span class="m">3</span>.10<span class="p">;</span><span class="w"> </span>conda<span class="w"> </span>activate<span class="w"> </span>anomalib_env
</pre></div>
</div>
<p>Anomalib requires Python 3.10, so make sure you have the correct version installed.</p>
<p>After this, install Anomalib and its dependencies. If you‚Äôre running this in a colab notebook, the installation might take a few minutes, but local installation should be faster.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>torchvision<span class="w"> </span>einops<span class="w"> </span>FrEIA<span class="w"> </span>timm<span class="w"> </span>open_clip_torch<span class="w"> </span>imgaug<span class="w"> </span>lightning<span class="w"> </span>kornia<span class="w"> </span>openvino<span class="w"> </span>git+https://github.com/openvinotoolkit/anomalib.git
</pre></div>
</div>
</div>
<p>Install Anomalib from source, per the instructions in the <a class="reference external" href="https://github.com/openvinotoolkit/anomalib?tab=readme-ov-file#-installation">Anomalib README</a></p>
<p>If you don‚Äôt have it already installed, install FiftyOne. Make sure your version is <code class="docutils literal notranslate"><span class="pre">fiftyone&gt;=0.23.8</span></code> so we can use the <a class="reference external" href="https://docs.voxel51.com/integrations/huggingface.html#huggingface-hub">Hugging Face Hub integration</a> to load the MVTec AD dataset:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>fiftyone
</pre></div>
</div>
</div>
<p>Just a few more packages to install, and we‚Äôre ready to go. Now you can see why we recommend using a virtual environment for this project!</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">huggingface_hub</span></code> for loading the MVTec AD dataset</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">clip</span></code> for computing image embeddings</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">umap-learn</span></code> for dimensionality reduction</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>pip<span class="w"> </span>install<span class="w"> </span>-U<span class="w"> </span>huggingface_hub<span class="w"> </span>umap-learn<span class="w"> </span>git+https://github.com/openai/CLIP.git
</pre></div>
</div>
</div>
</div>
<div class="section" id="Load-and-Visualize-the-MVTec-AD-dataset">
<h3>Load and Visualize the MVTec AD dataset<a class="headerlink" href="#Load-and-Visualize-the-MVTec-AD-dataset" title="Permalink to this headline">¬∂</a></h3>
<p>Now let‚Äôs import all of the relevant modules we will need from FiftyOne:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">fiftyone</span> <span class="k">as</span> <span class="nn">fo</span> <span class="c1"># base library and app</span>
<span class="kn">import</span> <span class="nn">fiftyone.brain</span> <span class="k">as</span> <span class="nn">fob</span> <span class="c1"># ML methods</span>
<span class="kn">import</span> <span class="nn">fiftyone.zoo</span> <span class="k">as</span> <span class="nn">foz</span> <span class="c1"># zoo datasets and models</span>
<span class="kn">from</span> <span class="nn">fiftyone</span> <span class="kn">import</span> <span class="n">ViewField</span> <span class="k">as</span> <span class="n">F</span> <span class="c1"># helper for defining views</span>
<span class="kn">import</span> <span class="nn">fiftyone.utils.huggingface</span> <span class="k">as</span> <span class="nn">fouh</span> <span class="c1"># Hugging Face integration</span>
</pre></div>
</div>
</div>
<p>And load the <a class="reference external" href="https://huggingface.co/datasets/Voxel51/mvtec-ad">MVTec AD dataset from the Hugging Face Hub</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="n">fouh</span><span class="o">.</span><span class="n">load_from_hub</span><span class="p">(</span><span class="s2">&quot;Voxel51/mvtec-ad&quot;</span><span class="p">,</span> <span class="n">persistent</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">overwrite</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>üí° It is also possible to load the MVTec AD data directly from Anomalib:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">anomalib.data</span> <span class="kn">import</span> <span class="n">MVTec</span>
<span class="n">datamodule</span> <span class="o">=</span> <span class="n">MVTec</span><span class="p">()</span>
</pre></div>
</div>
<p>But this way we have all of the metadata and annotations in FiftyOne, which is useful for visualization and evaluation.</p>
<p>Before moving on, let‚Äôs take a look at the dataset in the <a class="reference external" href="https://docs.voxel51.com/user_guide/app.html">FiftyOne App</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="MVTec AD Dataset" src="../_images/anomaly_detection_initial_dataset.gif" /></p>
<p>The dataset has 5354 images across 12 object categories. Each category has ‚Äúgood‚Äù images and ‚Äúanomalous‚Äù images with defects like scratches, dents, and holes. Each of the anomalous samples also has a mask which localizes the defective regions of the image.</p>
<p>The defect labels differ across categories, which is typical in real-world anomaly detection scenarios. In typical anomaly detection scenarios, you train a different model for each category. Here we‚Äôll go through the process for one category, and you can apply the same steps to other categories.</p>
<p>One more thing to note is that the dataset is split into training and test sets. The training set contains only ‚Äúgood‚Äù images, while the test set contains both ‚Äúgood‚Äù and ‚Äúanomalous‚Äù images.</p>
<p>Before we train a model, let‚Äôs dig into the dataset a bit more. We can get a feel for the structure and patterns hidden in our data by computing image embeddings and visualizing them in a lower-dimensional space. First, we‚Äôll compute embeddings for all the images in the dataset using the <a class="reference external" href="https://github.com/openai/CLIP">CLIP model</a>:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;clip-vit-base32-torch&quot;</span>
<span class="p">)</span>  <span class="c1"># load the CLIP model from the zoo</span>

<span class="c1"># Compute embeddings for the dataset</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">compute_embeddings</span><span class="p">(</span>
    <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">embeddings_field</span><span class="o">=</span><span class="s2">&quot;clip_embeddings&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span>
<span class="p">)</span>

<span class="c1"># Dimensionality reduction using UMAP on the embeddings</span>
<span class="n">fob</span><span class="o">.</span><span class="n">compute_visualization</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="s2">&quot;clip_embeddings&quot;</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="s2">&quot;umap&quot;</span><span class="p">,</span> <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;clip_vis&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p>Refresh the FiftyOne App, click the ‚Äú+‚Äù tab, and select ‚ÄúEmbeddings‚Äù. Choose ‚Äúall_clip_vis‚Äù from the dropdown menu. You‚Äôll see a scatter plot of the image embeddings in a 2D space, where each point corresponds to a sample in the dataset. Using the color-by dropdown, notice how the embeddings cluster based on the object category. This is because CLIP encodes semantic information about the images. Also note that <em>within</em> a category, CLIP embeddings don‚Äôt cluster based on the defect type.</p>
<p><img alt="CLIP Embeddings" src="../_images/anomaly_detection_clip_embeddings.gif" /></p>
<p>If instead we embed our images using a traditional computer vision model like ResNet, we also see some clustering within a category based on the defect type. However, as we established earlier, we will not have access to defect labels during inference. Instead, we‚Äôll use an unsupervised anomaly detection model.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">foz</span><span class="o">.</span><span class="n">load_zoo_model</span><span class="p">(</span>
    <span class="s2">&quot;resnet50-imagenet-torch&quot;</span>
<span class="p">)</span>  <span class="c1"># load the ResNet50 model from the zoo</span>

<span class="c1"># Compute embeddings for the dataset ‚Äî this might take a while on a CPU</span>
<span class="n">dataset</span><span class="o">.</span><span class="n">compute_embeddings</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">embeddings_field</span><span class="o">=</span><span class="s2">&quot;resnet50_embeddings&quot;</span><span class="p">)</span>

<span class="c1"># Dimensionality reduction using UMAP on the embeddings</span>
<span class="n">fob</span><span class="o">.</span><span class="n">compute_visualization</span><span class="p">(</span>
    <span class="n">dataset</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="o">=</span><span class="s2">&quot;resnet50_embeddings&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;umap&quot;</span><span class="p">,</span>
    <span class="n">brain_key</span><span class="o">=</span><span class="s2">&quot;resnet50_vis&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="ResNet50 Embeddings" src="../_images/anomaly_detection_resnet50_embeddings.gif" /></p>
<p>üí° For deep dives into embeddings and dimensionality reduction, check out our tutorials:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.voxel51.com/tutorials/image_embeddings.html">Using Image Embeddings</a></p></li>
<li><p><a class="reference external" href="https://docs.voxel51.com/tutorials/dimension_reduction.html">Visualizing Data with Dimensionality Reduction Techniques</a></p></li>
</ul>
</div>
</div>
<div class="section" id="Train-an-Anomaly-Detection-Model">
<h2>Train an Anomaly Detection Model<a class="headerlink" href="#Train-an-Anomaly-Detection-Model" title="Permalink to this headline">¬∂</a></h2>
<p>Now that we have a sense of the dataset, we‚Äôre ready to train an anomaly detection model using Anomalib.</p>
<p><strong>Task</strong>: Anomalib supports classification, detection, and segmentation tasks for images. We‚Äôll focus on segmentation, where the model predicts whether each pixel in the image is anomalous or not, creating a mask that localizes the defect.</p>
<p><strong>Model</strong>: Anomalib supports a variety of anomaly detection algorithms, including <a class="reference external" href="https://anomalib.readthedocs.io/en/v1.0.1/markdown/guides/reference/models/image/dfkde.html">Deep Feature Kernel Density Estimation (DFKDE)</a>, <a class="reference external" href="https://arxiv.org/abs/2111.07677">FastFlow</a>, and <a class="reference external" href="https://arxiv.org/abs/2201.10703v2">Reverse Distillation</a>. For a complete list of supported algorithms, check out <a class="reference external" href="https://anomalib.readthedocs.io/en/v1.0.1/markdown/guides/reference/models/image/index.html">Anomalib‚Äôs reference
guide</a>. For this walkthrough, we‚Äôll use two algorithms:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/2011.08785">PaDiM: a Patch Distribution Modeling Framework for Anomaly Detection and Localization</a></p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2106.08265">PatchCore: Towards Total Recall in Industrial Anomaly Detection</a></p></li>
</ul>
<p><strong>Preprocessing</strong>: For this walkthrough, we will resize the images to 256x256 pixels before training the model. Adding this as a transform via Torchvision‚Äôs <code class="docutils literal notranslate"><span class="pre">Resize</span></code> class lets us resize the images on-the-fly during training and inference.</p>
<p>Import the necessary modules from Anomalib and helper modules for processing images and paths:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">from</span> <span class="nn">pathlib</span> <span class="kn">import</span> <span class="n">Path</span>
<span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
<span class="kn">from</span> <span class="nn">torchvision.transforms.v2</span> <span class="kn">import</span> <span class="n">Resize</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">anomalib</span> <span class="kn">import</span> <span class="n">TaskType</span>
<span class="kn">from</span> <span class="nn">anomalib.data.image.folder</span> <span class="kn">import</span> <span class="n">Folder</span>
<span class="kn">from</span> <span class="nn">anomalib.deploy</span> <span class="kn">import</span> <span class="n">ExportType</span><span class="p">,</span> <span class="n">OpenVINOInferencer</span>
<span class="kn">from</span> <span class="nn">anomalib.engine</span> <span class="kn">import</span> <span class="n">Engine</span>
<span class="kn">from</span> <span class="nn">anomalib.models</span> <span class="kn">import</span> <span class="n">Padim</span><span class="p">,</span> <span class="n">Patchcore</span>
</pre></div>
</div>
</div>
<p>Now define some constants to use throughout the notebook.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">OBJECT</span></code>: The object category we‚Äôll focus on. For this walkthrough, we‚Äôll use ‚Äúbottle‚Äù. If you want to loop over categories, you can get the list of categories from the dataset with <code class="docutils literal notranslate"><span class="pre">dataset.distinct(&quot;category.label&quot;)</span></code>.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ROOT_DIR</span></code>: The root directory where Anomalib will look for images and masks. Our data is already stored on disk, so we will just symlink files to the directory Anomalib expects.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">TASK</span></code>: The task we‚Äôre performing. We‚Äôll use ‚Äúsegmentation‚Äù for this walkthrough.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">IMAGE_SIZE</span></code>: The size to resize images to before training the model. We‚Äôll use <span class="math notranslate nohighlight">\(256\)</span> x <span class="math notranslate nohighlight">\(256\)</span> pixels.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">OBJECT</span> <span class="o">=</span> <span class="s2">&quot;bottle&quot;</span> <span class="c1">## object to train on</span>
<span class="n">ROOT_DIR</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="s2">&quot;/tmp/mvtec_ad&quot;</span><span class="p">)</span> <span class="c1">## root directory to store data for anomalib</span>
<span class="n">TASK</span> <span class="o">=</span> <span class="n">TaskType</span><span class="o">.</span><span class="n">SEGMENTATION</span> <span class="c1">## task type for the model</span>
<span class="n">IMAGE_SIZE</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">)</span> <span class="c1">## preprocess image size for uniformity</span>
</pre></div>
</div>
</div>
<p>For a given object type (category), the <code class="docutils literal notranslate"><span class="pre">create_datamodule()</span></code> function below creates an Anomalib <code class="docutils literal notranslate"><span class="pre">DataModule</span></code> object. This will get passed into our engine‚Äôs <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method to train the model, and will be used to instantiate dataloaders for training and validation.</p>
<p>The code might look complex, so let‚Äôs break down what‚Äôs going on:</p>
<ul class="simple">
<li><p>We create subsets of our data containing only the ‚Äúgood‚Äù training images and ‚Äúanomalous‚Äù images for validation.</p></li>
<li><p>We symlink the images and masks to the directory Anomalib expects.</p></li>
<li><p>We instantiate and setup a datamodule from Anomalib‚Äôs <code class="docutils literal notranslate"><span class="pre">Folder</span></code>, which is the general-purpose class for custom datasets.</p></li>
</ul>
<p>üí° It is also possible to create a torch <code class="docutils literal notranslate"><span class="pre">DataLoader</span></code> from scratch and pass it to the engine‚Äôs <code class="docutils literal notranslate"><span class="pre">fit()</span></code> method. This gives you more control over the data loading process. This is left as an exercise for the reader üòâ.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_datamodule</span><span class="p">(</span><span class="n">object_type</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="c1">## Build transform</span>
    <span class="k">if</span> <span class="n">transform</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">transform</span> <span class="o">=</span> <span class="n">Resize</span><span class="p">(</span><span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">normal_data</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">&quot;category.label&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">object_type</span><span class="p">)</span><span class="o">.</span><span class="n">match</span><span class="p">(</span>
        <span class="n">F</span><span class="p">(</span><span class="s2">&quot;split&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;train&quot;</span>
    <span class="p">)</span>
    <span class="n">abnormal_data</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">dataset</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">&quot;category.label&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">object_type</span><span class="p">)</span>
        <span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">&quot;split&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span><span class="p">)</span>
        <span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">&quot;defect.label&quot;</span><span class="p">)</span> <span class="o">!=</span> <span class="s2">&quot;good&quot;</span><span class="p">)</span>
    <span class="p">)</span>

    <span class="n">normal_dir</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">ROOT_DIR</span><span class="p">)</span> <span class="o">/</span> <span class="n">object_type</span> <span class="o">/</span> <span class="s2">&quot;normal&quot;</span>
    <span class="n">abnormal_dir</span> <span class="o">=</span> <span class="n">ROOT_DIR</span> <span class="o">/</span> <span class="n">object_type</span> <span class="o">/</span> <span class="s2">&quot;abnormal&quot;</span>
    <span class="n">mask_dir</span> <span class="o">=</span> <span class="n">ROOT_DIR</span> <span class="o">/</span> <span class="n">object_type</span> <span class="o">/</span> <span class="s2">&quot;mask&quot;</span>

    <span class="c1"># create directories if they do not exist</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">normal_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">abnormal_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">mask_dir</span><span class="p">,</span> <span class="n">exist_ok</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">normal_dir</span><span class="p">)):</span>
        <span class="n">normal_data</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
            <span class="n">export_dir</span><span class="o">=</span><span class="nb">str</span><span class="p">(</span><span class="n">normal_dir</span><span class="p">),</span>
            <span class="n">dataset_type</span><span class="o">=</span><span class="n">fo</span><span class="o">.</span><span class="n">types</span><span class="o">.</span><span class="n">ImageDirectory</span><span class="p">,</span>
            <span class="n">export_media</span><span class="o">=</span><span class="s2">&quot;symlink&quot;</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">abnormal_data</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">():</span>
        <span class="n">base_filename</span> <span class="o">=</span> <span class="n">sample</span><span class="o">.</span><span class="n">filename</span>
        <span class="n">dir_name</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">dirname</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">new_filename</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">dir_name</span><span class="si">}</span><span class="s2">_</span><span class="si">{</span><span class="n">base_filename</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">abnormal_dir</span> <span class="o">/</span> <span class="n">new_filename</span><span class="p">)):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">symlink</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">abnormal_dir</span> <span class="o">/</span> <span class="n">new_filename</span><span class="p">))</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">mask_dir</span> <span class="o">/</span> <span class="n">new_filename</span><span class="p">)):</span>
            <span class="n">os</span><span class="o">.</span><span class="n">symlink</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">defect_mask</span><span class="o">.</span><span class="n">mask_path</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="n">mask_dir</span> <span class="o">/</span> <span class="n">new_filename</span><span class="p">))</span>

    <span class="n">datamodule</span> <span class="o">=</span> <span class="n">Folder</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">object_type</span><span class="p">,</span>
        <span class="n">root</span><span class="o">=</span><span class="n">ROOT_DIR</span><span class="p">,</span>
        <span class="n">normal_dir</span><span class="o">=</span><span class="n">normal_dir</span><span class="p">,</span>
        <span class="n">abnormal_dir</span><span class="o">=</span><span class="n">abnormal_dir</span><span class="p">,</span>
        <span class="n">mask_dir</span><span class="o">=</span><span class="n">mask_dir</span><span class="p">,</span>
        <span class="n">task</span><span class="o">=</span><span class="n">TASK</span><span class="p">,</span>
        <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
    <span class="p">)</span>
    <span class="n">datamodule</span><span class="o">.</span><span class="n">setup</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">datamodule</span>
</pre></div>
</div>
</div>
<p>Now we can put it all together. The <code class="docutils literal notranslate"><span class="pre">train_and_export_model()</span></code> function below trains an anomaly detection model using Anomalib‚Äôs <code class="docutils literal notranslate"><span class="pre">Engine</span></code> class, exports the model to OpenVINO, and returns the model ‚Äúinferencer‚Äù object. The inferencer object is used to make predictions on new images.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">train_and_export_model</span><span class="p">(</span><span class="n">object_type</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">engine</span> <span class="o">=</span> <span class="n">Engine</span><span class="p">(</span><span class="n">task</span><span class="o">=</span><span class="n">TASK</span><span class="p">)</span>
    <span class="n">datamodule</span> <span class="o">=</span> <span class="n">create_datamodule</span><span class="p">(</span><span class="n">object_type</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span><span class="p">)</span>
    <span class="n">engine</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">datamodule</span><span class="p">)</span>

    <span class="n">engine</span><span class="o">.</span><span class="n">export</span><span class="p">(</span>
        <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">,</span>
        <span class="n">export_type</span><span class="o">=</span><span class="n">ExportType</span><span class="o">.</span><span class="n">OPENVINO</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">output_path</span> <span class="o">=</span> <span class="n">Path</span><span class="p">(</span><span class="n">engine</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">default_root_dir</span><span class="p">)</span>


    <span class="n">openvino_model_path</span> <span class="o">=</span> <span class="n">output_path</span> <span class="o">/</span> <span class="s2">&quot;weights&quot;</span> <span class="o">/</span> <span class="s2">&quot;openvino&quot;</span> <span class="o">/</span> <span class="s2">&quot;model.bin&quot;</span>
    <span class="n">metadata</span> <span class="o">=</span> <span class="n">output_path</span> <span class="o">/</span> <span class="s2">&quot;weights&quot;</span> <span class="o">/</span> <span class="s2">&quot;openvino&quot;</span> <span class="o">/</span> <span class="s2">&quot;metadata.json&quot;</span>

    <span class="n">inferencer</span> <span class="o">=</span> <span class="n">OpenVINOInferencer</span><span class="p">(</span>
        <span class="n">path</span><span class="o">=</span><span class="n">openvino_model_path</span><span class="p">,</span>
        <span class="n">metadata</span><span class="o">=</span><span class="n">metadata</span><span class="p">,</span>
        <span class="n">device</span><span class="o">=</span><span class="s2">&quot;CPU&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">inferencer</span>
</pre></div>
</div>
</div>
<p>Let‚Äôs try this with <code class="docutils literal notranslate"><span class="pre">PaDiM</span></code> first. The training process should take less than a minute:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Padim</span><span class="p">()</span>

<span class="n">inferencer</span> <span class="o">=</span> <span class="n">train_and_export_model</span><span class="p">(</span><span class="n">OBJECT</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>And just like that, we have an anomaly detection model trained on the ‚Äúbottle‚Äù category. Let‚Äôs run our inferencer on a single image and inspect the results:</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## get the test split of the dataset</span>
<span class="n">test_split</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">match</span><span class="p">(</span><span class="n">F</span><span class="p">(</span><span class="s2">&quot;category.label&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="n">OBJECT</span><span class="p">)</span><span class="o">.</span><span class="n">match</span><span class="p">(</span>
    <span class="n">F</span><span class="p">(</span><span class="s2">&quot;split&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;test&quot;</span>
<span class="p">)</span>

<span class="c1">## get the first sample from the test split</span>
<span class="n">test_image</span> <span class="o">=</span> <span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">test_split</span><span class="o">.</span><span class="n">first</span><span class="p">()</span><span class="o">.</span><span class="n">filepath</span><span class="p">)</span>

<span class="n">output</span> <span class="o">=</span> <span class="n">inferencer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">test_image</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
ImageResult(image=[[[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 ...

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]], pred_score=0.7751642969087686, pred_label=1, anomaly_map=[[0.32784402 0.32784402 0.32784414 ... 0.3314721  0.33147204 0.33147204]
 [0.32784402 0.32784402 0.32784414 ... 0.3314721  0.33147204 0.33147204]
 [0.32784408 0.32784408 0.3278442  ... 0.33147222 0.33147216 0.33147216]
 ...
 [0.32959    0.32959    0.32959005 ... 0.3336093  0.3336093  0.3336093 ]
 [0.3295899  0.3295899  0.32958996 ... 0.33360928 0.33360928 0.33360928]
 [0.3295899  0.3295899  0.32958996 ... 0.33360928 0.33360928 0.33360928]], gt_mask=None, gt_boxes=None, pred_boxes=None, box_labels=None, pred_mask=[[0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 ...
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]
 [0 0 0 ... 0 0 0]], heat_map=[[[153 235 255]
  [153 235 255]
  [153 235 255]
  ...
  [153 236 255]
  [153 236 255]
  [153 236 255]]

 [[153 235 255]
  [153 235 255]
  [153 235 255]
  ...
  [153 236 255]
  [153 236 255]
  [153 236 255]]

 [[153 235 255]
  [153 235 255]
  [153 235 255]
  ...
  [153 236 255]
  [153 236 255]
  [153 236 255]]

 ...

 [[153 236 255]
  [153 236 255]
  [153 236 255]
  ...
  [153 238 255]
  [153 238 255]
  [153 238 255]]

 [[153 236 255]
  [153 236 255]
  [153 236 255]
  ...
  [153 238 255]
  [153 238 255]
  [153 238 255]]

 [[153 236 255]
  [153 236 255]
  [153 236 255]
  ...
  [153 238 255]
  [153 238 255]
  [153 238 255]]], segmentations=[[[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 ...

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]

 [[255 255 255]
  [255 255 255]
  [255 255 255]
  ...
  [255 255 255]
  [255 255 255]
  [255 255 255]]])
</pre></div></div>
</div>
<p>The output contains a scalar anomaly score <code class="docutils literal notranslate"><span class="pre">pred_score</span></code>, a <code class="docutils literal notranslate"><span class="pre">pred_mask</span></code> denoting the predicted anomalous regions, and a heatmap <code class="docutils literal notranslate"><span class="pre">anomaly_map</span></code> showing the anomaly scores for each pixel. This is all valuable information for understanding the model‚Äôs predictions. The <code class="docutils literal notranslate"><span class="pre">run_inference()</span></code> function below will take a FiftyOne sample collection (e.g.¬†our test set) as input, along with the inferencer object, and a key for storing the results in the samples. It will run the model on each sample in
the collection and store the results. The <code class="docutils literal notranslate"><span class="pre">threshold</span></code> argument acts as a cutoff for the anomaly score. If the score is above the threshold, the sample is considered anomalous. In this example, we‚Äôll use a threshold of <span class="math notranslate nohighlight">\(0.5\)</span>, but you can experiment with different values.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_inference</span><span class="p">(</span><span class="n">sample_collection</span><span class="p">,</span> <span class="n">inferencer</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">threshold</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">sample_collection</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">autosave</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
        <span class="n">output</span> <span class="o">=</span> <span class="n">inferencer</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">image</span><span class="o">=</span><span class="n">Image</span><span class="o">.</span><span class="n">open</span><span class="p">(</span><span class="n">sample</span><span class="o">.</span><span class="n">filepath</span><span class="p">))</span>

        <span class="n">conf</span> <span class="o">=</span> <span class="n">output</span><span class="o">.</span><span class="n">pred_score</span>
        <span class="n">anomaly</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span> <span class="k">if</span> <span class="n">conf</span> <span class="o">&lt;</span> <span class="n">threshold</span> <span class="k">else</span> <span class="s2">&quot;anomaly&quot;</span>

        <span class="n">sample</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pred_anomaly_score_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">conf</span>
        <span class="n">sample</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pred_anomaly_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Classification</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="n">anomaly</span><span class="p">)</span>
        <span class="n">sample</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pred_anomaly_map_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Heatmap</span><span class="p">(</span><span class="nb">map</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">anomaly_map</span><span class="p">)</span>
        <span class="n">sample</span><span class="p">[</span><span class="sa">f</span><span class="s2">&quot;pred_defect_mask_</span><span class="si">{</span><span class="n">key</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Segmentation</span><span class="p">(</span><span class="n">mask</span><span class="o">=</span><span class="n">output</span><span class="o">.</span><span class="n">pred_mask</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">run_inference</span><span class="p">(</span><span class="n">test_split</span><span class="p">,</span> <span class="n">inferencer</span><span class="p">,</span> <span class="s2">&quot;padim&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Let‚Äôs visualize these results in the FiftyOne App:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">launch_app</span><span class="p">(</span><span class="n">view</span><span class="o">=</span><span class="n">test_split</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p><img alt="Anomaly Detection PaDiM Results" src="../_images/anomaly_detection_padim.gif" /></p>
</div>
<div class="section" id="Evaluate-Anomaly-Detection-Models">
<h2>Evaluate Anomaly Detection Models<a class="headerlink" href="#Evaluate-Anomaly-Detection-Models" title="Permalink to this headline">¬∂</a></h2>
<p>We have an anomaly detection model, but how do we know if it‚Äôs any good? For one, we can evaluate the model using metrics like precision, recall, and F1 score. FiftyOne‚Äôs <a class="reference external" href="https://docs.voxel51.com/user_guide/evaluation.html#">Evaluation API</a> makes this easy. We are going to evaluate the full-image classification performance of the model, as well as the segmentation performance.</p>
<p>We need to prepare our data for evaluation. First, we need to add null masks for the ‚Äúnormal‚Äù images to ensure the evaluation is fair:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="n">test_split</span><span class="o">.</span><span class="n">iter_samples</span><span class="p">(</span><span class="n">autosave</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">progress</span><span class="o">=</span><span class="kc">True</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;defect&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">label</span> <span class="o">==</span> <span class="s2">&quot;good&quot;</span><span class="p">:</span>
        <span class="n">sample</span><span class="p">[</span><span class="s2">&quot;defect_mask&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fo</span><span class="o">.</span><span class="n">Segmentation</span><span class="p">(</span>
            <span class="n">mask</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s2">&quot;pred_defect_mask_padim&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mask</span><span class="p">)</span>
        <span class="p">)</span>
</pre></div>
</div>
</div>
<p>We also need to ensure consistency in naming/labels between ground truth and predictions. We‚Äôll rename all of our ‚Äúgood‚Äù images to ‚Äúnormal‚Äù and every type of anomaly to ‚Äúanomaly‚Äù:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">old_labels</span> <span class="o">=</span> <span class="n">test_split</span><span class="o">.</span><span class="n">distinct</span><span class="p">(</span><span class="s2">&quot;defect.label&quot;</span><span class="p">)</span>
<span class="n">label_map</span> <span class="o">=</span> <span class="p">{</span><span class="n">label</span><span class="p">:</span><span class="s2">&quot;anomaly&quot;</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">old_labels</span> <span class="k">if</span> <span class="n">label</span> <span class="o">!=</span> <span class="s2">&quot;good&quot;</span><span class="p">}</span>
<span class="n">label_map</span><span class="p">[</span><span class="s2">&quot;good&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;normal&quot;</span>
<span class="n">mapped_view</span> <span class="o">=</span> <span class="n">test_split</span><span class="o">.</span><span class="n">map_labels</span><span class="p">(</span><span class="s2">&quot;defect&quot;</span><span class="p">,</span> <span class="n">label_map</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><img alt="Mapped View" src="../_images/anomaly_detection_mapped_labels.jpg" /></p>
<p>For classification, we‚Äôll use binary evaluation, with ‚Äúnormal‚Äù as the negative class and ‚Äúanomaly‚Äù as the positive class:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[41]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_classif_padim</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">evaluate_classifications</span><span class="p">(</span>
    <span class="s2">&quot;pred_anomaly_padim&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;defect&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;eval_classif_padim&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;anomaly&quot;</span><span class="p">],</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[42]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_classif_padim</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      normal       0.95      0.90      0.92        20
     anomaly       0.97      0.98      0.98        63

    accuracy                           0.96        83
   macro avg       0.96      0.94      0.95        83
weighted avg       0.96      0.96      0.96        83

</pre></div></div>
</div>
<p>The model performs quite well on the classification task!</p>
<p>If we go back over to the app and sort by anomaly score, we can see that certain types of anomalies tend to have higher scores than others. In this example, <code class="docutils literal notranslate"><span class="pre">contamination</span></code> instances tend to have either very high or very low scores relative to <code class="docutils literal notranslate"><span class="pre">broken_small</span></code> and <code class="docutils literal notranslate"><span class="pre">broken_large</span></code>. When we put this model in production we might be more likely to miss certain types of anomalies. Other types of models, or ensembles of models, might be more robust to this!</p>
<p>For segmentation evaluation, we will only be interested in pixel values of <span class="math notranslate nohighlight">\(0\)</span> (normal) and <span class="math notranslate nohighlight">\(255\)</span> (anomaly), so we will filter our report for these ‚Äúclasses‚Äù:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_seg_padim</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">evaluate_segmentations</span><span class="p">(</span>
    <span class="s2">&quot;pred_defect_mask_padim&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;defect_mask&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;eval_seg_padim&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[44]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_seg_padim</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

           0       0.99      0.96      0.98 63343269.0
         255       0.60      0.89      0.72 3886731.0

   micro avg       0.96      0.96      0.96 67230000.0
   macro avg       0.80      0.93      0.85 67230000.0
weighted avg       0.97      0.96      0.96 67230000.0

</pre></div></div>
</div>
<div class="section" id="Compare-Anomaly-Detection-Models">
<h3>Compare Anomaly Detection Models<a class="headerlink" href="#Compare-Anomaly-Detection-Models" title="Permalink to this headline">¬∂</a></h3>
<p>Just because anomaly detection is unsupervised doesn‚Äôt mean we can‚Äôt compare models and choose the best one for our use case. We can train multiple models on the same data and compare their performance using metrics like F1 score, precision, and recall. We can also compare the models visually by inspecting the masks and heatmaps they generate.</p>
<p>Let‚Äôs repeat the training process for the <code class="docutils literal notranslate"><span class="pre">PatchCore</span></code> model and compare the two models:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Train Patchcore model and run inference</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">Patchcore</span><span class="p">()</span>

<span class="c1">## This will take a little longer to train, but should still be &lt; 5 minutes</span>
<span class="n">inferencer</span> <span class="o">=</span> <span class="n">train_and_export_model</span><span class="p">(</span><span class="n">OBJECT</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span>

<span class="n">run_inference</span><span class="p">(</span><span class="n">mapped_view</span><span class="p">,</span> <span class="n">inferencer</span><span class="p">,</span> <span class="s2">&quot;patchcore&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[46]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Evaluate Patchcore model on classification task</span>
<span class="n">eval_classif_patchcore</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">evaluate_classifications</span><span class="p">(</span>
    <span class="s2">&quot;pred_anomaly_patchcore&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;defect&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;eval_classif_patchcore&quot;</span><span class="p">,</span>
    <span class="n">method</span><span class="o">=</span><span class="s2">&quot;binary&quot;</span><span class="p">,</span>
    <span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;normal&quot;</span><span class="p">,</span> <span class="s2">&quot;anomaly&quot;</span><span class="p">],</span>
<span class="p">)</span>

<span class="n">eval_classif_patchcore</span><span class="o">.</span><span class="n">print_report</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

      normal       0.95      1.00      0.98        20
     anomaly       1.00      0.98      0.99        63

    accuracy                           0.99        83
   macro avg       0.98      0.99      0.98        83
weighted avg       0.99      0.99      0.99        83

</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_seg_patchcore</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">match</span><span class="p">(</span>
    <span class="n">F</span><span class="p">(</span><span class="s2">&quot;defect.label&quot;</span><span class="p">)</span> <span class="o">==</span> <span class="s2">&quot;anomaly&quot;</span>
<span class="p">)</span><span class="o">.</span><span class="n">evaluate_segmentations</span><span class="p">(</span>
    <span class="s2">&quot;pred_defect_mask_patchcore&quot;</span><span class="p">,</span>
    <span class="n">gt_field</span><span class="o">=</span><span class="s2">&quot;defect_mask&quot;</span><span class="p">,</span>
    <span class="n">eval_key</span><span class="o">=</span><span class="s2">&quot;eval_seg_patchcore&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[48]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">eval_seg_patchcore</span><span class="o">.</span><span class="n">print_report</span><span class="p">(</span><span class="n">classes</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">255</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
              precision    recall  f1-score   support

           0       0.99      0.95      0.97 47143269.0
         255       0.60      0.85      0.70 3886731.0

   micro avg       0.95      0.95      0.95 51030000.0
   macro avg       0.80      0.90      0.84 51030000.0
weighted avg       0.96      0.95      0.95 51030000.0

</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">shuffle</span><span class="p">()</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p><img alt="Compare Models" src="../_images/anomaly_detection_compare_models.gif" /></p>
<p>The metrics back up what we see in the app: PatchCore has much higher recall for the ‚Äúanomaly‚Äù class, but lower precision. This means it‚Äôs more likely to catch anomalies, but also more likely to make false positive predictions. After all, PatchCore is designed for ‚Äútotal recall‚Äù in industrial anomaly detection.</p>
<p>Looking at the heatmaps, we can also see what types of anomalies each model is better at detecting. An ensemble of the two models might be more robust to different types of anomalies.</p>
</div>
</div>
<div class="section" id="Test-Data-Augmentation-Techniques">
<h2>Test Data Augmentation Techniques<a class="headerlink" href="#Test-Data-Augmentation-Techniques" title="Permalink to this headline">¬∂</a></h2>
<p>Beyond the anomaly detection algorithm itself, there are many other knobs we can turn to improve the performance of our model. These include:</p>
<ul class="simple">
<li><p>Backbone: The architecture of the model used for feature extraction</p></li>
<li><p>Algorithm hyperparameters: Parameters specific to the anomaly detection algorithm. For PatchCore, this includes <code class="docutils literal notranslate"><span class="pre">coreset_sampling_ratio</span></code> and <code class="docutils literal notranslate"><span class="pre">num_neighbors</span></code>.</p></li>
<li><p>Data augmentation: Techniques to artificially increase the size of the training set and improve the model‚Äôs generalization.</p></li>
</ul>
<p>This section briefly illustrates the role of data augmentation techniques in anomaly detection. We‚Äôll use <a class="reference external" href="https://github.com/jacobmarks/fiftyone-albumentations-plugin">FiftyOne‚Äôs Albumentations plugin</a> to test and visualize transformations on ‚Äúgood‚Äù samples from the dataset and then apply the same transformations (with <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code>) to the training images.</p>
<p>The goal is to increase the diversity of the training set without changing the images so much that we verge into the ‚Äúanomalous‚Äù territory.</p>
<p>First, let‚Äôs install <a class="reference external" href="https://albumentations.ai/docs/">Albumentations</a> and download the plugin:</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">pip</span> <span class="n">install</span> <span class="o">-</span><span class="n">U</span> <span class="n">albumentations</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">!</span>fiftyone<span class="w"> </span>plugins<span class="w"> </span>download<span class="w"> </span>https://github.com/jacobmarks/fiftyone-albumentations-plugin
</pre></div>
</div>
</div>
<p>Refresh the FiftyOne App and hit the backtick key on your keyboard to open the operators list. Select <code class="docutils literal notranslate"><span class="pre">&quot;augment_with_albumentations&quot;</span></code> from the dropdown menu. Try applying transformations like <code class="docutils literal notranslate"><span class="pre">GaussianBlur</span></code>, and <code class="docutils literal notranslate"><span class="pre">ColorJitter</span></code>, and see how they affect the images. Use the <code class="docutils literal notranslate"><span class="pre">&quot;view_last_albumentations_run&quot;</span></code> operator to see the augmentations generated by the last run. Play around with the kernel size for blurring, and the brightness, contrast, and saturation values for color jittering.
Depending on the object category, it might also make sense to apply 90 degree rotations, horizontal flips, and other transformations!</p>
<p><img alt="Data Augmentation" src="../_images/anomaly_detection_augment.gif" /></p>
<p>Once you‚Äôre happy with the transformations, use the <code class="docutils literal notranslate"><span class="pre">&quot;get_last_albumentations_run_info&quot;</span></code> operator to see the transformations applied and their parameters. You can then use these with <code class="docutils literal notranslate"><span class="pre">torchvision.transforms</span></code> to augment the training images.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[49]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torchvision.transforms.v2</span> <span class="kn">import</span> <span class="n">GaussianBlur</span><span class="p">,</span> <span class="n">ColorJitter</span><span class="p">,</span> <span class="n">Compose</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[50]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">transform</span> <span class="o">=</span> <span class="n">Compose</span><span class="p">([</span>
    <span class="n">Resize</span><span class="p">(</span><span class="n">IMAGE_SIZE</span><span class="p">,</span> <span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">),</span>
    <span class="n">GaussianBlur</span><span class="p">(</span><span class="n">kernel_size</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">ColorJitter</span><span class="p">(</span><span class="n">brightness</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">contrast</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">saturation</span><span class="o">=</span><span class="mf">0.2</span><span class="p">),</span>
<span class="p">])</span>
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">Patchcore</span><span class="p">()</span>
<span class="n">augmented_inferencer</span> <span class="o">=</span> <span class="n">train_and_export_model</span><span class="p">(</span>
    <span class="n">OBJECT</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transform</span>
<span class="p">)</span>

<span class="n">run_inference</span><span class="p">(</span><span class="n">mapped_view</span><span class="p">,</span> <span class="n">augmented_inferencer</span><span class="p">,</span> <span class="s2">&quot;patchcore_augmented&quot;</span><span class="p">)</span>

<span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>
</pre></div>
</div>
</div>
<p>This is just a starting point for data augmentation. You can experiment with different transformations and parameters to see what works best for your dataset.</p>
</div>
<div class="section" id="Summary">
<h2>Summary<a class="headerlink" href="#Summary" title="Permalink to this headline">¬∂</a></h2>
<p>In this walkthrough, we learned how to perform anomaly detection on visual data using FiftyOne and Anomalib. We trained two anomaly detection models, PaDiM and PatchCore, on the MVTec AD dataset and evaluated their performance using metrics like precision, recall, and F1 score. We also visualized the models‚Äô predictions using heatmaps and masks. Finally, we tested data augmentation techniques to improve the models‚Äô generalization.</p>
<p>If you want to dive deeper into unsupervised learning, check out these tutorials:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://docs.voxel51.com/tutorials/dimension_reduction.html">Visualizing Data with Dimensionality Reduction Techniques</a></p></li>
<li><p><a class="reference external" href="https://docs.voxel51.com/tutorials/clustering.html">Clustering Images with Embeddings</a></p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[52]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">session</span><span class="o">.</span><span class="n">view</span> <span class="o">=</span> <span class="n">mapped_view</span><span class="o">.</span><span class="n">view</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</div>


             </article>
             
            </div>
            <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../recipes/index.html" class="btn btn-neutral float-right" title="FiftyOne Recipes" accesskey="n" rel="next">Next <img src="../_static/images/chevron-right-orange.svg" class="next-page"></a>
      
      
        <a href="small_object_detection.html" class="btn btn-neutral" title="Detecting Small Objects with SAHI" accesskey="p" rel="prev"><img src="../_static/images/chevron-right-orange.svg" class="previous-page"> Previous</a>
      
    </div>
  
</footer>
          </div>
        </div>

        <div class="pytorch-content-right" id="pytorch-content-right">
          <div class="pytorch-right-menu" id="pytorch-right-menu">
            <div class="pytorch-side-scroll" id="pytorch-side-scroll-right">
              <ul>
<li><a class="reference internal" href="#">Anomaly Detection with FiftyOne and Anomalib</a><ul>
<li><a class="reference internal" href="#Setup">Setup</a><ul>
<li><a class="reference internal" href="#Install-dependencies">Install dependencies</a></li>
<li><a class="reference internal" href="#Load-and-Visualize-the-MVTec-AD-dataset">Load and Visualize the MVTec AD dataset</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Train-an-Anomaly-Detection-Model">Train an Anomaly Detection Model</a></li>
<li><a class="reference internal" href="#Evaluate-Anomaly-Detection-Models">Evaluate Anomaly Detection Models</a><ul>
<li><a class="reference internal" href="#Compare-Anomaly-Detection-Models">Compare Anomaly Detection Models</a></li>
</ul>
</li>
<li><a class="reference internal" href="#Test-Data-Augmentation-Techniques">Test Data Augmentation Techniques</a></li>
<li><a class="reference internal" href="#Summary">Summary</a></li>
</ul>
</li>
</ul>

            </div>
          </div>
        </div>
      </section>
    </div>

  


  

     
       <script type="text/javascript" id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
         <script src="../_static/jquery.js"></script>
         <script src="../_static/underscore.js"></script>
         <script src="../_static/doctools.js"></script>
         <script src="https://unpkg.com/@jupyter-widgets/html-manager@^0.20.1/dist/embed-amd.js"></script>
         <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
         <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
         <script src="../_static/js/voxel51-website.js"></script>
         <script src="../_static/js/custom.js"></script>
     

  

  <script type="text/javascript" src="../_static/js/vendor/popper.min.js"></script>
  <script type="text/javascript" src="../_static/js/vendor/bootstrap.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/list.js/1.5.0/list.min.js"></script>
  <script type="text/javascript" src="../_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

  <!-- Begin Footer -->


<footer class="footer-wrapper" id="docs-tutorials-resources">
  <div class="footer pytorch-container">
    <div class="footer__logo">
      <a href="https://voxel51.com/"
        ><img
          src="https://voxel51.com/images/logo/voxel51-logo-horz-color-600dpi.png"
      /></a>
    </div>

    <div class="footer__address">
      330 E Liberty St<br />
      Ann Arbor, MI 48104<br />
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>

    <!--
    <div class="footer__contact">
      <a href="mailto:info@voxel51.com">info@voxel51.com</a>
    </div>
    -->

    <div class="footer__links">
      <div class="footer__links--col2">
        <p class="nav__item--brand">Products</p>
        <a href="https://voxel51.com/fiftyone/">FiftyOne</a>
        <a href="https://voxel51.com/fiftyone-teams/">FiftyOne Teams</a>
        <a href="https://voxel51.com/computer-vision-use-cases/">Use Cases</a>
        <a href="https://voxel51.com/success-stories/">Success Stories</a>
      </div>
      <div class="footer__links--col3">
        <p class="nav__item--brand">Resources</p>
        <a href="https://voxel51.com/blog/">Blog</a>
        <a href="https://docs.voxel51.com/">Docs</a>
        <a href="https://github.com/voxel51/">GitHub</a>
        <a href="https://community.voxel51.com">Discord</a>
        <a href="https://voxel51.com/ourstory/">About Us</a>
        <a href="https://voxel51.com/computer-vision-events/">Events</a>
        <a href="https://voxel51.com/jobs/">Careers</a>
        <a href="https://voxel51.com/press/">Press</a>
      </div>
    </div>

    <div class="footer__icons">
      <ul class="list-inline">
        <li>
          <a href="https://www.linkedin.com/company/voxel51/">
            <i class="fa-brands fa-linkedin"></i>
          </a>
        </li>
        <li>
          <a href="https://github.com/voxel51/">
            <i class="fa-brands fa-github"></i>
          </a>
        </li>
        <li>
          <a href="https://twitter.com/voxel51">
            <i class="fa-brands fa-twitter"></i>
          </a>
        </li>
        <li>
          <a href="https://www.facebook.com/voxel51/">
            <i class="fa-brands fa-facebook"></i>
          </a>
        </li>
      </ul>
    </div>

    <div class="footer__copyright">
      <ul class="list-inline">
        <li>&copy; 2024 Voxel51 Inc.</li>
        <li>
          <a href="https://voxel51.com/privacy/">Privacy Policy</a>
        </li>
        <li>
          <a href="https://voxel51.com/terms/">Terms of Service</a>
        </li>
      </ul>
    </div>
  </div>
</footer>

<!-- https://buttons.github.io -->
<script async defer src="https://buttons.github.io/buttons.js" ></script>

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XD15NFRY3M" ></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() {
    dataLayer.push(arguments);
  }
  gtag("js", new Date());
  gtag("config", "G-XD15NFRY3M");
</script>

<script>
  function open_modal(modal_id, modal_closer_id) {
    // Get the modal
    let the_modal = document.getElementById(modal_id);

    the_modal.style.display = "flex";
    document.body.style.overflow = "hidden";

    let the_modal_closer = document.getElementById(modal_closer_id);

    // When the user clicks on <span> (x), close the modal
    the_modal_closer &&
      (the_modal_closer.onclick = function () {
        the_modal.style.display = "none";
      });

    // When the user clicks anywhere outside of the modal, close it
    window.onclick = function (event) {
      if (event.target == the_modal) {
        the_modal.style.display = "none";
        document.body.style.overflow = "unset";
        window.onclick = undefined;
      }
    };
  }
</script>

<!-- bigpicture.io -->
<script>
  !(function (e, t, i) {
    var r = (e.bigPicture = e.bigPicture || []);
    if (!r.initialized)
      if (r.invoked)
        e.console &&
          console.error &&
          console.error("BigPicture.io snippet included twice.");
      else {
        (r.invoked = !0),
          (r.SNIPPET_VERSION = 1.5),
          (r.handler = function (e) {
            if (void 0 !== r.callback)
              try {
                return r.callback(e);
              } catch (e) {}
          }),
          (r.eventList = ["mousedown", "mouseup", "click", "submit"]),
          (r.methods = [
            "track",
            "identify",
            "page",
            "group",
            "alias",
            "integration",
            "ready",
            "intelReady",
            "consentReady",
            "on",
            "off",
          ]),
          (r.factory = function (e) {
            return function () {
              var t = Array.prototype.slice.call(arguments);
              return t.unshift(e), r.push(t), r;
            };
          });
        for (var n = 0; n < r.methods.length; n++) {
          var o = r.methods[n];
          r[o] = r.factory(o);
        }
        r.getCookie = function (e) {
          var i = ("; " + t.cookie).split("; " + e + "=");
          return 2 == i.length && i.pop().split(";").shift();
        };
        var c = (r.isEditor = (function () {
          try {
            return (
              e.self !== e.top &&
              (new RegExp("app" + i, "ig").test(t.referrer) ||
                "edit" == r.getCookie("_bpr_edit"))
            );
          } catch (e) {
            return !1;
          }
        })());
        r.init = function (n, o) {
          if (((r.projectId = n), (r._config = o), !c))
            for (var a = 0; a < r.eventList.length; a++)
              e.addEventListener(r.eventList[a], r.handler, !0);
          var s = t.createElement("script");
          s.async = !0;
          var d = c ? "/editor/editor" : "/public-" + n;
          (s.src = "//cdn" + i + d + ".js"),
            t.getElementsByTagName("head")[0].appendChild(s);
        };
      }
  })(window, document, ".bigpicture.io");
  bigPicture.init("1646");
</script>


  

  <!-- End Footer -->

  <!-- Begin Mobile Menu -->

  

  <!-- End Mobile Menu -->

  <script type="text/javascript" src="../_static/js/vendor/anchor.min.js"></script>

  <script type="text/javascript">
    $(document).ready(function() {
      mobileMenu.bind();
      mobileTOC.bind();
      pytorchAnchors.bind();
      sideMenus.bind();
      scrollToAnchor.bind();
      highlightNavigation.bind();
      mainMenuDropdown.bind();
      filterTags.bind();

      // Add class to links that have code blocks, since we cannot create links in code blocks
      $("article.pytorch-article a span.pre").each(function(e) {
        $(this).closest("a").addClass("has-code");
      });
    })
  </script>
</body>
</html>